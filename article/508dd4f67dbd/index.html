<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>【NLP】多标签分类【上】 | issey的博客</title><meta name="keywords" content="NLP、MultiLabelClassification"><meta name="author" content="issey"><meta name="copyright" content="issey"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="《【NLP】多标签分类》主要介绍利用三种机器学习方法和一种序列生成方法来解决多标签分类问题（包含实验与对应代码）。共分为上下两篇，上篇聚焦三种机器学习方法，分别是：Binary Relevance (BR)、Classifier Chains (CC)、Label Powerset (LP)，下篇聚焦利用序列生成解决多标签分类方法，将使用Transformer完成该任务。"><meta property="og:type" content="article"><meta property="og:title" content="【NLP】多标签分类【上】"><meta property="og:url" content="https://blog.issey.top/article/508dd4f67dbd/index.html"><meta property="og:site_name" content="issey的博客"><meta property="og:description" content="《【NLP】多标签分类》主要介绍利用三种机器学习方法和一种序列生成方法来解决多标签分类问题（包含实验与对应代码）。共分为上下两篇，上篇聚焦三种机器学习方法，分别是：Binary Relevance (BR)、Classifier Chains (CC)、Label Powerset (LP)，下篇聚焦利用序列生成解决多标签分类方法，将使用Transformer完成该任务。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://img.issey.top/img/logo.png"><meta property="article:published_time" content="2024-01-10T11:32:32.000Z"><meta property="article:modified_time" content="2024-04-08T10:57:00.660Z"><meta property="article:author" content="issey"><meta property="article:tag" content="实战"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://img.issey.top/img/logo.png"><link rel="shortcut icon" href="https://img.issey.top/img/logo.png"><link rel="canonical" href="https://blog.issey.top/article/508dd4f67dbd/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"NTF3ZGK1TF",apiKey:"4d3d88863c6c78ac79547d0f743e6b5e",indexName:"issey",hits:{per_page:10},languages:{input_placeholder:"搜索文章",hits_empty:"找不到您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，用时 ${time} 毫秒"}},localSearch:{path:"/search.xml",preload:!1,languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:800},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:50,languages:{author:"作者: issey",link:"链接: ",source:"来源: issey的博客",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://fastly.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js",css:"https://fastly.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"【NLP】多标签分类【上】",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-08 18:57:00"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="/css/rightMenu.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://img.issey.top/img/touxiang.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推荐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://img.issey.top/img/backhead.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">issey的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推荐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【NLP】多标签分类【上】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-10T11:32:32.000Z" title="发表于 2024-01-10 19:32:32">2024-01-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-08T10:57:00.660Z" title="更新于 2024-04-08 18:57:00">2024-04-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="【NLP】多标签分类【上】"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="简介">简介</h1><p>《【NLP】多标签分类》主要介绍利用三种机器学习方法和一种序列生成方法来解决多标签分类问题（包含实验与对应代码）。共分为上下两篇，上篇聚焦三种机器学习方法，分别是：Binary Relevance (BR)、Classifier Chains (CC)、Label Powerset (LP)，下篇聚焦利用序列生成解决多标签分类方法，将使用Transformer完成该任务。</p><p>本文共分为5节，第一节介绍实验数据来源、任务说明；第二节介绍BR、CC、LP各自原理以及优缺点；第三节介绍本文使用的多标签分类评估标准；第四节介绍实验环境、实验步骤、实验评估以及相关代码；第五节为全文总结。</p><h1 id="个人博客与相关链接">个人博客与相关链接</h1><p>本文相关代码和数据集已同步上传github: <a target="_blank" rel="noopener" href="https://github.com/iceissey/issey_Kaggle/tree/main/MultiLabelClassification">issey_Kaggle/MultiLabelClassification at main · iceissey/issey_Kaggle (github.com)</a></p><p>本文代码（Notebook）已公布至kaggle: <a target="_blank" rel="noopener" href="https://www.kaggle.com/code/isseyice/xlnet-embedding-and-machine-learning-br-cc-lp/notebook?scriptVersionId=158428477">XLNET embedding and machine learning（BR、CC、LP） | Kaggle</a></p><p>博主个人博客链接：<a target="_blank" rel="noopener" href="https://www.issey.top/">issey的博客 - 愿无岁月可回首</a></p><h1 id="实验数据与任务说明">实验数据与任务说明</h1><p>数据来源：<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/shivanandmn/multilabel-classification-dataset/data">Multi-Label Classification Dataset (kaggle.com)</a></p><p>任务说明：</p><ul><li>背景：NLP——多标签分类数据集。</li><li>内容：该数据集包含6个不同的标签（计算机科学、物理学、数学、统计学、定量生物学、定量金融），用于根据摘要和标题对研究论文进行分类。 标签列中的值1表示该标签属于该论文，每篇论文可以有多个标签为1。</li></ul><p><img src="https://img.issey.top/img/202401101606632.png"></p><h1 id="多标签分类任务与相关算法">多标签分类任务与相关算法</h1><h2 id="多标签分类任务简介">多标签分类任务简介</h2><p>多标签分类<strong>（Multi-label Classification）</strong>是一种机器学习任务，其中每个输入样本可以分配给多个类别标签，而不是只能分配给一个单一的类别标签。与传统的单标签分类不同，多标签分类允许一个样本同时属于多个类别，这更符合现实世界中许多复杂问题的性质。</p><h2 id="相关算法">相关算法</h2><p>多标签分类方法主要分为两大类，分别是<strong>问题转换方法</strong>和<strong>算法适应方法</strong>，本篇主要集中于问题转换方法中的前三种。</p><p><strong>问题转换方法</strong>：这些方法通过转换问题使其适用于标准的单标签分类算法。主要包括以下几种：</p><ul><li><strong>二元相关性（Binary Relevance, BR）</strong>：这种方法将多标签问题分解成多个独立的二分类问题，每个标签都被视为一个独立的二分类问题。<ul><li>优点：<ol type="1"><li><strong>简单易实现：</strong>BR方法的实现相对简单直接，因为它将复杂的多标签问题分解为多个标准的二分类问题。</li><li><strong>灵活性：</strong>由于BR方法在每个标签上独立训练分类器，因此可以针对不同的标签选择最适合的分类算法。</li><li><strong>可扩展性：</strong>在新标签加入时，只需增加相应的二分类器，而无需修改或重新训练其他分类器。</li><li><strong>高效：</strong>由于每个标签都独立处理，可以并行训练和预测，提高了处理速度。</li></ol></li><li>缺点：<ol type="1"><li><span style="color:red"><strong>忽略标签依赖性</strong>：BR方法的主要缺点是它忽略了标签之间的相关性。在实际应用中，标签往往不是完全独立的，它们之间的关联可能对分类结果有重要影响。</span></li><li><strong>预测性能问题</strong>：由于不考虑标签间的依赖关系，BR方法在某些复杂的多标签问题上的预测性能可能不如那些能够考虑标签依赖性的方法。</li></ol></li></ul></li><li><strong>标签幂集（Label Powerset, LP）</strong>：在这种方法中，每一种标签组合都被视为一个独立的类别，从而将多标签问题转换为单标签多类别问题。<ul><li>优点<ol type="1"><li><strong>考虑标签之间的依赖性</strong>：LP方法能够捕捉和利用标签之间的相关性。这在标签彼此之间存在强烈依赖性的情况下特别有用。</li><li><strong>简化模型训练</strong>：与需要为每个标签单独训练一个分类器的二元相关方法相比，LP只需训练一个模型，这可以简化训练过程。</li><li><strong>直接预测标签集合</strong>：LP方法直接预测整个标签集合，避免了将标签预测作为独立事件处理时可能出现的问题。</li></ol></li><li>缺点：<ol type="1"><li><span style="color:red"><strong>组合爆炸</strong>：当标签数量增多时，可能的标签组合数会指数级增长，导致计算和存储需求急剧增加。<strong>由于组合爆炸的问题，标签幂集无法处理标签种类较多的问题。</strong></span></li><li><strong>数据稀疏问题</strong>：对于一些罕见的标签组合，可能没有足够的训练数据，这会导致模型性能下降。</li><li><strong>效率问题</strong>：尽管只需训练一个模型，但模型可能变得非常复杂，特别是当存在大量的标签组合时。</li></ol></li></ul></li><li><strong>分类器链（Classifier Chains, CC）</strong>：这种方法通过构建一个分类器链来解决标签之间的依赖问题。每个分类器在链中负责一个标签，并将前面分类器的预测结果作为额外的输入。<ul><li>优点：<ol type="1"><li><strong>考虑标签间的依赖性</strong>：分类器链通过序列化的方式考虑标签间的依赖关系，这在标签相关性显著的情况下特别有用。</li><li><strong>可扩展性</strong>：相比于标签幂集方法，分类器链在处理大量标签时更为高效，因为它避免了组合爆炸问题。</li><li><strong>较好的泛化能力</strong>：相对于二元相关方法，分类器链通常能够提供更好的泛化能力，尤其是在标签之间存在依赖关系时。</li></ol></li><li>缺点：<ol type="1"><li><strong>链的顺序敏感性</strong>：分类器链的性能可能受到链中分类器顺序的影响。不同的标签顺序可能导致不同的性能表现。</li><li><strong>错误传播</strong>：链中早期分类器的错误可能会传播到链的后面部分，影响整体性能。</li></ol></li></ul></li><li><strong>随机k标签子集（Random k-Labelsets, RAkEL）</strong>：这种方法是通过随机选择标签子集并对每个子集应用LP方法，然后综合这些模型的预测结果。<code>由于本文涉及的实验总共标签总类也才6种，所以没有使用这种方法而直接选择了LP</code>。<ul><li>优点：<ol type="1"><li><strong>缓解组合爆炸问题</strong>：通过在较小的标签子集上应用LP方法，RAkEL减少了可能的标签组合数量，从而缓解了标签幂集法中的组合爆炸问题。</li><li><strong>考虑标签间的依赖性</strong>：与二元相关方法相比，RAkEL能够捕捉标签子集内部的依赖关系，提高了模型的准确性。</li><li><strong>更好的泛化能力</strong>：由于模型在多个随机选择的标签子集上训练，这可以增加模型的泛化能力。</li></ol></li><li>缺点：<ol type="1"><li><strong>随机性</strong>：标签子集的随机选择可能导致模型性能的不稳定性。</li><li><strong>可能忽略某些标签关系</strong>：如果某些相关标签从不在同一个子集中出现，那么它们之间的关系可能不会被模型捕捉到。</li><li><strong>计算复杂度</strong>：虽然RAkEL缓解了组合爆炸问题，但仍需要训练多个LP模型，这可能比单一的分类器链或二元相关方法更耗时。</li><li><strong>预测一致性问题</strong>：不同的标签子集模型可能对相同的标签做出不同的预测，需要有效的机制来整合这些预测。</li><li><strong>参数选择</strong>：选择合适的子集大小（k值）和子集数量是RAkEL方法的关键，这可能需要根据具体的数据集进行调整。</li></ol></li></ul></li></ul><p><strong>算法适应方法</strong>：这些方法通过修改现有的学习算法使其能够直接处理多标签数据。主要包括以下几种：适应决策树（Adapted Decision Trees）、适应神经网络（Adapted Neural Networks）、适应支持向量机（Adapted Support Vector Machines）、k最近邻修改版（k-Nearest Neighbors Adaptation）。</p><p>除问题转换方法和算法适应方法外，深度学习方法也在多标签分类中表现出色。<span style="color:red">在本文的下篇中，会介绍将多标签分类转换为多标签序列生成任务的方法。</span></p><h1 id="多标签分类评估方法">多标签分类评估方法</h1><h3 id="准确率accuracy">1. 准确率（Accuracy）</h3><ul><li><strong>定义</strong>：准确率是正确预测的样本数与总样本数的比例。在多标签分类中，如果所有的标签都被准确预测，则一个样本的预测被认为是正确的。</li><li><strong>实现：</strong>使用sklearn.metrics的accuracy_score方法实现。</li><li><strong>备注：</strong>由于只有当某样本所有标签全预测正确，才能算该样本预测正确，导致这种方式计算出的Acc结果普遍偏低。<span style="color:red">在下篇中，会介绍另一种计算Acc的方式，即先计算每一个label的Acc，然后在取平均值。</span></li></ul><h3 id="精确度precision--微观平均micro-average">2. 精确度（Precision）- 微观平均（Micro-average）</h3><ul><li><strong>定义</strong>：精确度是模型正确预测为正的实例（真正例）占模型预测为正的所有实例（真正例和假正例）的比例。</li><li><strong>计算方法</strong>：微观平均精确度是通过汇总所有类别的真正例和假正例的数量，然后计算总体精确度得到的。在多标签设置中，这意味着考虑所有标签的预测结果，而不是单独考虑每个标签。</li><li><strong>实现：</strong>使用sklearn.metrics的precision_score方法实现。</li></ul><h3 id="召回率recall--微观平均micro-average">3. 召回率（Recall）- 微观平均（Micro-average）</h3><ul><li><strong>定义</strong>：召回率是模型正确预测为正的实例占实际为正的所有实例（真正例和假负例）的比例。</li><li><strong>计算方法</strong>：微观平均召回率是通过汇总所有类别的真正例和假负例的数量，然后计算总体召回率得到的。它反映了模型在所有标签上的总体能力，来正确地识别正类实例。</li><li><strong>实现：</strong>使用sklearn.metrics的recall_score方法实现。</li></ul><h3 id="f1-分数f1-score--微观平均micro-average">4. F1 分数（F1 Score）- 微观平均（Micro-average）</h3><ul><li><strong>定义</strong>：F1 分数是精确度和召回率的调和平均值，用于平衡这两个指标。</li><li><strong>计算方法</strong>：微观平均 F1 分数是基于微观平均精确度和召回率计算得到的。它是这两个指标的调和平均值，因此在精确度和召回率都重要时，提供了一个综合性能度量。</li><li><strong>实现:</strong>使用sklearn.metrics的f1_score方法实现。</li></ul><h1 id="实验">实验</h1><h2 id="实验环境">实验环境</h2><p>本实验是在以下配置的环境中进行的：</p><ul><li><strong>编程语言和版本</strong>：<ul><li>Python 3.9：一个广泛使用的高级编程语言，适用于数据科学和机器学习项目。</li></ul></li><li><strong>主要库和框架</strong>：<ul><li>NumPy 1.23.3：用于高性能科学计算和数据分析的基础包。</li><li>Pandas 1.4.4：提供高效的数据结构和数据分析工具。</li><li>Matplotlib 3.5.3：用于数据可视化的绘图库。</li><li>PyTorch 1.13.0：一个灵活的深度学习框架，适用于研究和生产。</li><li>PyTorch CUDA 11.6：用于在NVIDIA GPU上加速PyTorch运算的CUDA支持库。</li></ul></li><li><strong>机器学习和深度学习库</strong>：<ul><li>Transformers 4.18.0：由Hugging Face提供的，用于自然语言处理的预训练模型和转换器。</li><li>scikit-learn 1.2.2：提供简单有效的数据挖掘和数据分析工具。</li><li>scikit-multilearn 0.2.0：用于多标签分类的机器学习库。</li></ul></li></ul><h2 id="实验步骤">实验步骤</h2><p>本篇的实验步骤主要包括：1）数据观察与预处理阶段。2）词嵌入阶段。3）模型训练与测试阶段。4）进一步探索。</p><h3 id="数据观察与预处理">数据观察与预处理</h3><h4 id="数据观察">数据观察</h4><ul><li><strong>单词数量统计：</strong>在本实验中，我们专注于观察数据集中每个文本项的单词数量。通过统计信息，我们可以了解数据集中文本的长度分布。</li></ul><h4 id="数据预处理">数据预处理</h4><ul><li><strong>最小化预处理</strong>：由于本实验在后续词嵌入时使用XL-NET模型，且与使用传统文本分类方法相比，使用XL-NET等先进的预训练模型时，常规的文本预处理步骤（如去除特殊符号、停用词移除、词形还原）并不是必要的。这些模型的分词器能够有效处理原始文本中的复杂词汇结构，同时保留对上下文理解至关重要的词汇和语法特征。</li><li><strong>实验步骤完整性</strong>：虽然在本实验中不需要传统的预处理步骤，但为了保持实验步骤的完整性和系统性，我们仍然包含了这一部分。这有助于清晰地展示实验流程，并为可能需要适当预处理的后续研究提供参考。</li></ul><hr><h4 id="代码部分">代码部分</h4><ul><li><strong>准备工作</strong></li></ul><p>导入相关库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizer, XLNetModel</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><p>检查GPU是否可用。在上篇的实验中，如果GPU不可用问题也不大，直接用CPU跑即可，因为上篇使用GPU的地方只有embedding。不过在下篇时GPU是必要的，如果本地环境不支持，建议放到云服务器（如kaggle）上跑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the device to GPU (if available).</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Using device:&quot;</span>, device)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using device: cuda</span><br></pre></td></tr></table></figure><ul><li><strong>准备数据集</strong></li></ul><p>由于题目要求使用TITLE和ABSTRACT共同参与预测，所以简单做一下拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Prepare the data&quot;&quot;&quot;</span></span><br><span class="line">input_csv = <span class="string">&quot;/kaggle/input/multilabel-classification-dataset/train.csv&quot;</span></span><br><span class="line">data = pd.read_csv(input_csv)  </span><br><span class="line"><span class="comment"># data = data[:20]  # Test</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(data))</span><br><span class="line">data[<span class="string">&#x27;combined_text&#x27;</span>] = data[<span class="string">&#x27;TITLE&#x27;</span>] + <span class="string">&quot; &quot;</span> + data[<span class="string">&#x27;ABSTRACT&#x27;</span>] </span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;combined_text&#x27;</span>].head())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">20972</span><br><span class="line">0    Reconstructing Subject-Specific Effect Maps   ...</span><br><span class="line">1    Rotation Invariance Neural Network   Rotation ...</span><br><span class="line">2    Spherical polyharmonics and Poisson kernels fo...</span><br><span class="line">3    A finite element approximation <span class="keyword">for</span> the stochas...</span><br><span class="line">4    Comparative study of Discrete Wavelet Transfor...</span><br><span class="line">Name: combined_text, dtype: object</span><br></pre></td></tr></table></figure><ul><li><strong>统计combined_text单词分布</strong></li></ul><p>检查combined_text最长、最小、平均单词长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;View the distribution of word counts&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Split the text using spaces and calculate the number of words</span></span><br><span class="line">data[<span class="string">&#x27;word_count&#x27;</span>] = data[<span class="string">&#x27;combined_text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(<span class="built_in">str</span>(x).split()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print statistical information about the number of words</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Word count statistics:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Maximum word count:&quot;</span>, data[<span class="string">&#x27;word_count&#x27;</span>].<span class="built_in">max</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Minimum word count:&quot;</span>, data[<span class="string">&#x27;word_count&#x27;</span>].<span class="built_in">min</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average word count:&quot;</span>, data[<span class="string">&#x27;word_count&#x27;</span>].mean())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Word count statistics:</span><br><span class="line">Maximum word count: 462</span><br><span class="line">Minimum word count: 5</span><br><span class="line">Average word count: 157.9198455082968</span><br></pre></td></tr></table></figure><p>绘制单词分布柱状图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.hist(data[<span class="string">&#x27;word_count&#x27;</span>], bins=<span class="number">50</span>, alpha=<span class="number">0.75</span>, color=<span class="string">&#x27;b&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Word count&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Frequency&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Word count distribution&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202401101705063.png"></p><h3 id="词嵌入阶段">词嵌入阶段</h3><h4 id="xl-net嵌入">XL-NET嵌入</h4><p>在本实验中，我们使用了预训练的XL-NET模型来生成文本嵌入，这是一个关键步骤，旨在将文本转换为能被机器学习模型有效处理的数值形式。</p><ul><li><strong>模型和分词器加载</strong>：我们首先加载了XLNet的基础模型（<code>xlnet-base-cased</code>）和对应的分词器。这个分词器将负责将原始文本转换成模型可以理解的令牌序列。</li><li><strong>设定批处理大小</strong>：考虑到计算效率和内存限制，我们设定了一个合适的批处理大小（<code>batch_size = 32</code>）。这意味着每次向模型输入32个文本样本进行处理。</li></ul><h4 id="嵌入生成过程">嵌入生成过程</h4><ul><li><strong>文本准备和处理</strong>：我们将数据集中的文本转换为字符串列表，并按批次处理。每个批次的文本被分词器编码，其中包括截断和填充操作以确保文本长度一致。</li><li><strong>嵌入计算</strong>：对于每个批次，我们将编码后的文本输入XL-NET模型。通过模型，我们获取每个文本的嵌入表示，这些表示捕捉了文本中的语义信息。</li><li><strong>处理和存储嵌入</strong>：得到的嵌入被转换为NumPy数组，并被收集在一起。最终，所有的嵌入被存储在HDF5文件格式中，方便后续的机器学习任务使用。</li></ul><h4 id="不进行微调的决定">不进行微调的决定</h4><ul><li><strong>一次性嵌入过程</strong>：本实验选择不对XL-NET模型进行微调，而是直接使用预训练模型一次性生成所有文本的嵌入。这种方法简化了实验流程，同时允许我们充分利用XL-NET预训练模型的强大语义捕捉能力。</li><li><strong>效率和实用性</strong>：将所有文本的嵌入预先计算并存储起来，提高了后续实验步骤的效率。</li></ul><hr><h4 id="代码部分-1">代码部分</h4><ul><li><strong>加载分词器和预训练模型</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Load the XLNet tokenizer and model&quot;&quot;&quot;</span></span><br><span class="line">tokenizer = XLNetTokenizer.from_pretrained(<span class="string">&#x27;xlnet-base-cased&#x27;</span>)</span><br><span class="line">model = XLNetModel.from_pretrained(<span class="string">&#x27;xlnet-base-cased&#x27;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># Determine the batch size</span></span><br><span class="line">all_embeddings = []</span><br><span class="line"><span class="comment"># token = tokenizer.convert_ids_to_tokens(5)</span></span><br><span class="line"><span class="comment"># print(token)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;&quot;Choose not to fine-tune the embedding layer, so embed all texts at once into vectors&quot;&quot;&quot;</span></span><br><span class="line">texts = data[<span class="string">&#x27;combined_text&#x27;</span>].astype(<span class="built_in">str</span>).tolist()</span><br></pre></td></tr></table></figure><ul><li>embedding，并将嵌入好的向量一次性存储下来</li></ul><p>tqdm是一个可视化进度条的库，可以方便的查看处理进度。</p><p>这里解释一下如何从XL-NET模型的输出中提取嵌入(embedding)。</p><ul><li><strong>模型输出理解</strong>：当我们将输入文本通过XL-NET模型处理时，<code>outputs</code> 对象包含了多个不同的输出组件。其中，<code>last_hidden_state</code> 是一个多维张量，其维度通常是 <code>[批处理大小, 序列长度, 隐藏单元数]</code>。这个张量包含了模型对每个输入令牌的最后一层隐藏状态的表示。</li><li><strong>选择特定令牌的嵌入</strong>：在XL-NET和类似的变压器模型中，每个输入令牌都有一个对应的输出向量。在这里，<code>outputs.last_hidden_state[:, 0, :]</code> 表示我们选择了每个序列的第一个令牌（通常是特殊的分类令牌，如BERT中的<code>[CLS]</code>）的输出向量。<span style="color:red">这个向量被认为是整个输入序列的聚合表示，并经常用于分类任务。</span></li></ul><blockquote><p>还记得我在今年早些的时候做的那个Bert+Bilstm的任务<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_52466006/article/details/130064014?spm=1001.2014.3001.5502">【NLP实战】基于Bert和双向LSTM的情感分类【中篇】-CSDN博客</a>，当时我在embeding后直接取的last_hidden_state，也就是个三维向量，接着用Bilstm得到最终的二维隐藏层（只保留了最后的隐藏状态），现在想来当时对Bert的理解还是不到位。然而这两种方法都是有效的，不过一个是词维度的嵌入，一个是句维度的嵌入，本文上篇使用的embedding就是句维度的嵌入。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify the directory path</span></span><br><span class="line">directory_path = <span class="string">&#x27;/kaggle/working/multilabel-classification-dataset/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the directory if it doesn&#x27;t exist</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(directory_path):</span><br><span class="line">    os.makedirs(directory_path)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> start_index <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(texts), batch_size)):</span><br><span class="line">    <span class="comment"># Encode the text</span></span><br><span class="line">    batch_texts = texts[start_index:start_index + batch_size]</span><br><span class="line">    encoded_inputs = tokenizer(batch_texts, return_tensors=<span class="string">&#x27;pt&#x27;</span>, max_length=<span class="number">512</span>, truncation=<span class="literal">True</span>, padding=<span class="string">&#x27;max_length&#x27;</span>)</span><br><span class="line">    <span class="comment"># get embeddings</span></span><br><span class="line">    input_ids = encoded_inputs[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">    attention_mask = encoded_inputs[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">    <span class="comment">#  calculate embeddings</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">    <span class="comment">#  move the results back to CPU and convert to numpy arrays</span></span><br><span class="line">    embeddings = outputs.last_hidden_state[:, <span class="number">0</span>, :].cpu().numpy()</span><br><span class="line">    <span class="comment"># print(embeddings.shape)</span></span><br><span class="line">    </span><br><span class="line">    all_embeddings.extend(embeddings)</span><br><span class="line"><span class="comment"># Convert all embeddings to numpy arrays</span></span><br><span class="line">all_embeddings = np.array(all_embeddings)</span><br><span class="line"><span class="built_in">print</span>(all_embeddings.shape)</span><br><span class="line"><span class="comment"># Store embedding vectors to an HDF5 file</span></span><br><span class="line">hdf5_filename = <span class="string">&#x27;/kaggle/working/multilabel-classification-dataset/embeddings.h5&#x27;</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(hdf5_filename, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> hdf5_file:</span><br><span class="line">    hdf5_file.create_dataset(<span class="string">&#x27;embeddings&#x27;</span>, data=all_embeddings)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Embeddings have been stored in the <span class="subst">&#123;hdf5_filename&#125;</span> file.&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">100</span>%|██████████| <span class="number">656</span>/<span class="number">656</span> [<span class="number">15</span>:<span class="number">11</span>&lt;<span class="number">00</span>:<span class="number">00</span>,  <span class="number">1.39</span>s/it]</span><br><span class="line">(<span class="number">20972</span>, <span class="number">768</span>)</span><br><span class="line">Embeddings have been stored <span class="keyword">in</span> the /kaggle/working/multilabel-classification-dataset/embeddings.h5 file.</span><br></pre></td></tr></table></figure><p>可以看到，现在我们的数据集中的text（也就是<code>'combined_text'</code>），被编译为了一个768维度的向量。一共有20972行text，所以嵌入矩阵为<code>(20972, 768)</code>。</p><h3 id="模型训练与测试阶段">模型训练与测试阶段</h3><h4 id="数据准备">数据准备</h4><ul><li><strong>数据集加载与分割</strong>：我们从CSV文件中加载了数据集，并提取了标签列。接着，使用XL-NET生成的嵌入向量作为特征，将数据集分割为训练集和测试集，保证了模型训练和评估的有效性和公正性。</li></ul><h4 id="多标签分类方法">多标签分类方法</h4><p>我们采用了三种不同的多标签分类方法：二元相关（Binary Relevance, BR）、分类器链（Classifier Chains, CC）和标签幂集（Label Powerset, LP）。每种方法都使用了随机森林分类器作为基学习器。</p><ul><li><strong>二元相关（Binary Relevance）</strong>：这种方法将多标签问题分解为多个独立的二分类问题。我们首先训练了BR模型，并记录了训练时间。接着，我们在测试集上进行预测，并计算了准确度、精确度、召回率和F1分数（微观平均）。</li><li><strong>分类器链（Classifier Chains）</strong>：这种方法通过构建一个分类器链，使每个分类器在预测时考虑到之前分类器的输出。同样，我们训练了CC模型，记录了训练时间，并在测试集上进行了评估。</li><li><strong>标签幂集（Label Powerset）</strong>：LP方法将多标签问题转换为单标签多类别问题。我们训练了LP模型，并对其进行了测试集上的性能评估。</li></ul><h4 id="性能评估">性能评估</h4><ul><li><p><strong>评估指标</strong>：为了全面评估每种方法的性能，我们计算了准确度、精确度、召回率和F1分数（均采用微观平均），评估指标详细说明如第三节所示。这些指标帮助我们理解不同方法在处理多标签分类任务时的效果和局限。</p></li><li><p><strong>训练时间和性能</strong>：每种方法的训练时间都被记录下来，以评估其在实际应用中的可行性。</p></li></ul><hr><h4 id="代码部分-2">代码部分</h4><ul><li><strong>导入相关库</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skmultilearn.problem_transform <span class="keyword">import</span> BinaryRelevance, ClassifierChain, LabelPowerset</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><ul><li><strong>准备数据</strong></li></ul><p>提取标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_path = <span class="string">&quot;/kaggle/input/multilabel-classification-dataset/train.csv&quot;</span></span><br><span class="line">data = pd.read_csv(data_path)</span><br><span class="line"></span><br><span class="line">label_columns = data.columns[-<span class="number">6</span>:]  <span class="comment"># Extract the &#x27;labels&#x27; column</span></span><br><span class="line">y = data[label_columns].values</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(20972, 6)</span><br></pre></td></tr></table></figure><p>加载经过XL-NET嵌入后的隐向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load embedding vectors</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(<span class="string">&#x27;/kaggle/input/xlnet-embedding-for-multilabel-classification/embeddings.h5&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    embeddings = np.array(f[<span class="string">&#x27;embeddings&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(embeddings.shape)</span><br><span class="line"><span class="comment"># 确保标签和嵌入向量的行数相同</span></span><br><span class="line"><span class="keyword">assert</span> embeddings.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(20972, 768)</span><br></pre></td></tr></table></figure><p>用于后续测试，如果要让模型快速运行就把注释打开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST</span></span><br><span class="line"><span class="comment"># embeddings = embeddings[:1000]</span></span><br><span class="line"><span class="comment"># y = y[:1000]</span></span><br></pre></td></tr></table></figure><p>分割数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split the dataset into a training set and a test set.</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape, X_test.shape, y_train.shape, y_test.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(16777, 768) (4195, 768) (16777, 6) (4195, 6)</span><br></pre></td></tr></table></figure><ul><li>模型训练与测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Binary Relevance</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">br_classifier = BinaryRelevance(RandomForestClassifier())</span><br><span class="line">br_classifier.fit(X_train, y_train)</span><br><span class="line">br_training_time = time.time() - start_time</span><br><span class="line">br_predictions = br_classifier.predict(X_test)</span><br><span class="line">br_precision = precision_score(y_test, br_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">br_recall = recall_score(y_test, br_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">br_f1 = f1_score(y_test, br_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Training Time:&quot;</span>, br_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Accuracy =&quot;</span>, accuracy_score(y_test, br_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Precision (micro-average) =&quot;</span>, br_precision)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Recall (micro-average) =&quot;</span>, br_recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR F1 Score (micro-average) =&quot;</span>, br_f1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier Chains</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">cc_classifier = ClassifierChain(RandomForestClassifier())</span><br><span class="line">cc_classifier.fit(X_train, y_train)</span><br><span class="line">cc_training_time = time.time() - start_time</span><br><span class="line">cc_predictions = cc_classifier.predict(X_test)</span><br><span class="line">cc_precision = precision_score(y_test, cc_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">cc_recall = recall_score(y_test, cc_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">cc_f1 = f1_score(y_test, cc_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Training Time:&quot;</span>, cc_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Accuracy =&quot;</span>, accuracy_score(y_test, cc_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Precision (micro-average) =&quot;</span>, cc_precision)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Recall (micro-average) =&quot;</span>, cc_recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC F1 Score (micro-average) =&quot;</span>, cc_f1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label Powerset</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">lp_classifier = LabelPowerset(RandomForestClassifier())</span><br><span class="line">lp_classifier.fit(X_train, y_train)</span><br><span class="line">lp_training_time = time.time() - start_time</span><br><span class="line">lp_predictions = lp_classifier.predict(X_test)</span><br><span class="line">lp_precision = precision_score(y_test, lp_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">lp_recall = recall_score(y_test, lp_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">lp_f1 = f1_score(y_test, lp_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Training Time:&quot;</span>, lp_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Accuracy =&quot;</span>, accuracy_score(y_test, lp_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Precision (micro-average) =&quot;</span>, lp_precision)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Recall (micro-average) =&quot;</span>, lp_recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP F1 Score (micro-average) =&quot;</span>, lp_f1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">===================================</span><br><span class="line">BR Training Time: 445.8087875843048</span><br><span class="line">BR Accuracy = 0.4476758045292014</span><br><span class="line">BR Precision (micro-average) = 0.8038496791934006</span><br><span class="line">BR Recall (micro-average) = 0.4978240302743614</span><br><span class="line">BR F1 Score (micro-average) = 0.6148632858144426</span><br><span class="line">===================================</span><br><span class="line">CC Training Time: 410.08831691741943</span><br><span class="line">CC Accuracy = 0.4786650774731824</span><br><span class="line">CC Precision (micro-average) = 0.8012065498419995</span><br><span class="line">CC Recall (micro-average) = 0.5277199621570482</span><br><span class="line">CC F1 Score (micro-average) = 0.6363221537759525</span><br><span class="line">===================================</span><br><span class="line">LP Training Time: 74.27938294410706</span><br><span class="line">LP Accuracy = 0.5349225268176401</span><br><span class="line">LP Precision (micro-average) = 0.7178777393310265</span><br><span class="line">LP Recall (micro-average) = 0.5888363292336802</span><br><span class="line">LP F1 Score (micro-average) = 0.646985446985447</span><br></pre></td></tr></table></figure><h4 id="结果分析">结果分析</h4><p>可以看到，LP不仅训练时间最短，而且Acc和F1都要更好。因此，我们可以继续探究使用支持向量机（SVM）作为基分类器的效果。</p><h3 id="进一步探索--使用svm的标签幂集方法">进一步探索--使用SVM的标签幂集方法</h3><h4 id="实验设计">实验设计</h4><ul><li><strong>基分类器更换</strong>：鉴于LP方法的成功，我们决定用SVM替换原先的随机森林分类器，以进一步探索不同基分类器对多标签分类任务性能的影响。</li><li><strong>SVM配置</strong>：我们选择了线性核的SVM，并将其包装在<code>OneVsRestClassifier</code>中，以适应多类别问题。线性核是因其在处理高维数据时的有效性和计算效率而被选用。</li></ul><h4 id="训练和评估">训练和评估</h4><ul><li><p><strong>模型训练</strong>：使用LP方法结合SVM分类器训练模型，并记录了训练时间。</p></li><li><p><strong>性能评估</strong>：在测试集上评估了模型的准确度、精确度、召回率和F1分数（均采用微观平均）。这些指标有助于我们全面了解SVM在多标签分类任务中的表现。</p></li><li><p><strong>训练时间对比</strong>：与之前使用随机森林的LP方法相比，我们特别关注SVM版本的训练时间，以评估其在实际应用中的效率。</p><hr></li></ul><h4 id="代码部分-3">代码部分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use SVM as the base classifier</span></span><br><span class="line">svm_classifier = OneVsRestClassifier(SVC(kernel=<span class="string">&#x27;linear&#x27;</span>))  <span class="comment"># The kernel function uses a linear function.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Label Powerset with SVM</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">lp_svm_classifier = LabelPowerset(svm_classifier)</span><br><span class="line">lp_svm_classifier.fit(X_train, y_train)</span><br><span class="line">lp_svm_training_time = time.time() - start_time</span><br><span class="line">lp_svm_predictions = lp_svm_classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Training Time:&quot;</span>, lp_svm_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Accuracy =&quot;</span>, accuracy_score(y_test, lp_svm_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Precision (micro-average) =&quot;</span>, precision_score(y_test, lp_svm_predictions, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Recall (micro-average) =&quot;</span>, recall_score(y_test, lp_svm_predictions, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM F1 Score (micro-average) =&quot;</span>, f1_score(y_test, lp_svm_predictions, average=<span class="string">&#x27;micro&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">===================================</span><br><span class="line">LP-SVM Training Time: <span class="number">13640.821268558502</span></span><br><span class="line">LP-SVM Accuracy = <span class="number">0.5914183551847437</span></span><br><span class="line">LP-SVM Precision (micro-average) = <span class="number">0.7367712141620165</span></span><br><span class="line">LP-SVM Recall (micro-average) = <span class="number">0.7245033112582782</span></span><br><span class="line">LP-SVM F1 Score (micro-average) = <span class="number">0.7305857660751764</span></span><br></pre></td></tr></table></figure><h4 id="结果分析-1">结果分析</h4><p>可以看到，LP-SVM的训练时间比使用随机森林的LP长了184倍，但所有评估标准都比使用随机森林的LP好。显然，它是我们本篇中最好的模型。</p><h1 id="总结">总结</h1><p>本篇为《【NLP】多标签分类》的上篇，本文详细细探讨了多标签分类问题，聚焦于三种机器学习方法（Binary Relevance, Classifier Chains, Label Powerset），展示了每种方法的原理、优缺点，以及具体的实验评估和代码实现。本文还探讨了如何使用XL-NET做嵌入。实验结果表明，标签幂集方法配合随机森林分类器在训练时间和性能（准确度和F1分数）上表现良好。进一步探索使用SVM作为基分类器后，虽然训练时间增长，但所有评估标准均有所提升，显示出更好的性能。文章通过详细的实验步骤和评估方法，为选择适合特定多标签分类任务的方法提供了实证依据。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.issey.top">issey</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.issey.top/article/508dd4f67dbd/">https://blog.issey.top/article/508dd4f67dbd/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.issey.top" target="_blank">issey的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AE%9E%E6%88%98/">实战</a></div><div class="post_share"><div class="social-share" data-image="https://img.issey.top/img/logo.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/5bd830bf3404/"><img class="prev-cover" src="https://img.issey.top/img/logo.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【NLP】多标签分类【下】</div></div></a></div><div class="next-post pull-right"><a href="/article/4b6ff6ea27bb/"><img class="next-cover" src="https://img.issey.top/img/logo.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【NLP实战】基于Bert和双向LSTM的情感分类【下篇】</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/ceedbae09481/" title="【NLP实战】基于Bert和双向LSTM的情感分类【上篇】"><img class="cover" src="https://img.issey.top/img/logo2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-10</div><div class="title">【NLP实战】基于Bert和双向LSTM的情感分类【上篇】</div></div></a></div><div><a href="/article/b9ce6b3f66fa/" title="【NLP实战】基于Bert和双向LSTM的情感分类【中篇】"><img class="cover" src="https://img.issey.top/img/logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-10</div><div class="title">【NLP实战】基于Bert和双向LSTM的情感分类【中篇】</div></div></a></div><div><a href="/article/4b6ff6ea27bb/" title="【NLP实战】基于Bert和双向LSTM的情感分类【下篇】"><img class="cover" src="https://img.issey.top/img/logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">【NLP实战】基于Bert和双向LSTM的情感分类【下篇】</div></div></a></div><div><a href="/article/5bd830bf3404/" title="【NLP】多标签分类【下】"><img class="cover" src="https://img.issey.top/img/logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-08</div><div class="title">【NLP】多标签分类【下】</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://img.issey.top/img/touxiang.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">issey</div><div class="author-info__description">建议通过邮件或评论与站长取得联系,通过Crisp联系时,站长不一定能及时看到。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_52466006"><i></i><span>CSDN(Twilight Sparkle)</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/iceissey" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_52466006" target="_blank" title="CSDN"><i class="fas fa-c"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1013813363&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1013813363@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">装修中...如果遇到公式和模块加载不出来,可以返回首页刷新。本站为双线部署,通常来说不需要加速。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E4%B8%8E%E7%9B%B8%E5%85%B3%E9%93%BE%E6%8E%A5"><span class="toc-number">2.</span> <span class="toc-text">个人博客与相关链接</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BB%BB%E5%8A%A1%E8%AF%B4%E6%98%8E"><span class="toc-number">3.</span> <span class="toc-text">实验数据与任务说明</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">多标签分类任务与相关算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B"><span class="toc-number">4.1.</span> <span class="toc-text">多标签分类任务简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">相关算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">多标签分类评估方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87accuracy"><span class="toc-number">5.0.1.</span> <span class="toc-text">1. 准确率（Accuracy）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%BE%E7%A1%AE%E5%BA%A6precision--%E5%BE%AE%E8%A7%82%E5%B9%B3%E5%9D%87micro-average"><span class="toc-number">5.0.2.</span> <span class="toc-text">2. 精确度（Precision）- 微观平均（Micro-average）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E%E7%8E%87recall--%E5%BE%AE%E8%A7%82%E5%B9%B3%E5%9D%87micro-average"><span class="toc-number">5.0.3.</span> <span class="toc-text">3. 召回率（Recall）- 微观平均（Micro-average）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#f1-%E5%88%86%E6%95%B0f1-score--%E5%BE%AE%E8%A7%82%E5%B9%B3%E5%9D%87micro-average"><span class="toc-number">5.0.4.</span> <span class="toc-text">4. F1 分数（F1 Score）- 微观平均（Micro-average）</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83"><span class="toc-number">6.1.</span> <span class="toc-text">实验环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4"><span class="toc-number">6.2.</span> <span class="toc-text">实验步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%82%E5%AF%9F%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">6.2.1.</span> <span class="toc-text">数据观察与预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%82%E5%AF%9F"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">数据观察</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">6.2.1.2.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86"><span class="toc-number">6.2.1.3.</span> <span class="toc-text">代码部分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E9%98%B6%E6%AE%B5"><span class="toc-number">6.2.2.</span> <span class="toc-text">词嵌入阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#xl-net%E5%B5%8C%E5%85%A5"><span class="toc-number">6.2.2.1.</span> <span class="toc-text">XL-NET嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="toc-number">6.2.2.2.</span> <span class="toc-text">嵌入生成过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%E7%9A%84%E5%86%B3%E5%AE%9A"><span class="toc-number">6.2.2.3.</span> <span class="toc-text">不进行微调的决定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86-1"><span class="toc-number">6.2.2.4.</span> <span class="toc-text">代码部分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5"><span class="toc-number">6.2.3.</span> <span class="toc-text">模型训练与测试阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">6.2.3.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.3.2.</span> <span class="toc-text">多标签分类方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="toc-number">6.2.3.3.</span> <span class="toc-text">性能评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86-2"><span class="toc-number">6.2.3.4.</span> <span class="toc-text">代码部分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">6.2.3.5.</span> <span class="toc-text">结果分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A2%E7%B4%A2--%E4%BD%BF%E7%94%A8svm%E7%9A%84%E6%A0%87%E7%AD%BE%E5%B9%82%E9%9B%86%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.4.</span> <span class="toc-text">进一步探索--使用SVM的标签幂集方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1"><span class="toc-number">6.2.4.1.</span> <span class="toc-text">实验设计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0"><span class="toc-number">6.2.4.2.</span> <span class="toc-text">训练和评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86-3"><span class="toc-number">6.2.4.3.</span> <span class="toc-text">代码部分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90-1"><span class="toc-number">6.2.4.4.</span> <span class="toc-text">结果分析</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/article/b4786567229c/" title="【23-24年】年度总结与迎新引荐"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【23-24年】年度总结与迎新引荐"></a><div class="content"><a class="title" href="/article/b4786567229c/" title="【23-24年】年度总结与迎新引荐">【23-24年】年度总结与迎新引荐</a><time datetime="2024-09-16T10:22:30.000Z" title="发表于 2024-09-16 18:22:30">2024-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/3e500d72817d/" title="【Python日志功能】二.高级配置与日志处理器"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【Python日志功能】二.高级配置与日志处理器"></a><div class="content"><a class="title" href="/article/3e500d72817d/" title="【Python日志功能】二.高级配置与日志处理器">【Python日志功能】二.高级配置与日志处理器</a><time datetime="2024-09-16T04:08:34.000Z" title="发表于 2024-09-16 12:08:34">2024-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/1ac2215b60cb/" title="【Python日志功能】一.日志基础与基本配置"><img src="https://img.issey.top/img/logo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【Python日志功能】一.日志基础与基本配置"></a><div class="content"><a class="title" href="/article/1ac2215b60cb/" title="【Python日志功能】一.日志基础与基本配置">【Python日志功能】一.日志基础与基本配置</a><time datetime="2024-09-15T14:28:15.000Z" title="发表于 2024-09-15 22:28:15">2024-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/5bd830bf3404/" title="【NLP】多标签分类【下】"><img src="https://img.issey.top/img/logo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【NLP】多标签分类【下】"></a><div class="content"><a class="title" href="/article/5bd830bf3404/" title="【NLP】多标签分类【下】">【NLP】多标签分类【下】</a><time datetime="2024-04-08T10:58:07.000Z" title="发表于 2024-04-08 18:58:07">2024-04-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/508dd4f67dbd/" title="【NLP】多标签分类【上】"><img src="https://img.issey.top/img/logo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【NLP】多标签分类【上】"></a><div class="content"><a class="title" href="/article/508dd4f67dbd/" title="【NLP】多标签分类【上】">【NLP】多标签分类【上】</a><time datetime="2024-01-10T11:32:32.000Z" title="发表于 2024-01-10 19:32:32">2024-01-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://img.issey.top/img/backhead.png)"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2024 <i id="heartbeat" class="fa fas fa-heartbeat"></i> By issey</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">听雨滴屋檐.<p><a target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a>&nbsp;<a target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p><div><a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral">本网站由<img src="https://img.issey.top/img/202209191239745.png" width="45px">提供CDN加速/云存储服务</a></div></div><img src="https://img.issey.top/img/o_1dfilp8ruo521thr1hvf18ji17soa.png"> <a href="https://wap.miit.gov.cn/index.html" style="color:#f72b07" target="_blank">蜀ICP备2022008043号-1</a></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage" href="/random/index.html"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://fastly.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://fastly.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.2},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],insertScript:[200,()=>{document.querySelectorAll("mjx-container:not([display])").forEach(t=>{const e=t.parentNode;"li"===e.nodeName.toLowerCase()?e.parentNode.classList.add("has-jax"):e.classList.add("has-jax")})},"",!1]}}};const t=document.createElement("script");t.src="https://fastly.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>(()=>{const t=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://comment.issey.top/",region:"",onCommentLoaded:function(){btf.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null))},o=()=>{"object"!=typeof twikoo?getScript("https://fastly.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js").then(t):setTimeout(t,0)};o()})()</script></div><script src="/js/sakura.js"></script><script defer src="https://fastly.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer src="https://fastly.jsdelivr.net/npm/hexo-theme-volantis@latest/source/js/issues.min.js"></script><script defer id="fluttering_ribbon" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer color="249,204,226" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="18px" data-random="false" async></script><script>window.$crisp=[],window.CRISP_WEBSITE_ID="4ca4b1aa-4161-47b6-80b2-f8cf05e635cf",d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s),$crisp.push(["safe",!0]),$crisp.push(["do","chat:hide"]),$crisp.push(["on","chat:closed",function(){$crisp.push(["do","chat:hide"])}]);var chatBtnHide,chatBtnShow,chatBtnFn=()=>{document.getElementById("chat_btn").addEventListener("click",(function(){$crisp.push(["do","chat:show"]),$crisp.push(["do","chat:open"])}))};chatBtnFn()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>