<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>机器学习笔记6——【支持向量机1】支持向量机的原理与推导 | issey的博客</title><meta name="keywords" content="最大间隔分离超平面,硬间隔与软间隔,松弛因子,线性和非线性支持向量机的原理与推导,合页损失函数,核函数,对偶问题与KKT条件,核函数戏法"><meta name="author" content="issey"><meta name="copyright" content="issey"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="最大间隔分离超平面,硬间隔与软间隔,松弛因子,线性和非线性支持向量机的原理与推导,合页损失函数,核函数,对偶问题与KKT条件,核函数戏法。"><meta property="og:type" content="article"><meta property="og:title" content="机器学习笔记6——【支持向量机1】支持向量机的原理与推导"><meta property="og:url" content="https://blog.issey.top/article/1c4151e0792a/index.html"><meta property="og:site_name" content="issey的博客"><meta property="og:description" content="最大间隔分离超平面,硬间隔与软间隔,松弛因子,线性和非线性支持向量机的原理与推导,合页损失函数,核函数,对偶问题与KKT条件,核函数戏法。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://img.issey.top/img/logo2.png"><meta property="article:published_time" content="2022-08-07T17:43:11.000Z"><meta property="article:modified_time" content="2022-09-21T03:45:03.523Z"><meta property="article:author" content="issey"><meta property="article:tag" content="监督学习"><meta property="article:tag" content="分类算法"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://img.issey.top/img/logo2.png"><link rel="shortcut icon" href="https://img.issey.top/img/logo.png"><link rel="canonical" href="https://blog.issey.top/article/1c4151e0792a/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"NTF3ZGK1TF",apiKey:"4d3d88863c6c78ac79547d0f743e6b5e",indexName:"issey",hits:{per_page:10},languages:{input_placeholder:"搜索文章",hits_empty:"找不到您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，用时 ${time} 毫秒"}},localSearch:{path:"/search.xml",preload:!1,languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:800},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:50,languages:{author:"作者: issey",link:"链接: ",source:"来源: issey的博客",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://fastly.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js",css:"https://fastly.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"机器学习笔记6——【支持向量机1】支持向量机的原理与推导",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-09-21 11:45:03"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="/css/rightMenu.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://img.issey.top/img/touxiang.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推荐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://img.issey.top/img/backhead.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">issey的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推荐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习笔记6——【支持向量机1】支持向量机的原理与推导</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-07T17:43:11.000Z" title="发表于 2022-08-08 01:43:11">2022-08-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-21T03:45:03.523Z" title="更新于 2022-09-21 11:45:03">2022-09-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="机器学习笔记6——【支持向量机1】支持向量机的原理与推导"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>前言：以下为支持向量机学习笔记，参考教程：</p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1qf4y1x7kB?p=1&amp;vd_source=747540861ba5c41c17852ccf069029f5">(强推)浙江大学-机器学习_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/7624837">支持向量机通俗导论（理解SVM的三层境界）</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ca411M7KA?p=1&amp;vd_source=747540861ba5c41c17852ccf069029f5">【太...完整了！】上海交大和腾讯强强联合的机器学习与深度学习课程分享！-人工智能/AI/神经网络_哔哩哔哩_bilibili</a></p></blockquote><h1 id="线性可分与线性不可分">线性可分与线性不可分</h1><p>    简单来说，线性可分就是可以用线性函数将两类样本分开。在二维中，表现为一条直线，在三维中为一个平面，而更高维中为超平面。如果不存在这样的线性函数，则为线性不可分。</p><p><img src="https://img.issey.top/img/202209211133192.png"></p><p>    事实：如果一个数据集是线性可分的，那一定存在无数多个超平面将各个类别分开。</p><h1 id="支持向量机support-vector-machine">支持向量机（Support Vector Machine）</h1><p>    支持向量机（简称SVM）最初是一种解决二分类的有监督学习算法，SVM的目的为：在给定两类样本的数据集的前提下，寻找一个将两类样本分隔开的超平面，并且使得两类样本之间的边界间隔(margin)最大化。最终得到的超平面被称为决策边界(decision boundary)。</p><p>    示例（二维）：</p><p><img src="https://img.issey.top/img/202209211134546.png"></p><p>    上面的三个超平面都可以将不同类别的样本分开，但是哪个是最好的呢？</p><p>    如果这里判断超平面“好坏”的标准为哪条直线对样本误差的容忍程度最高，那么直线2显然是最好的。支持向量机就是基于最优化理论，来寻找线2的算法。</p><p>    注意：在支持向量机中，样本输出值都是-1或1。</p><h1 id="最大间隔分离超平面">最大间隔分离超平面</h1><p><img src="https://img.issey.top/img/202209211134871.png"></p><p>    <strong>上面那个图可能不太标准，红圈圈住的样本应该刚好在虚线上才对！</strong></p><p>    我们假定两类数据中间有一个超平面，将这个超平面向两边平移，直到刚好擦过样本为止（图中两条虚线），我们定义这两个超平面刚好经过的训练样本为这个数据集的<strong>支持向量</strong>(图中红圈所示样本)，把这两个超平面中间的距离叫做<strong>间隔（margin）</strong>。支持向量机要找的是使间隔最大的那个超平面。并且，求得的超平面只能有一个，所以这个超平面应该处于上线两超平面的中间，即到支持向量距离相等。</p><p>    于是，支持向量机寻找的最优分类超平面应该满足：</p><ol type="1"><li><p>该超平面分开了两类</p></li><li><p>该超平面最大化了间隔</p></li><li><p>该超平面处于间隔的中间，到所有支持向量距离相等。</p></li></ol><h1 id="线性可分支持向量机硬间隔">线性可分支持向量机（硬间隔）</h1><h2 id="线性可分的数学定义">线性可分的数学定义</h2><p>    一个训练样本集<span class="math inline">\({(\vec x_i,y_i)},i = 1 \sim n\)</span> 线性可分，是指存在<span class="math inline">\((\vec w,b)\)</span> 使得：</p><p>    <span class="math inline">\(当 y_i = +1时，\vec w \cdot \vec x_i+b&gt;0,\)</span></p><p>    <span class="math inline">\(当y_i = -1时，\vec w \cdot \vec x_i +b&lt;0\)</span>.</p><p>    （注：有时会看到<span class="math inline">\(W^Tx_i+b\)</span>的形式，意思都一样的。）</p><h2 id="svm目标函数">SVM目标函数</h2><p>    假定训练样本集线性可分，那么支持向量机寻找的最大化间隔超平面为：</p><p>    已知训练样本集<span class="math inline">\({(\vec x_i,y_i)},i = 1\sim n,y_i = -1  or 1\)</span>；</p><p>    求解<span class="math inline">\((\vec w,b)\)</span>使得：</p><p>    <span class="math inline">\(最小化(Minimize):\frac 1 2||\vec w||^2\)</span></p><p>    <span class="math inline">\(限制条件：y_i(\vec w\cdot \vec x_i +b) \geq1,(i = 1 \sim n)\)</span></p><p>    其中<span class="math inline">\(||\vec w||^2\)</span> (向量w模的平方)为，<span class="math inline">\(||\vec w||^2 = w_1^2+w_2^2+...+w_m^2 = \sum_{i=1}^m w_i^2\)</span></p><p>    我们可以看出，需要求的目标函数其实是凸优化（Convex Optimization）中的二次规划问题。关于目标函数求解，用的是<strong>拉格朗日乘子法</strong>以及<strong>拉格朗日对偶问题</strong>求解。拉格朗日乘子法和对偶问题暂时不叙述。这里直接使用凸优化求解包进行求解。</p><h3 id="目标函数推导过程">目标函数推导过程</h3><blockquote><p>事实1：</p><p><span class="math inline">\(\vec w \cdot \vec x + b = 0\)</span>与<span class="math inline">\(a(\vec w\cdot \vec x+b) = 0，(a \neq 0)\)</span> 表示同一个超平面。</p><p>事实2：一个点<span class="math inline">\(X_0\)</span>到超平面<span class="math inline">\(\vec w\cdot \vec x+b = 0\)</span>的距离为：</p><p><span class="math inline">\(d = \frac{|\vec w\cdot \vec x_0+b|}{||\vec w||}\)</span></p></blockquote><p>    假设我们已知最终要求的超平面为：<span class="math inline">\(\vec w\cdot \vec x+b\)</span> ，因为这个超平面在间隔最大的两个平行的超平面正中间，而且上下两个超平面都经过支持向量，<strong>所以支持向量到所求超平面的距离应该都是相同的。</strong></p><p>    于是，我们可以根据事实1，将<span class="math inline">\((\vec w,b)\)</span> 放缩为<span class="math inline">\((a\vec w,ab)\)</span> ,最终使得：</p><p>    在支持向量<span class="math inline">\(X_0\)</span>上有<span class="math inline">\(|\vec w\cdot \vec x_0+b| = 1\)</span>；</p><p>    那么显而易见在非支持向量上有<span class="math inline">\(|\vec w\cdot \vec x_0+b|&gt;1\)</span>。</p><p>   （有点难懂，我的理解是a对于每一个支持向量都是一个不同的值，使其满足上述条件，也就是说a并不是一个定值。但是无论a怎么变，都表示的同一个超平面，所以对后续没有影响。至于非支持向量上为什么绝对值都大于1，是因为事实2，因为支持向量离超平面距离是最近的，所以分母相同的情况下，非支持向量作为分子自然就更大。）</p><p>    变换后，根据事实2，支持向量<span class="math inline">\(X_0\)</span>到超平面的距离将会变为：<span class="math inline">\(d = \frac{|\vec w\cdot \vec x_0+b|}{||\vec w||} = \frac {1}{||\vec w||}\)</span></p><p>    我们的目标是使支持向量到超平面的距离最大，也就是<span class="math inline">\(maximize(\frac 1 {||\vec w||})\)</span></p><p>    又因<span class="math inline">\(maximize(\frac 1 {||\vec w||}) = minimize(||\vec w||)\)</span></p><p>    于是我们将问题优化的目标函数定为：<span class="math inline">\(最小化(Minimize):\frac 1 2||\vec w||^2\)</span></p><p>    而 最小化<span class="math inline">\(\frac 1 2||\vec w||^2\)</span> 与 最小化$||w|| $ 是完全等价的。之所以写成这种形式，是为了后续求导更加方便。</p><p>    同时，因为我们将<span class="math inline">\((\vec w,b)\)</span>放缩为<span class="math inline">\((a\vec w,ab)\)</span> ,我们可以得到限制条件：</p><p>    <span class="math inline">\(y_i(\vec w\cdot \vec x_i +b) \geq1,(i = 1 \sim n)\)</span></p><p>    其中，<span class="math inline">\(y_i = -1  or  1\)</span></p><h2 id="凸优化中的二次规划">凸优化中的二次规划</h2><ol type="1"><li><p>目标函数为二次项</p></li><li><p>限制条件是一次项</p></li></ol><p>    因为我们的目标函数<span class="math inline">\(\frac 1 2||\vec w||^2 = \frac 1 2(w_1^2+w_2^2+...+w_m^2)\)</span>为二次项,限制条件<span class="math inline">\(y_i(\vec w\cdot \vec x_i +b) \geq1,(i = 1 \sim n)\)</span>为一次项，所以满足二次规划。</p><p>   凸优化的二次规划问题要么无解，要么只有唯一最小值解。于是，我们就可以用梯度下降算法求解啦！另外，只要一个优化问题是凸的，我们总能找到高效快速的算法去解决它。<strong>线性可分条件下的支持向量机是凸优化问题</strong>，因此能迅速找到高效的算法解决。不过我们不会详细探讨求解凸优化问题，关于凸优化求解是一门专门的课程，有兴趣可以学习《凸优化理论》这门课程。</p><h1 id="线性支持向量机软间隔">线性支持向量机（软间隔）</h1><h2 id="硬间隔与软间隔">硬间隔与软间隔</h2><p>硬间隔：间隔内不存在样本。训练集完全分类正确，损失函数不存在，损失值为0。也就是说，找到的超平面完全分离两类。上述都是硬间隔。硬间隔容易受到极端值影响，泛化能力不强，于是我们提出了软间隔。</p><p>软间隔：间隔内允许样本存在。允许一定量的样本分类错误，不过这些错误样本范围不会超过间隔区间。软间隔是硬间隔SVM的拓展版本。</p><h2 id="松弛因子">松弛因子</h2><blockquote><p>注：因为线性支持向量机模拟出的直线允许误差存在，所以根据线性可分的定义，线性支持向量机其实属于线性不可分。</p></blockquote><p>若数据线性不可分，则增加松弛因子<span class="math inline">\(\zeta_i \geq0\)</span> ,使函数间隔加上松弛变量大于等于1。于是，</p><p>约束条件变为:<span class="math inline">\(y_i(\vec w\cdot \vec x_i+b) \geq 1-\zeta_i\)</span></p><p>目标函数变为：<span class="math inline">\(minimize_{w,b}(\frac 1 2||\vec w||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>其中，C为惩罚因子，是为了防止松弛因子过大加入的一个代价。当C等于无穷大，只有当<span class="math inline">\(\zeta_i = 0\)</span>时才有最小值，因此，当C为无穷大时，退化为线性可分支持向量机。</p><p>目标函数求解依然是代入拉格朗日乘子，转化为对偶问题并求解。</p><h1 id="合页损失函数hinge-loss-function">合页损失函数（hinge loss function）</h1><p>公式：<span class="math inline">\(L(y(\vec w\cdot \vec x+b)) = [1-y(\vec w\cdot \vec x+b)]_+\)</span></p><p>下标“+”表示以下情况取正值：</p><p><span class="math inline">\([z]_+ =\left\{\begin{aligned} z, z&gt;0 \\ 0,z\leq0\end{aligned} \right.\)</span></p><p>当函数间隔<span class="math inline">\(y_i(\vec w\cdot \vec x+b)&gt;1\)</span>时，即当分类正确并在（软）间隔之外时，损失为0。否则损失为<span class="math inline">\(1-y_i(\vec w\cdot \vec x+b)\)</span></p><p><img src="https://img.issey.top/img/202209211135657.jpg"></p><p>当样本正确分类，<span class="math inline">\(y_i(\vec w\cdot \vec x+b)&gt;0\)</span>,反之小于0。</p><p><span class="math inline">\(|y_i(\vec w\cdot \vec x+b)|\)</span> 表示样本与决策边界的距离。绝对值越大，距离决策边界越远。</p><p>于是：</p><p>当<span class="math inline">\(y_i(\vec w\cdot \vec x+b)&gt;0\)</span>,即分类正确情况下，距离决策边界越远区分程度越好。</p><p>当<span class="math inline">\(y_i(\vec w\cdot \vec x+b)&lt;0\)</span>,即分类错误情况下，距离决策边界越远区分程度越差。</p><h2 id="svm的损失函数">SVM的损失函数</h2><p>SVM有另一种解释，即最小化以下目标函数：</p><p><span class="math inline">\(\sum_{i=1}^N[1-y_i(\vec w\cdot \vec x_i+b)]_++\lambda||\vec w||^2\)</span></p><p>这里不提供相关证明，详情见文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/lynn_001/article/details/84198007">线性支持向量机-合页损失函数(Hinge Loss)_搏击俱乐部_的博客-CSDN博客_支持向量机损失函数</a></p><p>也就是说，SVM目标函数实际上就是合页损失函数加上<span class="math inline">\(\lambda||\vec w||^2\)</span>。</p><h1 id="非线性支持向量机">非线性支持向量机</h1><h2 id="将特征空间从低位映射到高维">将特征空间从低位映射到高维</h2><p>当遇到如下图所示的非线性数据时，支持向量机的处理是将该训练集的特征从低维映射到高维，在高维仍然采用线性超平面对数据进行分类。</p><p><img src="https://img.issey.top/img/202209211135086.png"></p><p>现有以下假设：</p><p>假设1：在一个M维空间上随机取N个训练样本，随机的对每个训练样本赋予标签+1或-1，设这些训练样本线性可分的概率为<span class="math inline">\(P(M)\)</span>。那么当M趋于无穷大时，<span class="math inline">\(P(M)=1\)</span>。</p><p>这里略去该假设的证明。</p><p>也就是说，一个训练集在低维上不可分，但它到高维的映射将会是可分的。于是，支持向量机将训练样本由低维映射到高维以增加线性可分的概率。</p><p>我们设<span class="math inline">\(\phi(x)\)</span>为x在高维上的映射，那么假定<span class="math inline">\(\phi(x)\)</span>形式已知的条件下，引入松弛变量的目标函数将会变为：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vec w||^2+C\sum_{i=1}^N\zeta_i),    \zeta_i\geq0,(i=1\sim n)\)</span></p><p>限制条件：</p><p><span class="math inline">\(y_i[\vec w\cdot\phi(\vec x_i)+b]\geq 1-\zeta_i,(i=1 \sim n)\)</span></p><p>注意：这里的<span class="math inline">\(\vec w\)</span> 是与高维的<span class="math inline">\(\phi(\vec x)\)</span> 对应的。</p><p>我们可以看到，转化为高维后同样可以采用凸优化的二次规划求解。</p><h2 id="核函数kernel-function">核函数（Kernel Function）</h2><p>注意：接下来向量将会用<span class="math inline">\(X\)</span> 表示，向量点乘则变为矩阵乘法，例如：</p><p><span class="math inline">\(\vec x_1\cdot\vec x_2\)</span>将变为<span class="math inline">\(X_1^TX_2\)</span> 。</p><p>根据低维映射到高维的规则，重点在于如何找到<span class="math inline">\(\phi(X)\)</span> 使得线性不可分训练集在高维线性可分。实际上，我们可以不用知道 <span class="math inline">\(\phi(X)\)</span>的具体形式。取而代之，如果对于任意两个向量<span class="math inline">\(X_1,X_2\)</span>,有<span class="math inline">\(K(X_1,X_2) = \phi(X_1)^T\phi(X_2)\)</span> ,那么我们仍然可以通过一些技巧完成测试样本的预测。</p><p>我们定义<span class="math inline">\(K(X_1,X_2)\)</span> 为核函数。易得，核函数是一个实数。</p><p>可以证明，<span class="math inline">\(K(X_1,X_2)\)</span>与<span class="math inline">\(\phi(X_1),\phi(X_2)\)</span> 是一一对应的关系，证明略。另外，核函数必须满足以下条件才能写成两个向量内积的形式：</p><p><span class="math inline">\(K(X_1,X_2)\)</span>能写成<span class="math inline">\(\phi(X_1)^T\phi(X_2)\)</span> 的充要条件：</p><ol type="1"><li><p><span class="math inline">\(K(X_1,X_2) = K(X_2,X_1)\)</span>,即交换性</p></li><li><p>$C_i(i=1N),N有<em>{i=1}^N</em>{j=1}^NC_iC_jK(X_iX_j) $ ,即半正定性</p></li></ol><p>接下来，我们将研究如何在已知<span class="math inline">\(K(X_1,X_2)\)</span>而不知道<span class="math inline">\(\phi(X)\)</span>的条件下求解支持向量机的目标函数。</p><h2 id="对偶问题与kkt条件">对偶问题与KKT条件</h2><h3 id="原问题与对偶问题">原问题与对偶问题</h3><p>原问题（Prime problem）定义：</p><p>    最小化（Minimize）：<span class="math inline">\(f(w)\)</span></p><p>    限制条件（Subject to）： <span class="math display">\[ \begin{split}&amp;g_i(w) \leq0 ,i=1\sim K\\&amp;h_i(w) = 0,i=1\sim m\end{split} \]</span> 注：自变量为<span class="math inline">\(w\)</span>,目标函数是<span class="math inline">\(f(w)\)</span>，限制条件：有K个不等式，分别用<span class="math inline">\(g_i(w)\)</span> 来表示，等式有m个，分别用<span class="math inline">\(h_i(m)\)</span> 表示 。</p><p>为了定义对偶问题，我们先定义一个函数：</p><p><span class="math inline">\(L(w,a,\beta) = f(w)+a^Tg(w)+\beta^Th(w)\)</span></p><p>其中，</p><p><span class="math inline">\(a = [a_1,a_2,...,a_K]^T\)</span>,</p><p><span class="math inline">\(\beta = [\beta_1,\beta_2,...\beta_M]^T\)</span>,</p><p><span class="math inline">\(g(w) = [g_1(w),g_2(w),...,g_K(w)]^T\)</span></p><p><span class="math inline">\(h(w) = [h_1(w),h_2(w),...,h_M(w)]^T\)</span></p><p>然后，定义对偶问题如下：</p><p><span class="math inline">\(最大化：\theta(a,\beta) = inf  L(w,a,\beta),所有定义域内的w\)</span>    </p><p>限制条件：<span class="math inline">\(a_i \geq0,i=1\sim K\)</span></p><p>对偶问题是：最大化<span class="math inline">\(\theta(a,\beta)\)</span>, 它等于<span class="math inline">\(L(w,a,\beta)\)</span> 去遍历所有定义域上的<span class="math inline">\(w\)</span>找到使<span class="math inline">\(L(w,a,\beta)\)</span>最小的那个<span class="math inline">\(w\)</span>，同时将求得的<span class="math inline">\(L(w,a,\beta)\)</span> 赋值为<span class="math inline">\(\theta(a,\beta)\)</span>。注意限制条件。</p><p>联合原问题与对偶问题，有以下定理：</p><p>定理1：若<span class="math inline">\(w^*\)</span>是原问题的解，<span class="math inline">\((a^*,\beta^*)\)</span>是对偶问题的解，那么：<span class="math inline">\(f(w^*)\geq \theta(a^*,\beta^*)\)</span></p><p>证明略。</p><p>这个定理说明：原问题的解<span class="math inline">\(f(w^*)\)</span>总是大于等于对偶问题的解<span class="math inline">\(\theta(a^*,\beta^*)\)</span> 。</p><p>我们将<span class="math inline">\(f(w^*)- \theta(a^*,\beta^*)\)</span> 定义为对偶差距(Doality Gap)。根据定理1，对偶差距大于等于0。</p><h3 id="强对偶定理">强对偶定理</h3><p>如果<span class="math inline">\(g(w) = Aw+b,h(w)=Cw+d,f(w)\)</span>为凸函数，则有<span class="math inline">\(f(w^*) = \theta(a^*,\beta^*)\)</span>,对偶差距为0。</p><p>简单来说就是，<strong>如果原问题的目标函数是凸函数，而限制条件是线性函数</strong>那么<span class="math inline">\(f(w^*) = \theta(a^*,\beta^*)\)</span> 。证明略。</p><h3 id="kkt条件">KKT条件</h3><p>如果强对偶定理成立，即<span class="math inline">\(f(w^*) = \theta(a^*,\beta^*)\)</span> ,则定理1中必然能推出：对于所有的<span class="math inline">\(i=1\sim K\)</span> ,要么<span class="math inline">\(a_i = 0\)</span>，要么<span class="math inline">\(g_i(w^*) = 0\)</span>。这个条件被称为KKT条件。</p><h2 id="将支持向量机目标函数转化为对偶问题并求解">将支持向量机目标函数转化为对偶问题并求解</h2><h3 id="目标函数转化为原问题形式">目标函数转化为原问题形式</h3><p>回顾一下现在的支持向量机目标函数：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vec w||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>限制条件：</p><p><span class="math inline">\(\zeta_i\geq0,(i=1\sim n)\)</span></p><p><span class="math inline">\(y_i[\vec w\cdot\phi(\vec x_i)+b]\geq 1-\zeta_i,(i=1 \sim n)\)</span></p><p>对比原问题(Prime problem)的形式：</p><p>最小化（Minimize）：<span class="math inline">\(f(w)\)</span></p><p>限制条件（Subject to）: <span class="math display">\[ \begin{split}&amp;g_i(w) \leq0 ,i=1\sim K\\&amp;h_i(w) = 0,i=1\sim m\end{split} \]</span> 注意到，原问题中不等式<span class="math inline">\(g_i(w) \leq0\)</span>,而支持向量机的限制条件中两个不等式都是大于等于0的。所以我们要先将支持向量机中的限制条件转为小于等于。</p><ul><li><p>将<span class="math inline">\(\zeta_i\)</span> 转为相反数</p></li><li><p>展开并化简第二个不等式</p></li></ul><p>于是，目标函数将变为：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vec w||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>限制条件：</p><p><span class="math inline">\(\zeta_i\leq0,(i=1\sim n)\)</span></p><p><span class="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib\leq0(i=1\sim N)\)</span></p><p>因为目标函数是凸的，而其限制条件都是线性函数，所以满足强对偶定理。</p><h3 id="利用对偶定理求解">利用对偶定理求解</h3><p>现在，对偶问题中的<span class="math inline">\(w\)</span> 就是这里的<span class="math inline">\((w,b,\zeta_i)\)</span>,而不等式<span class="math inline">\(g_i(w)\leq 0\)</span>是这里限制条件（两部分）：</p><p><span class="math inline">\(\zeta_i\leq0,(i=1\sim n)\)</span></p><p><span class="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib\leq0(i=1\sim N)\)</span></p><p>另外，因为限制条件不存在等式，所以不存在对偶问题中的<span class="math inline">\(h_i(w)\)</span>。</p><p>然后，对偶问题可以写成如下形式：</p><p><span class="math inline">\(最大化：\theta(a,\beta) = inf_{w,\zeta,b}\{\frac 1 2||w||^2-C\sum_{i=1}^N\beta_i\zeta_i+\sum_{i=1}^Na_i[1+\zeta_i-y_iw^T\phi(X_i)-y_ib]\}\)</span></p><p>限制条件：</p><ol type="1"><li><span class="math inline">\(a_i\geq0\)</span></li></ol><p>(2)<span class="math inline">\(\beta_i\geq0\)</span></p><h4 id="如何将原目标函数转化为对偶问题">如何将原目标函数转化为对偶问题</h4><p>先对<span class="math inline">\((w,b,\zeta_i)\)</span>求导并令导数为0：</p><ol type="1"><li><p><span class="math inline">\(\frac {\partial \theta}{\partial w} = 0\)</span>推出<span class="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span></p></li><li><p><span class="math inline">\(\frac {\partial \theta}{\partial \zeta_i} = 0\)</span>推出<span class="math inline">\(a_i+\beta_i=C\)</span></p></li><li><p><span class="math inline">\(\frac {\partial \theta}{\partial b} = 0\)</span>推出<span class="math inline">\(\sum_{i=1}^Na_iy_i=0\)</span></p></li></ol><p>(详细过程略)</p><p>于是，可以将支持向量机原目标函数化为以下对偶问题：</p><p><span class="math inline">\(最大化：\theta(a,\beta) = \sum_{i=1}^Na_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_ja_ia_j\phi(X_i)^T\)</span></p><p>限制条件：</p><p>(1)  <span class="math inline">\(0 \leq a_i\leq C,(i=1\sim N)\)</span></p><p>(2)<span class="math inline">\(\sum_{i=1}^Na_iy_i=0,(i=1\sim N)\)</span></p><p>可以看出，这个对偶问题也是一个凸优化的二次规划问题，可以通过最优化算法快速求解。（利用凸优化包）</p><h3 id="如何求解这个对偶问题">如何求解这个对偶问题</h3><p>由于<span class="math inline">\(K(X_i,X_j) = \phi(X_i)^T\phi(X_j)\)</span> ,所以我们只需要知道核函数，就可以求解这个对偶问题了。当我们求解了这个对偶问题，解出了所有的<span class="math inline">\(a_i\)</span> 。我们可以继续观察<span class="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span>,因为<span class="math inline">\(\phi(X_i)\)</span> 不具有显式表达，所以<span class="math inline">\(w\)</span>也不具有显式表达。但是我们可以推导：即使<span class="math inline">\(w\)</span>不具有显示表达，我们也可以通过核函数算出<span class="math inline">\(w^T\phi(X)+b\)</span>的值。</p><h4 id="首先如何求b">首先，如何求b</h4><p>由于<span class="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span> ,则</p><p><span class="math inline">\(w^T\phi(X_i) = \sum_{j=1}^Na_jy_j\phi(X_j)^T\phi(X_i) = \sum_{j=1}^Na_jy_jK(X_j,X_i)\)</span></p><p>其次，根据KKT条件，我们可以推出：</p><ol type="1"><li><p><span class="math inline">\(a_i[1+\zeta_i-y_iw^T\phi(X_i)-y_ib] = 0\)</span></p></li><li><p><span class="math inline">\(\beta_i\zeta_i=0\)</span>即<span class="math inline">\((c-a_i)\zeta_i = 0\)</span></p></li></ol><p>另外，如果对某个i，<span class="math inline">\(a_i \not= 0且a_i \not= c\)</span>，则根据上面KKT推出的两个公式必有<span class="math inline">\(\zeta_i = 0\)</span>,且<span class="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib = 0\)</span> 。</p><p>而这时<span class="math inline">\(y_iw^T\phi(X_i) = \sum_{j=1}^Na_iy_iy_jK(X_j,X_i)\)</span></p><p>所以，只需要找一个<span class="math inline">\(0&lt;a_i&lt;c\)</span> ，</p><p><span class="math inline">\(b = \frac{1-\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)}{y_i}\)</span></p><h4 id="如何求wtphixb-核函数戏法kernel-trick">如何求<span class="math inline">\(w^T\phi(X)+b\)</span>    —— 核函数戏法（Kernel Trick）</h4><p>将<span class="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span>代入得：</p><p><span class="math display">\[ \begin{split}w^T\phi(X)+b &amp;= w\\&amp;=\sum_{i=1}^Na_iy_i\phi(X_i)^T\phi(X)+b\\&amp; = \sum_{i=1}^Na_iy_iK(X_i,X)+b\end{split} \]</span></p><p>我们发现，即使不知道<span class="math inline">\(\phi(X)和w\)</span>的显式形式，也可以通过核函数求得<span class="math inline">\(w^T\phi(X)+b\)</span> ，这一结论被称为核函数戏法。</p><p>最后，我们可以用如下的判别标准来判定一个样本属于哪一类别：</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \geq 0\)</span>，那么<span class="math inline">\(X \in C_1\)</span> ;</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \leq 0\)</span>，那么<span class="math inline">\(X \in C_2\)</span> 。</p><h2 id="总结支持向量机训练和测试流程">总结：支持向量机训练和测试流程</h2><p><strong>训练过程：</strong></p><p>输入训练集${(X_i,y_i)},i=1N <span class="math inline">\(,其中，\)</span>y_i = +1或-1$ 。</p><p>接下来，求解如下目标函数：</p><p><span class="math inline">\(最大化：\theta(a,\beta) = \sum_{i=1}^Na_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_ja_ia_j\phi(X_i)^T\)</span></p><p>限制条件：</p><p>(1)<span class="math inline">\(0 \leq a_i\leq C,(i=1\sim N)\)</span></p><p>(2)<span class="math inline">\(\sum_{i=1}^Na_iy_i=0,(i=1\sim N)\)</span></p><p>求出<span class="math inline">\(a\)</span> 。</p><p>然后，求出b：</p><p>    找一个<span class="math inline">\(a_i \not= 0且a_i \not=c\)</span>,</p><p>    <span class="math inline">\(b = \frac{1-\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)}{y_i}\)</span></p><p>一旦求出了<span class="math inline">\(a，b\)</span>,就完成了支持向量机的训练过程。</p><p><strong>测试过程：</strong></p><p>给出一个测试数据X，预测它的类别y。</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \geq 0\)</span>，那么<span class="math inline">\(y = +1\)</span> ;</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b &lt; 0\)</span>，那么<span class="math inline">\(y = -1\)</span> ;</p><p>关于支持向量机的具体应用以及更多细节比如核函数的选择、超参数的控制等等将会在下一章支持向量机中进行说明。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.issey.top">issey</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.issey.top/article/1c4151e0792a/">https://blog.issey.top/article/1c4151e0792a/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.issey.top" target="_blank">issey的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">监督学习</a><a class="post-meta__tags" href="/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">分类算法</a></div><div class="post_share"><div class="social-share" data-image="https://img.issey.top/img/logo2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/2f3e74c9632f/"><img class="prev-cover" src="https://img.issey.top/img/logo.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习笔记7——决策树原理与应用</div></div></a></div><div class="next-post pull-right"><a href="/article/f84b08dc3a2c/"><img class="next-cover" src="https://img.issey.top/img/logo.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习笔记1——神经网络的搭建与简单应用</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/bba895870268/" title="机器学习笔记4——逻辑回归"><img class="cover" src="https://img.issey.top/img/logo2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-02</div><div class="title">机器学习笔记4——逻辑回归</div></div></a></div><div><a href="/article/2f3e74c9632f/" title="机器学习笔记7——决策树原理与应用"><img class="cover" src="https://img.issey.top/img/logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-02</div><div class="title">机器学习笔记7——决策树原理与应用</div></div></a></div><div><a href="/article/324dbb5b1fc5/" title="机器学习笔记9——KNN算法实现和应用"><img class="cover" src="https://img.issey.top/img/logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-06</div><div class="title">机器学习笔记9——KNN算法实现和应用</div></div></a></div><div><a href="/article/c4e174e36609/" title="机器学习笔记1——一元线性回归"><img class="cover" src="https://img.issey.top/img/logo2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-28</div><div class="title">机器学习笔记1——一元线性回归</div></div></a></div><div><a href="/article/92ab78771d6d/" title="机器学习笔记2——多元线性回归"><img class="cover" src="https://img.issey.top/img/logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-30</div><div class="title">机器学习笔记2——多元线性回归</div></div></a></div><div><a href="/article/31f58288ca8b/" title="机器学习笔记3——多项式回归"><img class="cover" src="https://img.issey.top/img/logo2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-30</div><div class="title">机器学习笔记3——多项式回归</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://img.issey.top/img/touxiang.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">issey</div><div class="author-info__description">学习笔记,现阶段主要集中于机器学习,但不仅限于机器学习</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_52466006"><i></i><span>CSDN(Twilight Sparkle)</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/iceissey" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_52466006" target="_blank" title="CSDN"><i class="fas fa-c"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1013813363&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1013813363@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">装修中...如果遇到公式和模块加载不出来,可以返回首页刷新。本站为双线部署,通常来说不需要加速。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E4%B8%8E%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86"><span class="toc-number">1.</span> <span class="toc-text">线性可分与线性不可分</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsupport-vector-machine"><span class="toc-number">2.</span> <span class="toc-text">支持向量机（Support Vector Machine）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E8%B6%85%E5%B9%B3%E9%9D%A2"><span class="toc-number">3.</span> <span class="toc-text">最大间隔分离超平面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%A1%AC%E9%97%B4%E9%9A%94"><span class="toc-number">4.</span> <span class="toc-text">线性可分支持向量机（硬间隔）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E7%9A%84%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="toc-number">4.1.</span> <span class="toc-text">线性可分的数学定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#svm%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text">SVM目标函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="toc-number">4.2.1.</span> <span class="toc-text">目标函数推导过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%AD%E7%9A%84%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92"><span class="toc-number">4.3.</span> <span class="toc-text">凸优化中的二次规划</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E8%BD%AF%E9%97%B4%E9%9A%94"><span class="toc-number">5.</span> <span class="toc-text">线性支持向量机（软间隔）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E9%97%B4%E9%9A%94%E4%B8%8E%E8%BD%AF%E9%97%B4%E9%9A%94"><span class="toc-number">5.1.</span> <span class="toc-text">硬间隔与软间隔</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%BE%E5%BC%9B%E5%9B%A0%E5%AD%90"><span class="toc-number">5.2.</span> <span class="toc-text">松弛因子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0hinge-loss-function"><span class="toc-number">6.</span> <span class="toc-text">合页损失函数（hinge loss function）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#svm%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">SVM的损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">7.</span> <span class="toc-text">非线性支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E4%BB%8E%E4%BD%8E%E4%BD%8D%E6%98%A0%E5%B0%84%E5%88%B0%E9%AB%98%E7%BB%B4"><span class="toc-number">7.1.</span> <span class="toc-text">将特征空间从低位映射到高维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0kernel-function"><span class="toc-number">7.2.</span> <span class="toc-text">核函数（Kernel Function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E4%B8%8Ekkt%E6%9D%A1%E4%BB%B6"><span class="toc-number">7.3.</span> <span class="toc-text">对偶问题与KKT条件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E9%97%AE%E9%A2%98%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">7.3.1.</span> <span class="toc-text">原问题与对偶问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%AF%B9%E5%81%B6%E5%AE%9A%E7%90%86"><span class="toc-number">7.3.2.</span> <span class="toc-text">强对偶定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kkt%E6%9D%A1%E4%BB%B6"><span class="toc-number">7.3.3.</span> <span class="toc-text">KKT条件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E5%B9%B6%E6%B1%82%E8%A7%A3"><span class="toc-number">7.4.</span> <span class="toc-text">将支持向量机目标函数转化为对偶问题并求解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%8E%9F%E9%97%AE%E9%A2%98%E5%BD%A2%E5%BC%8F"><span class="toc-number">7.4.1.</span> <span class="toc-text">目标函数转化为原问题形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E5%AF%B9%E5%81%B6%E5%AE%9A%E7%90%86%E6%B1%82%E8%A7%A3"><span class="toc-number">7.4.2.</span> <span class="toc-text">利用对偶定理求解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E5%8E%9F%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">7.4.2.1.</span> <span class="toc-text">如何将原目标函数转化为对偶问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3%E8%BF%99%E4%B8%AA%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">7.4.3.</span> <span class="toc-text">如何求解这个对偶问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A6%96%E5%85%88%E5%A6%82%E4%BD%95%E6%B1%82b"><span class="toc-number">7.4.3.1.</span> <span class="toc-text">首先，如何求b</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%B1%82wtphixb-%E6%A0%B8%E5%87%BD%E6%95%B0%E6%88%8F%E6%B3%95kernel-trick"><span class="toc-number">7.4.3.2.</span> <span class="toc-text">如何求\(w^T\phi(X)+b\)    —— 核函数戏法（Kernel Trick）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E6%B5%81%E7%A8%8B"><span class="toc-number">7.5.</span> <span class="toc-text">总结：支持向量机训练和测试流程</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/article/34b6e799e503/" title="【matlab图像处理】【应用型笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【matlab图像处理】【应用型笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法"></a><div class="content"><a class="title" href="/article/34b6e799e503/" title="【matlab图像处理】【应用型笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法">【matlab图像处理】【应用型笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法</a><time datetime="2022-10-16T12:15:42.000Z" title="发表于 2022-10-16 20:15:42">2022-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/3adde30b6e37/" title="【matlab图像处理】【应用型笔记3】【图像变换】（二）图像的形态学变换"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【matlab图像处理】【应用型笔记3】【图像变换】（二）图像的形态学变换"></a><div class="content"><a class="title" href="/article/3adde30b6e37/" title="【matlab图像处理】【应用型笔记3】【图像变换】（二）图像的形态学变换">【matlab图像处理】【应用型笔记3】【图像变换】（二）图像的形态学变换</a><time datetime="2022-10-16T12:15:42.000Z" title="发表于 2022-10-16 20:15:42">2022-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/962730553a9a/" title="【从FT到DFT和FFT】（三）从离散傅里叶变换到快速傅里叶变换"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【从FT到DFT和FFT】（三）从离散傅里叶变换到快速傅里叶变换"></a><div class="content"><a class="title" href="/article/962730553a9a/" title="【从FT到DFT和FFT】（三）从离散傅里叶变换到快速傅里叶变换">【从FT到DFT和FFT】（三）从离散傅里叶变换到快速傅里叶变换</a><time datetime="2022-10-15T09:38:34.000Z" title="发表于 2022-10-15 17:38:34">2022-10-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/5921f880c513/" title="【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换"></a><div class="content"><a class="title" href="/article/5921f880c513/" title="【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换">【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换</a><time datetime="2022-10-14T15:34:21.000Z" title="发表于 2022-10-14 23:34:21">2022-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/article/bdbeeb7f1cb2/" title="【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导"><img src="https://img.issey.top/img/logo2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导"></a><div class="content"><a class="title" href="/article/bdbeeb7f1cb2/" title="【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导">【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导</a><time datetime="2022-10-12T12:45:50.000Z" title="发表于 2022-10-12 20:45:50">2022-10-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://img.issey.top/img/backhead.png)"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 <i id="heartbeat" class="fa fas fa-heartbeat"></i> By issey</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">听雨滴屋檐.<p><a target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a>&nbsp;<a target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p><div><a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral">本网站由<img src="https://img.issey.top/img/202209191239745.png" width="45px">提供CDN加速/云存储服务</a></div></div><img src="https://img.issey.top/img/o_1dfilp8ruo521thr1hvf18ji17soa.png"> <a href="http://www.beian.gov.cn" style="color:#f72b07" target="_blank">蜀ICP备2022008043号-1</a></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage" href="/random/index.html"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://fastly.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://fastly.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.2},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],insertScript:[200,()=>{document.querySelectorAll("mjx-container:not([display])").forEach(t=>{const e=t.parentNode;"li"===e.nodeName.toLowerCase()?e.parentNode.classList.add("has-jax"):e.classList.add("has-jax")})},"",!1]}}};const t=document.createElement("script");t.src="https://fastly.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>(()=>{const t=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://comment.issey.top/",region:"",onCommentLoaded:function(){btf.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null))},o=()=>{"object"!=typeof twikoo?getScript("https://fastly.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js").then(t):setTimeout(t,0)};o()})()</script></div><script src="/js/sakura.js"></script><script defer src="https://fastly.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer src="https://fastly.jsdelivr.net/npm/hexo-theme-volantis@latest/source/js/issues.min.js"></script><script defer id="fluttering_ribbon" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer color="249,204,226" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="18px" data-random="false" async></script><script>window.$crisp=[],window.CRISP_WEBSITE_ID="4ca4b1aa-4161-47b6-80b2-f8cf05e635cf",d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s),$crisp.push(["safe",!0]),$crisp.push(["do","chat:hide"]),$crisp.push(["on","chat:closed",function(){$crisp.push(["do","chat:hide"])}]);var chatBtnHide,chatBtnShow,chatBtnFn=()=>{document.getElementById("chat_btn").addEventListener("click",(function(){$crisp.push(["do","chat:show"]),$crisp.push(["do","chat:open"])}))};chatBtnFn()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>