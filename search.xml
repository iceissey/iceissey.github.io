<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【NLP】多标签分类【下】</title>
      <link href="/article/5bd830bf3404/"/>
      <url>/article/5bd830bf3404/</url>
      
        <content type="html"><![CDATA[<h1 id="简介">简介</h1><p>在《【NLP】多标签分类》系列的上一篇文章中，我们深入探讨了三种机器学习方法：BinaryRelevance (BR)、Classifier Chains (CC) 以及 Label Powerset(LP)，旨在解决多标签分类的挑战。这些方法各展所长，为我们提供了不同角度解析和处理多标签问题的视角。继先前对这些机器学习方法的详尽分析之后，本篇文章转向更为先进的解决策略——专注于序列生成方法，并以Transformer模型的一种变体，即T5预训练模型为核心，进行实验探索。</p><p>本文将不仅详细介绍如何利用T5模型对多标签分类任务进行微调，而且还将通过实验对比，展现其相较于之前讨论的传统方法在性能上的优势和潜在应用价值。通过精心设计的实验和深入的结果分析，揭示序列生成方法特别是Transformer架构的强大能力和灵活性。</p><h1 id="个人博客与相关链接">个人博客与相关链接</h1><p>本文相关代码和数据集已同步上传github: <ahref="https://github.com/iceissey/issey_Kaggle/tree/main/MultiLabelClassification">issey_Kaggle/MultiLabelClassificationat main · iceissey/issey_Kaggle (github.com)</a></p><p>本文代码（Notebook）已公布至kaggle: <ahref="https://www.kaggle.com/code/isseyice/transformer-multi-label-classification">Transformer-Multi-Label-Classification(kaggle.com)</a></p><p>博主个人博客链接：<a href="https://www.issey.top/">issey的博客 -愿无岁月可回首</a></p><h1 id="实验数据与任务说明">实验数据与任务说明</h1><p>数据来源：<ahref="https://www.kaggle.com/datasets/shivanandmn/multilabel-classification-dataset/data">Multi-LabelClassification Dataset (kaggle.com)</a></p><p>任务说明：</p><ul><li>背景：NLP——多标签分类数据集。</li><li>内容：该数据集包含6个不同的标签（计算机科学、物理学、数学、统计学、定量生物学、定量金融），用于根据摘要和标题对研究论文进行分类。标签列中的值1表示该标签属于该论文，每篇论文可以有多个标签为1。</li></ul><p><img src="https://img.issey.top/img/202401101606632.png" /></p><h1 id="模型介绍">模型介绍</h1><h2 id="transformer">Transformer</h2><p>Transformer模型自从2017年由Vaswani等人在论文《Attention Is All YouNeed》中首次提出以来，已经证明了其在多种自然语言处理任务上的强大能力。尽管本文不会深入讲解Transformer的详细架构及其组成模块，我们仍然强烈推荐感兴趣的读者参考原始论文以获得全面的理解。</p><h3 id="transformer能做什么">Transformer能做什么？</h3><p>Transformer的创新之处在于其独特的自注意力机制，使其能够在处理文本时更有效地捕捉长距离依赖关系。这一特性不仅提高了处理速度，还提升了模型对文本的理解深度，打开了自然语言处理领域的新篇章。以下是Transformer在NLP领域的一些关键应用：</p><ul><li><strong>文本分类</strong>：Transformer能够理解复杂的文本结构和语义，使其在文本分类任务上表现优异，包括情感分析、主题识别等。</li><li><strong>机器翻译</strong>：由于其强大的语言模型能力，Transformer模型已成为机器翻译领域的主导技术，提供了更加流畅和准确的翻译结果。</li><li><strong>文本摘要</strong>：Transformer模型能够理解和提取文本的关键信息，生成准确且连贯的摘要，无论是抽取式还是生成式摘要。</li><li><strong>问答系统</strong>：利用其深度理解能力，Transformer能够从大量文本中提取答案，为问答系统提供强有力的支持。</li><li><strong>语言生成</strong>：Transformer的变体，如GPT系列，已经展示了在生成文本、编写代码等任务上的卓越能力，推动了创造性文本生成和自动编程的新发展。</li></ul><h2 id="hugging-face">Hugging Face</h2><p>在深入探讨如何将Transformer模型应用于多标签分类任务之前，让我们先了解一下HuggingFace。作为一个致力于推进机器学习技术民主化的开源社区和公司，HuggingFace为研究者和开发者们提供了丰富的预训练模型库及相关工具，极大地简化了NLP任务的开发流程。</p><p>官网链接：<a href="https://huggingface.co/">Hugging Face – The AIcommunity building the future.</a></p><h3 id="hugging-face的transformers库">Hugging Face的Transformers库</h3><p>作为一个广泛使用的Python库，HuggingFace的Transformers库集合了数百种预训练的Transformer模型，支持轻松应用于文本分类、文本生成、问答等多种NLP任务。该库的一个主要优势是其提供了统一的接口，让不同的Transformer模型，比如BERT、GPT-2、RoBERTa等，在几乎不需修改代码的情况下就能互相替换使用。</p><h3 id="社区支持和资源">社区支持和资源</h3><p>HuggingFace不仅提供预训练模型，还维护着一个充满活力的社区，社区成员在此分享经验、解决方案及最佳实践。这样的平台为初学者和专家提供了交流与学习的机会，进一步推动了NLP领域的发展。更进一步，HuggingFace也提供了模型共享平台，允许研究者和开发者上传及分享自己训练的模型，进一步增强了社区资源。</p><h3 id="预训练模型的应用">预训练模型的应用</h3><p>对于多标签分类任务而言，HuggingFace的Transformers库开辟了一个既简单又强大的途径，以便利用最先进的模型。用户可根据自身任务需求选择合适的预训练模型，并通过微调（fine-tuning）的方式使其适应具体的多标签分类任务，从而大幅度降低了模型开发和训练的时间及资源消耗。在接下来的部分中，我们会详细展示这一过程的实现，包括模型的选择、数据准备、训练以及性能评估等关键步骤。</p><h2 id="t5模型text-to-text-transfer-transformer">T5模型（Text-To-TextTransfer Transformer）</h2><p>本节将介绍我们在本次实验中使用的预训练模型T5，全称为Text-To-TextTransferTransformer。T5模型以其创新性著称，其设计理念是将所有自然语言处理（NLP）任务转化为一个统一的文本到文本的格式。这种独特的通用性使得T5成为解决多标签分类等复杂任务的理想选择。</p><h3 id="t5的核心理念">T5的核心理念</h3><p>T5模型的设计核心在于将各种NLP任务统一到一个简单的框架中：接受文本输入并产生文本输出。这意味着无论是进行文本分类、翻译，还是处理更为复杂的多标签分类和问答任务，T5模型都以相同的方法处理，极大地提升了模型的灵活性和适用范围。</p><h3 id="t5的架构和训练方法">T5的架构和训练方法</h3><p>T5遵循了经典的Encoder-Decoder架构，但在训练策略上进行了创新。它首先在大量文本数据上进行预训练，掌握语言的广泛知识，然后在特定任务的数据集上进行微调（fine-tuning）。这种结合预训练和微调的方法使T5在许多NLP任务上取得了卓越的表现。</p><h3 id="t5在多标签分类任务中的运用">T5在多标签分类任务中的运用</h3><p>在多标签分类任务中，T5模型将任务视为一个文本到文本的转换问题：它将文章内容作为输入，并输出一系列的标签作为分类结果。这种方法简化了任务的处理流程，并允许T5利用其预训练阶段学到的丰富语言知识，以提升任务的处理效率和分类准确性。</p><h1 id="实验步骤">实验步骤</h1><p>本实验的主要步骤包括：1）数据预处理。2）模型训练与测试。3）结果转化与评估。</p><h2 id="数据预处理">数据预处理</h2><p>正如前一节所述，T5模型以序列生成的形式处理任务，即接收文本输入并产生文本输出。因此，我们需要将原始数据转换成符合这一格式的形式，以便模型能够有效处理。以下是我们的原始数据格式示例：</p><table style="width:100%;"><colgroup><col style="width: 29%" /><col style="width: 16%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 9%" /><col style="width: 16%" /><col style="width: 16%" /></colgroup><thead><tr class="header"><th>文本内容</th><th>计算机科学</th><th>物理</th><th>数学</th><th>统计学</th><th>定量生物学</th><th>定量金融学</th></tr></thead><tbody><tr class="odd"><td>这是一个文本示例。</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr></tbody></table><p>为了将这些数据转换为适合序列生成任务的格式，我们需要将标签（即标记为1的类别）转化为一串文本标签，如下所示：</p><p>示例：</p><table><thead><tr class="header"><th>文本内容</th><th>标签</th></tr></thead><tbody><tr class="odd"><td>这是一个文本示例。</td><td>计算机科学;统计学</td></tr></tbody></table><p>注意：标签之间可以使用其他符号进行隔开，本例中使用的是分号（;）。我们的目标是将标记为1的标签拼接成一条文本数据，以便模型可以将这些标签作为生成任务的一部分来处理。</p><h2 id="模型训练与测试">模型训练与测试</h2><h3 id="模型选择">模型选择：</h3><p>模型名称：T5-Small</p><p>模型链接：<ahref="https://huggingface.co/google-t5/t5-small">google-t5/t5-small ·Hugging Face</a></p><h3 id="参数设置">参数设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">learning_rate = <span class="number">2e-5</span></span><br></pre></td></tr></table></figure><h2 id="结果转化与评估">结果转化与评估</h2><p>在使用T5模型完成多标签分类任务后，我们会得到模型生成的文本序列作为输出。这些输出序列以文本形式列出了预测的标签，例如：</p><table><thead><tr class="header"><th>预测标签</th></tr></thead><tbody><tr class="odd"><td>统计学;定量生物学;定量金融学</td></tr></tbody></table><p>为了对模型的性能进行评估，并使用我们在上篇文章中介绍的多标签分类评估方法，必须先将这些文本格式的标签转换回原始数据的格式，即将每个标签对应到它们各自的分类列上，并用0或1表示其是否被预测为该类。转换后的格式如下所示：</p><table><thead><tr class="header"><th>计算机科学</th><th>物理</th><th>数学</th><th>统计学</th><th>定量生物学</th><th>定量金融学</th></tr></thead><tbody><tr class="odd"><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>在这个转换过程中，我们首先将每个预测的标签字符串分割为单独的标签（在本例中，我们使用分号";"作为分隔符）。然后，我们检查每个原始标签列，并将其与分割后的标签进行匹配，如果预测中包含某个标签，则在相应的列中标记为1；如果不包含，则标记为0。这样，我们就能得到一个与原始数据格式相匹配的矩阵，便于我们采用上篇文章中介绍的评估方法来量化模型的性能。</p><h1 id="代码与实验">代码与实验</h1><blockquote><p>该部分强烈建议搭配Kaggle使用，见"相关链接"部分。（如果觉得有帮助，可以顺便点个赞谢谢）</p></blockquote><h2 id="数据预处理-1">数据预处理</h2><p>将原始的多标签分类数据集转换为适用于T5模型的格式。具体来说，我们将文章的标题和摘要合并为一个单独的文本输入，并将标记为1的多个标签合并为一个分号分隔的标签字符串。最终，这一预处理步骤将生成一个清晰的文本到文本格式，为T5模型的训练做好准备。</p><p><code>DataPreprocessing.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;准备数据&quot;&quot;&quot;</span></span><br><span class="line">input_csv = <span class="string">&quot;../../../archive/train.csv&quot;</span></span><br><span class="line">data = pd.read_csv(input_csv)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(data))</span><br><span class="line">label_columns = data.columns[-<span class="number">6</span>:]  <span class="comment"># 提取labels列</span></span><br><span class="line"><span class="built_in">print</span>(label_columns)</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;text&#x27;</span>] = data[<span class="string">&#x27;TITLE&#x27;</span>] + <span class="string">&quot; &quot;</span> + data[<span class="string">&#x27;ABSTRACT&#x27;</span>]  <span class="comment"># 准备text</span></span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;text&#x27;</span>].head())</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;labels&#x27;</span>] = data[label_columns].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27;; &#x27;</span>.join(x.index[x == <span class="number">1</span>]), axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;labels&#x27;</span>])</span><br><span class="line"><span class="comment"># Displaying the updated dataset</span></span><br><span class="line">preprocessed_data = data[[<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(preprocessed_data.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储为新的 CSV 文件</span></span><br><span class="line">output_path = <span class="string">&quot;../../../archive/preprocessed_data.csv&quot;</span></span><br><span class="line">preprocessed_data.to_csv(output_path, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="模型训练与预测">模型训练与预测</h2><p>本次仍然使用了Pytorch以及Pytorch lightning作为实验框架，关于Pytorchlightning的使用方法请自行查阅官网。</p><p>Pytorch lightning： <ahref="https://lightning.ai/docs/pytorch/stable/">Welcome to ⚡ PyTorchLightning — PyTorch Lightning 2.2.1 documentation</a></p><p>该部分对应文件名：<code>Transformer.py</code></p><h3 id="自定义批处理函数">自定义批处理函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    自定义批处理函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    texts = [item[<span class="string">&#x27;text&#x27;</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">    labels = [item[<span class="string">&#x27;labels&#x27;</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">    <span class="comment"># 使用 tokenizer 对文本和标签进行编码,最大长度512</span></span><br><span class="line">    encoding = tokenizer(texts, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    <span class="comment"># 使用 tokenizer 的 target_tokenizer 对标签进行编码</span></span><br><span class="line">    <span class="keyword">with</span> tokenizer.as_target_tokenizer():</span><br><span class="line">        labels_encoding = tokenizer(labels, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    <span class="comment"># 将标签中的 pad token 替换为 -100，这是 T5 模型的要求</span></span><br><span class="line">    labels_encoding[<span class="string">&quot;input_ids&quot;</span>][labels_encoding[<span class="string">&quot;input_ids&quot;</span>] == tokenizer.pad_token_id] = -<span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;input_ids&#x27;</span>: encoding[<span class="string">&#x27;input_ids&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;attention_mask&#x27;</span>: encoding[<span class="string">&#x27;attention_mask&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;labels&#x27;</span>: labels_encoding[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="自定义dataset">自定义Dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">T5Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义数据集&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset</span>):</span><br><span class="line">        self.dataset = dataset</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        item = self.dataset[idx]</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;text&#x27;</span>: item[<span class="string">&#x27;text&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;labels&#x27;</span>: item[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset)</span><br></pre></td></tr></table></figure><h3 id="自定义lightningmodule">自定义LightningModule</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">T5FineTuner</span>(pl.LightningModule):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义LightningModule&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, train_dataset, val_dataset, test_dataset, learning_rate=<span class="number">2e-5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(T5FineTuner, self).__init__()</span><br><span class="line">        self.validation_loss = []</span><br><span class="line">        self.model = T5ForConditionalGeneration.from_pretrained(<span class="string">&#x27;t5-small&#x27;</span>)</span><br><span class="line">        self.learning_rate = learning_rate  <span class="comment"># 微调</span></span><br><span class="line">        self.train_dataset = train_dataset</span><br><span class="line">        self.val_dataset = val_dataset</span><br><span class="line">        self.test_dataset = test_dataset</span><br><span class="line">        self.prediction = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, labels=<span class="literal">None</span></span>):</span><br><span class="line">        output = self.model(input_ids=input_ids,</span><br><span class="line">                            attention_mask=attention_mask,</span><br><span class="line">                            labels=labels)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> AdamW(self.model.parameters(), lr=self.learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        train_loader = DataLoader(dataset=self.train_dataset, batch_size=batch_size, collate_fn=collate_fn,</span><br><span class="line">                                  shuffle=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> train_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">val_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        val_loader = DataLoader(dataset=self.val_dataset, batch_size=batch_size, collate_fn=collate_fn,</span><br><span class="line">                                shuffle=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> val_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        test_loader = DataLoader(dataset=self.test_dataset, batch_size=batch_size, collate_fn=collate_fn,</span><br><span class="line">                                 shuffle=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> test_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">        labels = batch[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">        output = self(input_ids, attention_mask, labels)</span><br><span class="line">        loss = output.loss</span><br><span class="line">        self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)  <span class="comment"># 将loss输出在控制台</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">        labels = batch[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">        output = self(input_ids, attention_mask, labels)</span><br><span class="line">        loss = output.loss</span><br><span class="line">        self.log(<span class="string">&#x27;val_loss&#x27;</span>, loss, prog_bar=<span class="literal">False</span>, logger=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment"># 生成输出序列</span></span><br><span class="line">        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        <span class="comment"># 将生成的token ids转换为文本</span></span><br><span class="line">        generated_texts = [tokenizer.decode(generated_id, skip_special_tokens=<span class="literal">True</span>, clean_up_tokenization_spaces=<span class="literal">True</span>)</span><br><span class="line">                           <span class="keyword">for</span> generated_id <span class="keyword">in</span> generated_ids]</span><br><span class="line">        <span class="comment"># 返回解码后的文本</span></span><br><span class="line">        <span class="comment"># print(generated_texts)</span></span><br><span class="line">        self.prediction.extend(generated_texts)</span><br></pre></td></tr></table></figure><h3 id="训练与预测函数">训练与预测函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, fast_run</span>):</span><br><span class="line">    trainer = pl.Trainer(fast_dev_run=fast_run)</span><br><span class="line">    trainer.test(model)</span><br><span class="line">    test_result = model.prediction</span><br><span class="line">    <span class="comment"># print(type(test_result))</span></span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> test_result[:<span class="number">10</span>]:</span><br><span class="line">        <span class="built_in">print</span>(text)</span><br><span class="line">    <span class="keyword">return</span> test_result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">fast_run</span>):</span><br><span class="line">    <span class="comment"># 增加回调最优模型</span></span><br><span class="line">    checkpoint_callback = ModelCheckpoint(</span><br><span class="line">        monitor=<span class="string">&#x27;val_loss&#x27;</span>,  <span class="comment"># 监控对象为&#x27;val_loss&#x27;</span></span><br><span class="line">        dirpath=<span class="string">&#x27;../../archive/log/T5FineTuner_checkpoints&#x27;</span>,  <span class="comment"># 保存模型的路径</span></span><br><span class="line">        filename=<span class="string">&#x27;Models-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,  <span class="comment"># 最优模型的名称</span></span><br><span class="line">        save_top_k=<span class="number">1</span>,  <span class="comment"># 只保存最好的那个</span></span><br><span class="line">        mode=<span class="string">&#x27;min&#x27;</span>  <span class="comment"># 当监控对象指标最小时</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 设置日志保存的路径</span></span><br><span class="line">    log_dir = <span class="string">&quot;../../archive/log&quot;</span></span><br><span class="line">    logger = TensorBoardLogger(save_dir=log_dir, name=<span class="string">&quot;T5FineTuner_logs&quot;</span>)</span><br><span class="line">    <span class="comment"># Trainer可以帮助调试，比如快速运行、只使用一小部分数据进行测试、完整性检查等，</span></span><br><span class="line">    <span class="comment"># 详情请见官方文档https://lightning.ai/docs/pytorch/latest/debug/debugging_basic.html</span></span><br><span class="line">    <span class="comment"># auto自适应gpu数量</span></span><br><span class="line">    trainer = pl.Trainer(max_epochs=epochs, log_every_n_steps=<span class="number">10</span>, accelerator=<span class="string">&#x27;gpu&#x27;</span>, devices=<span class="string">&quot;auto&quot;</span>, fast_dev_run=fast_run,</span><br><span class="line">                         callbacks=[checkpoint_callback], logger=logger)</span><br><span class="line">    model = T5FineTuner(train_dataset, valid_dataset, test_dataset, learning_rate)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="保存结果以及任务启动">保存结果以及任务启动</h3><p>当在Kaggle上进行操作时，请注意，直接使用<code>load_dataset</code>函数从CSV文件加载数据集可能会导致错误。为了避免这个问题，推荐先使用pandas库将CSV文件读入为DataFrame，之后再将其转换为适合模型训练的格式。具体的代码实现和操作可以参考Kaggle笔记本中的相关部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_to_csv</span>(<span class="params">test_dataset, predictions, filename=<span class="string">&quot;../../archive/test_predictions.csv&quot;</span></span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, mode=<span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        writer = csv.writer(file)</span><br><span class="line">        writer.writerow([<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;true_labels&#x27;</span>, <span class="string">&#x27;pred_labels&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> item, pred_label <span class="keyword">in</span> <span class="built_in">zip</span>(test_dataset, predictions):</span><br><span class="line">            text = item[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">            true_labels = item[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">            writer.writerow([text, true_labels, pred_label])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = load_dataset(<span class="string">&#x27;csv&#x27;</span>, data_files=&#123;<span class="string">&#x27;train&#x27;</span>: <span class="string">&#x27;../../archive/preprocessed_data.csv&#x27;</span>&#125;)[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">    <span class="comment"># 分割数据集为训练集和测试+验证集</span></span><br><span class="line">    train_testvalid = data.train_test_split(test_size=<span class="number">0.3</span>, seed=<span class="number">42</span>)</span><br><span class="line">    <span class="comment"># 分割测试+验证集为测试集和验证集</span></span><br><span class="line">    test_valid = train_testvalid[<span class="string">&#x27;test&#x27;</span>].train_test_split(test_size=<span class="number">0.5</span>, seed=<span class="number">42</span>)</span><br><span class="line">    <span class="comment"># 现在我们有了训练集、验证集和测试集</span></span><br><span class="line">    train_dataset = train_testvalid[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">    valid_dataset = test_valid[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">    test_dataset = test_valid[<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">    <span class="comment"># 打印各个数据集的大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training set size:&quot;</span>, <span class="built_in">len</span>(train_dataset))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation set size:&quot;</span>, <span class="built_in">len</span>(valid_dataset))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test set size:&quot;</span>, <span class="built_in">len</span>(test_dataset))</span><br><span class="line">    <span class="comment"># 准备Dataset</span></span><br><span class="line">    train_dataset = T5Dataset(train_dataset)</span><br><span class="line">    valid_dataset = T5Dataset(valid_dataset)</span><br><span class="line">    test_dataset = T5Dataset(test_dataset)</span><br><span class="line">    <span class="comment"># print(train_dataset.__len__())</span></span><br><span class="line">    <span class="comment"># print(train_dataset[0])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化分词器</span></span><br><span class="line">    tokenizer = T5Tokenizer.from_pretrained(<span class="string">&#x27;t5-small&#x27;</span>)</span><br><span class="line">    <span class="comment"># 装载dataLoader</span></span><br><span class="line">    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size,</span><br><span class="line">                                  collate_fn=collate_fn, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 查看装载情况</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Batch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Input IDs:&quot;</span>, batch[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Input IDs shape:&quot;</span>, batch[<span class="string">&#x27;input_ids&#x27;</span>].shape)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Attention Mask:&quot;</span>, batch[<span class="string">&#x27;attention_mask&#x27;</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Attention Mask shape:&quot;</span>, batch[<span class="string">&#x27;attention_mask&#x27;</span>].shape)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Labels:&quot;</span>, batch[<span class="string">&#x27;labels&#x27;</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    fast_run = <span class="literal">True</span></span><br><span class="line">    model = train(fast_run)</span><br><span class="line">    <span class="comment"># model = T5FineTuner.load_from_checkpoint(</span></span><br><span class="line">    <span class="comment">#     &quot;../../archive/log/T5FineTuner_checkpoints/model-epoch=09-val_loss=0.32.ckpt&quot;,</span></span><br><span class="line">    <span class="comment">#     train_dataset=train_dataset, val_dataset=valid_dataset,</span></span><br><span class="line">    <span class="comment">#     test_dataset=test_dataset)</span></span><br><span class="line">    pre_texts = test(model, fast_run)</span><br><span class="line">    save_to_csv(test_dataset, pre_texts)</span><br></pre></td></tr></table></figure><h2 id="结果转化与模型评估">结果转化与模型评估</h2><p><code>Estimate.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MultiLabelBinarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, accuracy_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_labels</span>(<span class="params">label_str</span>):</span><br><span class="line">    <span class="keyword">return</span> label_str.split(<span class="string">&#x27;;&#x27;</span>) <span class="keyword">if</span> label_str <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_label</span>(<span class="params">label</span>):</span><br><span class="line">    <span class="keyword">return</span> label.strip()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取提供的 CSV 文件</span></span><br><span class="line">file_path = <span class="string">&quot;../../archive/test_predictions.csv&quot;</span></span><br><span class="line">data = pd.read_csv(file_path)</span><br><span class="line"><span class="comment"># print(data.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取并转换真实标签和预测标签</span></span><br><span class="line">true_labels = [convert_labels(label_str) <span class="keyword">for</span> label_str <span class="keyword">in</span> data[<span class="string">&#x27;true_labels&#x27;</span>]]</span><br><span class="line">pred_labels = [convert_labels(label_str) <span class="keyword">for</span> label_str <span class="keyword">in</span> data[<span class="string">&#x27;pred_labels&#x27;</span>]]</span><br><span class="line"><span class="comment"># 使用清理后的标签重新创建真实标签和预测标签列表</span></span><br><span class="line">true_labels_cleaned = [<span class="built_in">list</span>(<span class="built_in">map</span>(clean_label, label_list)) <span class="keyword">for</span> label_list <span class="keyword">in</span> true_labels]</span><br><span class="line">pred_labels_cleaned = [<span class="built_in">list</span>(<span class="built_in">map</span>(clean_label, label_list)) <span class="keyword">for</span> label_list <span class="keyword">in</span> pred_labels]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 MultiLabelBinarizer 对标签进行独热编码</span></span><br><span class="line">mlb = MultiLabelBinarizer()</span><br><span class="line">mlb.fit(true_labels_cleaned + pred_labels_cleaned)</span><br><span class="line">y_true = mlb.transform(true_labels_cleaned)</span><br><span class="line">y_pred = mlb.transform(pred_labels_cleaned)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transformer(T5) Accuracy =&quot;</span>, accuracy_score(y_true, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transformer(T5) Precision (micro-average) =&quot;</span>, precision_score(y_true, y_pred, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transformer(T5) Recall (micro-average) =&quot;</span>, recall_score(y_true, y_pred, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transformer(T5) F1 Score (micro-average) =&quot;</span>, f1_score(y_true, y_pred, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnother way to calculate accuracy:&quot;</span>)</span><br><span class="line"><span class="comment"># 计算每一列的准确率</span></span><br><span class="line">column_accuracies = np.mean(y_true == y_pred, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 为每列准确率添加列名</span></span><br><span class="line">column_accuracy_with_labels = <span class="built_in">list</span>(<span class="built_in">zip</span>(mlb.classes_, column_accuracies))</span><br><span class="line"><span class="comment"># 计算列准确率的均值</span></span><br><span class="line">mean_column_accuracy = np.mean(column_accuracies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> acc <span class="keyword">in</span> column_accuracy_with_labels:</span><br><span class="line">    <span class="built_in">print</span>(acc)</span><br><span class="line"><span class="comment"># print(column_accuracy_with_labels)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average accuracy = &quot;</span>, mean_column_accuracy)</span><br></pre></td></tr></table></figure><h1 id="实验结果">实验结果</h1><p>本节汇总并比较了上篇和下篇文章中各种实验的结果。我们采用了几种不同的算法来处理多标签分类问题，包括BinaryRelevance（BR）与Random Forest组合、Classifier Chains（CC）与RandomForest组合、Label Powerset（LP）与RandomForest组合、LP与SVM组合，以及使用了Transformer（T5）模型的序列生成方法。</p><p>通过对比准确率（Accuracy）、微观精确度（Precision_micro）、微观召回率（Recall_micro）和微观F1分数（F1_micro）这四个关键性能指标，我们发现：</p><ul><li>使用基于RandomForest的BR、CC和LP方法可以得到相对较好的预测性能。</li><li>当LP与SVM组合使用时，性能有所提高，特别是在召回率和F1分数方面。</li><li>最为显著的是，Transformer（T5）模型尤其在准确率、召回率和F1分数上达到了最高值。</li></ul><p>具体数值如下所示：</p><table><colgroup><col style="width: 25%" /><col style="width: 15%" /><col style="width: 20%" /><col style="width: 19%" /><col style="width: 19%" /></colgroup><thead><tr class="header"><th>Algorithms</th><th>Acc</th><th><span class="math inline">\(Pre_{micro}\)</span></th><th><span class="math inline">\(Re_{micro}\)</span></th><th><span class="math inline">\(F1_{micro}\)</span></th></tr></thead><tbody><tr class="odd"><td>BR(RandomForest)</td><td>0.4477</td><td><strong>0.8038</strong></td><td>0.4978</td><td>0.6149</td></tr><tr class="even"><td>CC(RandomForest)</td><td>0.4787</td><td>0.8012</td><td>0.5277</td><td>0.6363</td></tr><tr class="odd"><td>LP(RandomForest)</td><td>0.5349</td><td>0.7179</td><td>0.5889</td><td>0.6470</td></tr><tr class="even"><td>LP(SVM)</td><td>0.5914</td><td>0.7368</td><td>0.7245</td><td>0.7306</td></tr><tr class="odd"><td>Transformer(T5)</td><td><strong>0.6427</strong></td><td>0.7994</td><td><strong>0.7840</strong></td><td><strong>0.7916</strong></td></tr></tbody></table><p>从结果中我们可以得出结论，Transformer模型在处理复杂的多标签分类任务时，展现出了其强大的能力。这也表明了序列到序列模型，在NLP领域的广泛应用潜力和有效性。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLP】多标签分类【上】</title>
      <link href="/article/508dd4f67dbd/"/>
      <url>/article/508dd4f67dbd/</url>
      
        <content type="html"><![CDATA[<h1 id="简介">简介</h1><p>《【NLP】多标签分类》主要介绍利用三种机器学习方法和一种序列生成方法来解决多标签分类问题（包含实验与对应代码）。共分为上下两篇，上篇聚焦三种机器学习方法，分别是：BinaryRelevance (BR)、Classifier Chains (CC)、Label Powerset(LP)，下篇聚焦利用序列生成解决多标签分类方法，将使用Transformer完成该任务。</p><p>本文共分为5节，第一节介绍实验数据来源、任务说明；第二节介绍BR、CC、LP各自原理以及优缺点；第三节介绍本文使用的多标签分类评估标准；第四节介绍实验环境、实验步骤、实验评估以及相关代码；第五节为全文总结。</p><h1 id="个人博客与相关链接">个人博客与相关链接</h1><p>本文相关代码和数据集已同步上传github: <ahref="https://github.com/iceissey/issey_Kaggle/tree/main/MultiLabelClassification">issey_Kaggle/MultiLabelClassificationat main · iceissey/issey_Kaggle (github.com)</a></p><p>本文代码（Notebook）已公布至kaggle: <ahref="https://www.kaggle.com/code/isseyice/xlnet-embedding-and-machine-learning-br-cc-lp/notebook?scriptVersionId=158428477">XLNETembedding and machine learning（BR、CC、LP） | Kaggle</a></p><p>博主个人博客链接：<a href="https://www.issey.top/">issey的博客 -愿无岁月可回首</a></p><h1 id="实验数据与任务说明">实验数据与任务说明</h1><p>数据来源：<ahref="https://www.kaggle.com/datasets/shivanandmn/multilabel-classification-dataset/data">Multi-LabelClassification Dataset (kaggle.com)</a></p><p>任务说明：</p><ul><li>背景：NLP——多标签分类数据集。</li><li>内容：该数据集包含6个不同的标签（计算机科学、物理学、数学、统计学、定量生物学、定量金融），用于根据摘要和标题对研究论文进行分类。标签列中的值1表示该标签属于该论文，每篇论文可以有多个标签为1。</li></ul><p><img src="https://img.issey.top/img/202401101606632.png" /></p><h1 id="多标签分类任务与相关算法">多标签分类任务与相关算法</h1><h2 id="多标签分类任务简介">多标签分类任务简介</h2><p>多标签分类<strong>（Multi-labelClassification）</strong>是一种机器学习任务，其中每个输入样本可以分配给多个类别标签，而不是只能分配给一个单一的类别标签。与传统的单标签分类不同，多标签分类允许一个样本同时属于多个类别，这更符合现实世界中许多复杂问题的性质。</p><h2 id="相关算法">相关算法</h2><p>多标签分类方法主要分为两大类，分别是<strong>问题转换方法</strong>和<strong>算法适应方法</strong>，本篇主要集中于问题转换方法中的前三种。</p><p><strong>问题转换方法</strong>：这些方法通过转换问题使其适用于标准的单标签分类算法。主要包括以下几种：</p><ul><li><strong>二元相关性（Binary Relevance,BR）</strong>：这种方法将多标签问题分解成多个独立的二分类问题，每个标签都被视为一个独立的二分类问题。<ul><li>优点：<ol type="1"><li><strong>简单易实现：</strong>BR方法的实现相对简单直接，因为它将复杂的多标签问题分解为多个标准的二分类问题。</li><li><strong>灵活性：</strong>由于BR方法在每个标签上独立训练分类器，因此可以针对不同的标签选择最适合的分类算法。</li><li><strong>可扩展性：</strong>在新标签加入时，只需增加相应的二分类器，而无需修改或重新训练其他分类器。</li><li><strong>高效：</strong>由于每个标签都独立处理，可以并行训练和预测，提高了处理速度。</li></ol></li><li>缺点：<ol type="1"><li><spanstyle="color: red;"><strong>忽略标签依赖性</strong>：BR方法的主要缺点是它忽略了标签之间的相关性。在实际应用中，标签往往不是完全独立的，它们之间的关联可能对分类结果有重要影响。</span></li><li><strong>预测性能问题</strong>：由于不考虑标签间的依赖关系，BR方法在某些复杂的多标签问题上的预测性能可能不如那些能够考虑标签依赖性的方法。</li></ol></li></ul></li><li><strong>标签幂集（Label Powerset,LP）</strong>：在这种方法中，每一种标签组合都被视为一个独立的类别，从而将多标签问题转换为单标签多类别问题。<ul><li>优点<ol type="1"><li><strong>考虑标签之间的依赖性</strong>：LP方法能够捕捉和利用标签之间的相关性。这在标签彼此之间存在强烈依赖性的情况下特别有用。</li><li><strong>简化模型训练</strong>：与需要为每个标签单独训练一个分类器的二元相关方法相比，LP只需训练一个模型，这可以简化训练过程。</li><li><strong>直接预测标签集合</strong>：LP方法直接预测整个标签集合，避免了将标签预测作为独立事件处理时可能出现的问题。</li></ol></li><li>缺点：<ol type="1"><li><spanstyle="color: red;"><strong>组合爆炸</strong>：当标签数量增多时，可能的标签组合数会指数级增长，导致计算和存储需求急剧增加。<strong>由于组合爆炸的问题，标签幂集无法处理标签种类较多的问题。</strong></span></li><li><strong>数据稀疏问题</strong>：对于一些罕见的标签组合，可能没有足够的训练数据，这会导致模型性能下降。</li><li><strong>效率问题</strong>：尽管只需训练一个模型，但模型可能变得非常复杂，特别是当存在大量的标签组合时。</li></ol></li></ul></li><li><strong>分类器链（Classifier Chains,CC）</strong>：这种方法通过构建一个分类器链来解决标签之间的依赖问题。每个分类器在链中负责一个标签，并将前面分类器的预测结果作为额外的输入。<ul><li>优点：<ol type="1"><li><strong>考虑标签间的依赖性</strong>：分类器链通过序列化的方式考虑标签间的依赖关系，这在标签相关性显著的情况下特别有用。</li><li><strong>可扩展性</strong>：相比于标签幂集方法，分类器链在处理大量标签时更为高效，因为它避免了组合爆炸问题。</li><li><strong>较好的泛化能力</strong>：相对于二元相关方法，分类器链通常能够提供更好的泛化能力，尤其是在标签之间存在依赖关系时。</li></ol></li><li>缺点：<ol type="1"><li><strong>链的顺序敏感性</strong>：分类器链的性能可能受到链中分类器顺序的影响。不同的标签顺序可能导致不同的性能表现。</li><li><strong>错误传播</strong>：链中早期分类器的错误可能会传播到链的后面部分，影响整体性能。</li></ol></li></ul></li><li><strong>随机k标签子集（Random k-Labelsets,RAkEL）</strong>：这种方法是通过随机选择标签子集并对每个子集应用LP方法，然后综合这些模型的预测结果。<code>由于本文涉及的实验总共标签总类也才6种，所以没有使用这种方法而直接选择了LP</code>。<ul><li>优点：<ol type="1"><li><strong>缓解组合爆炸问题</strong>：通过在较小的标签子集上应用LP方法，RAkEL减少了可能的标签组合数量，从而缓解了标签幂集法中的组合爆炸问题。</li><li><strong>考虑标签间的依赖性</strong>：与二元相关方法相比，RAkEL能够捕捉标签子集内部的依赖关系，提高了模型的准确性。</li><li><strong>更好的泛化能力</strong>：由于模型在多个随机选择的标签子集上训练，这可以增加模型的泛化能力。</li></ol></li><li>缺点：<ol type="1"><li><strong>随机性</strong>：标签子集的随机选择可能导致模型性能的不稳定性。</li><li><strong>可能忽略某些标签关系</strong>：如果某些相关标签从不在同一个子集中出现，那么它们之间的关系可能不会被模型捕捉到。</li><li><strong>计算复杂度</strong>：虽然RAkEL缓解了组合爆炸问题，但仍需要训练多个LP模型，这可能比单一的分类器链或二元相关方法更耗时。</li><li><strong>预测一致性问题</strong>：不同的标签子集模型可能对相同的标签做出不同的预测，需要有效的机制来整合这些预测。</li><li><strong>参数选择</strong>：选择合适的子集大小（k值）和子集数量是RAkEL方法的关键，这可能需要根据具体的数据集进行调整。</li></ol></li></ul></li></ul><p><strong>算法适应方法</strong>：这些方法通过修改现有的学习算法使其能够直接处理多标签数据。主要包括以下几种：适应决策树（AdaptedDecision Trees）、适应神经网络（Adapted NeuralNetworks）、适应支持向量机（Adapted Support VectorMachines）、k最近邻修改版（k-Nearest Neighbors Adaptation）。</p><p>除问题转换方法和算法适应方法外，深度学习方法也在多标签分类中表现出色。<spanstyle="color: red;">在本文的下篇中，会介绍将多标签分类转换为多标签序列生成任务的方法。</span></p><h1 id="多标签分类评估方法">多标签分类评估方法</h1><h3 id="准确率accuracy">1. 准确率（Accuracy）</h3><ul><li><strong>定义</strong>：准确率是正确预测的样本数与总样本数的比例。在多标签分类中，如果所有的标签都被准确预测，则一个样本的预测被认为是正确的。</li><li><strong>实现：</strong>使用sklearn.metrics的accuracy_score方法实现。</li><li><strong>备注：</strong>由于只有当某样本所有标签全预测正确，才能算该样本预测正确，导致这种方式计算出的Acc结果普遍偏低。<spanstyle="color: red;">在下篇中，会介绍另一种计算Acc的方式，即先计算每一个label的Acc，然后在取平均值。</span></li></ul><h3 id="精确度precision--微观平均micro-average">2. 精确度（Precision）-微观平均（Micro-average）</h3><ul><li><strong>定义</strong>：精确度是模型正确预测为正的实例（真正例）占模型预测为正的所有实例（真正例和假正例）的比例。</li><li><strong>计算方法</strong>：微观平均精确度是通过汇总所有类别的真正例和假正例的数量，然后计算总体精确度得到的。在多标签设置中，这意味着考虑所有标签的预测结果，而不是单独考虑每个标签。</li><li><strong>实现：</strong>使用sklearn.metrics的precision_score方法实现。</li></ul><h3 id="召回率recall--微观平均micro-average">3. 召回率（Recall）-微观平均（Micro-average）</h3><ul><li><strong>定义</strong>：召回率是模型正确预测为正的实例占实际为正的所有实例（真正例和假负例）的比例。</li><li><strong>计算方法</strong>：微观平均召回率是通过汇总所有类别的真正例和假负例的数量，然后计算总体召回率得到的。它反映了模型在所有标签上的总体能力，来正确地识别正类实例。</li><li><strong>实现：</strong>使用sklearn.metrics的recall_score方法实现。</li></ul><h3 id="f1-分数f1-score--微观平均micro-average">4. F1 分数（F1 Score）-微观平均（Micro-average）</h3><ul><li><strong>定义</strong>：F1分数是精确度和召回率的调和平均值，用于平衡这两个指标。</li><li><strong>计算方法</strong>：微观平均 F1分数是基于微观平均精确度和召回率计算得到的。它是这两个指标的调和平均值，因此在精确度和召回率都重要时，提供了一个综合性能度量。</li><li><strong>实现:</strong>使用sklearn.metrics的f1_score方法实现。</li></ul><h1 id="实验">实验</h1><h2 id="实验环境">实验环境</h2><p>本实验是在以下配置的环境中进行的：</p><ul><li><strong>编程语言和版本</strong>：<ul><li>Python3.9：一个广泛使用的高级编程语言，适用于数据科学和机器学习项目。</li></ul></li><li><strong>主要库和框架</strong>：<ul><li>NumPy 1.23.3：用于高性能科学计算和数据分析的基础包。</li><li>Pandas 1.4.4：提供高效的数据结构和数据分析工具。</li><li>Matplotlib 3.5.3：用于数据可视化的绘图库。</li><li>PyTorch 1.13.0：一个灵活的深度学习框架，适用于研究和生产。</li><li>PyTorch CUDA 11.6：用于在NVIDIAGPU上加速PyTorch运算的CUDA支持库。</li></ul></li><li><strong>机器学习和深度学习库</strong>：<ul><li>Transformers 4.18.0：由HuggingFace提供的，用于自然语言处理的预训练模型和转换器。</li><li>scikit-learn 1.2.2：提供简单有效的数据挖掘和数据分析工具。</li><li>scikit-multilearn 0.2.0：用于多标签分类的机器学习库。</li></ul></li></ul><h2 id="实验步骤">实验步骤</h2><p>本篇的实验步骤主要包括：1）数据观察与预处理阶段。2）词嵌入阶段。3）模型训练与测试阶段。4）进一步探索。</p><h3 id="数据观察与预处理">数据观察与预处理</h3><h4 id="数据观察">数据观察</h4><ul><li><strong>单词数量统计：</strong>在本实验中，我们专注于观察数据集中每个文本项的单词数量。通过统计信息，我们可以了解数据集中文本的长度分布。</li></ul><h4 id="数据预处理">数据预处理</h4><ul><li><strong>最小化预处理</strong>：由于本实验在后续词嵌入时使用XL-NET模型，且与使用传统文本分类方法相比，使用XL-NET等先进的预训练模型时，常规的文本预处理步骤（如去除特殊符号、停用词移除、词形还原）并不是必要的。这些模型的分词器能够有效处理原始文本中的复杂词汇结构，同时保留对上下文理解至关重要的词汇和语法特征。</li><li><strong>实验步骤完整性</strong>：虽然在本实验中不需要传统的预处理步骤，但为了保持实验步骤的完整性和系统性，我们仍然包含了这一部分。这有助于清晰地展示实验流程，并为可能需要适当预处理的后续研究提供参考。</li></ul><hr /><h4 id="代码部分">代码部分</h4><ul><li><strong>准备工作</strong></li></ul><p>导入相关库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizer, XLNetModel</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><p>检查GPU是否可用。在上篇的实验中，如果GPU不可用问题也不大，直接用CPU跑即可，因为上篇使用GPU的地方只有embedding。不过在下篇时GPU是必要的，如果本地环境不支持，建议放到云服务器（如kaggle）上跑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the device to GPU (if available).</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Using device:&quot;</span>, device)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using device: cuda</span><br></pre></td></tr></table></figure><ul><li><strong>准备数据集</strong></li></ul><p>由于题目要求使用TITLE和ABSTRACT共同参与预测，所以简单做一下拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Prepare the data&quot;&quot;&quot;</span></span><br><span class="line">input_csv = <span class="string">&quot;/kaggle/input/multilabel-classification-dataset/train.csv&quot;</span></span><br><span class="line">data = pd.read_csv(input_csv)  </span><br><span class="line"><span class="comment"># data = data[:20]  # Test</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(data))</span><br><span class="line">data[<span class="string">&#x27;combined_text&#x27;</span>] = data[<span class="string">&#x27;TITLE&#x27;</span>] + <span class="string">&quot; &quot;</span> + data[<span class="string">&#x27;ABSTRACT&#x27;</span>] </span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;combined_text&#x27;</span>].head())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">20972</span><br><span class="line">0    Reconstructing Subject-Specific Effect Maps   ...</span><br><span class="line">1    Rotation Invariance Neural Network   Rotation ...</span><br><span class="line">2    Spherical polyharmonics and Poisson kernels fo...</span><br><span class="line">3    A finite element approximation <span class="keyword">for</span> the stochas...</span><br><span class="line">4    Comparative study of Discrete Wavelet Transfor...</span><br><span class="line">Name: combined_text, dtype: object</span><br></pre></td></tr></table></figure><ul><li><strong>统计combined_text单词分布</strong></li></ul><p>检查combined_text最长、最小、平均单词长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;View the distribution of word counts&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Split the text using spaces and calculate the number of words</span></span><br><span class="line">data[<span class="string">&#x27;word_count&#x27;</span>] = data[<span class="string">&#x27;combined_text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(<span class="built_in">str</span>(x).split()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print statistical information about the number of words</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Word count statistics:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Maximum word count:&quot;</span>, data[<span class="string">&#x27;word_count&#x27;</span>].<span class="built_in">max</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Minimum word count:&quot;</span>, data[<span class="string">&#x27;word_count&#x27;</span>].<span class="built_in">min</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average word count:&quot;</span>, data[<span class="string">&#x27;word_count&#x27;</span>].mean())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Word count statistics:</span><br><span class="line">Maximum word count: 462</span><br><span class="line">Minimum word count: 5</span><br><span class="line">Average word count: 157.9198455082968</span><br></pre></td></tr></table></figure><p>绘制单词分布柱状图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.hist(data[<span class="string">&#x27;word_count&#x27;</span>], bins=<span class="number">50</span>, alpha=<span class="number">0.75</span>, color=<span class="string">&#x27;b&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Word count&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Frequency&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Word count distribution&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202401101705063.png" /></p><h3 id="词嵌入阶段">词嵌入阶段</h3><h4 id="xl-net嵌入">XL-NET嵌入</h4><p>在本实验中，我们使用了预训练的XL-NET模型来生成文本嵌入，这是一个关键步骤，旨在将文本转换为能被机器学习模型有效处理的数值形式。</p><ul><li><strong>模型和分词器加载</strong>：我们首先加载了XLNet的基础模型（<code>xlnet-base-cased</code>）和对应的分词器。这个分词器将负责将原始文本转换成模型可以理解的令牌序列。</li><li><strong>设定批处理大小</strong>：考虑到计算效率和内存限制，我们设定了一个合适的批处理大小（<code>batch_size = 32</code>）。这意味着每次向模型输入32个文本样本进行处理。</li></ul><h4 id="嵌入生成过程">嵌入生成过程</h4><ul><li><strong>文本准备和处理</strong>：我们将数据集中的文本转换为字符串列表，并按批次处理。每个批次的文本被分词器编码，其中包括截断和填充操作以确保文本长度一致。</li><li><strong>嵌入计算</strong>：对于每个批次，我们将编码后的文本输入XL-NET模型。通过模型，我们获取每个文本的嵌入表示，这些表示捕捉了文本中的语义信息。</li><li><strong>处理和存储嵌入</strong>：得到的嵌入被转换为NumPy数组，并被收集在一起。最终，所有的嵌入被存储在HDF5文件格式中，方便后续的机器学习任务使用。</li></ul><h4 id="不进行微调的决定">不进行微调的决定</h4><ul><li><strong>一次性嵌入过程</strong>：本实验选择不对XL-NET模型进行微调，而是直接使用预训练模型一次性生成所有文本的嵌入。这种方法简化了实验流程，同时允许我们充分利用XL-NET预训练模型的强大语义捕捉能力。</li><li><strong>效率和实用性</strong>：将所有文本的嵌入预先计算并存储起来，提高了后续实验步骤的效率。</li></ul><hr /><h4 id="代码部分-1">代码部分</h4><ul><li><strong>加载分词器和预训练模型</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Load the XLNet tokenizer and model&quot;&quot;&quot;</span></span><br><span class="line">tokenizer = XLNetTokenizer.from_pretrained(<span class="string">&#x27;xlnet-base-cased&#x27;</span>)</span><br><span class="line">model = XLNetModel.from_pretrained(<span class="string">&#x27;xlnet-base-cased&#x27;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># Determine the batch size</span></span><br><span class="line">all_embeddings = []</span><br><span class="line"><span class="comment"># token = tokenizer.convert_ids_to_tokens(5)</span></span><br><span class="line"><span class="comment"># print(token)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;&quot;Choose not to fine-tune the embedding layer, so embed all texts at once into vectors&quot;&quot;&quot;</span></span><br><span class="line">texts = data[<span class="string">&#x27;combined_text&#x27;</span>].astype(<span class="built_in">str</span>).tolist()</span><br></pre></td></tr></table></figure><ul><li>embedding，并将嵌入好的向量一次性存储下来</li></ul><p>tqdm是一个可视化进度条的库，可以方便的查看处理进度。</p><p>这里解释一下如何从XL-NET模型的输出中提取嵌入(embedding)。</p><ul><li><strong>模型输出理解</strong>：当我们将输入文本通过XL-NET模型处理时，<code>outputs</code>对象包含了多个不同的输出组件。其中，<code>last_hidden_state</code>是一个多维张量，其维度通常是<code>[批处理大小, 序列长度, 隐藏单元数]</code>。这个张量包含了模型对每个输入令牌的最后一层隐藏状态的表示。</li><li><strong>选择特定令牌的嵌入</strong>：在XL-NET和类似的变压器模型中，每个输入令牌都有一个对应的输出向量。在这里，<code>outputs.last_hidden_state[:, 0, :]</code>表示我们选择了每个序列的第一个令牌（通常是特殊的分类令牌，如BERT中的<code>[CLS]</code>）的输出向量。<spanstyle="color: red;">这个向量被认为是整个输入序列的聚合表示，并经常用于分类任务。</span></li></ul><blockquote><p>还记得我在今年早些的时候做的那个Bert+Bilstm的任务<ahref="https://blog.csdn.net/qq_52466006/article/details/130064014?spm=1001.2014.3001.5502">【NLP实战】基于Bert和双向LSTM的情感分类【中篇】-CSDN博客</a>，当时我在embeding后直接取的last_hidden_state，也就是个三维向量，接着用Bilstm得到最终的二维隐藏层（只保留了最后的隐藏状态），现在想来当时对Bert的理解还是不到位。然而这两种方法都是有效的，不过一个是词维度的嵌入，一个是句维度的嵌入，本文上篇使用的embedding就是句维度的嵌入。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify the directory path</span></span><br><span class="line">directory_path = <span class="string">&#x27;/kaggle/working/multilabel-classification-dataset/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the directory if it doesn&#x27;t exist</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(directory_path):</span><br><span class="line">    os.makedirs(directory_path)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> start_index <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(texts), batch_size)):</span><br><span class="line">    <span class="comment"># Encode the text</span></span><br><span class="line">    batch_texts = texts[start_index:start_index + batch_size]</span><br><span class="line">    encoded_inputs = tokenizer(batch_texts, return_tensors=<span class="string">&#x27;pt&#x27;</span>, max_length=<span class="number">512</span>, truncation=<span class="literal">True</span>, padding=<span class="string">&#x27;max_length&#x27;</span>)</span><br><span class="line">    <span class="comment"># get embeddings</span></span><br><span class="line">    input_ids = encoded_inputs[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">    attention_mask = encoded_inputs[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">    <span class="comment">#  calculate embeddings</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">    <span class="comment">#  move the results back to CPU and convert to numpy arrays</span></span><br><span class="line">    embeddings = outputs.last_hidden_state[:, <span class="number">0</span>, :].cpu().numpy()</span><br><span class="line">    <span class="comment"># print(embeddings.shape)</span></span><br><span class="line">    </span><br><span class="line">    all_embeddings.extend(embeddings)</span><br><span class="line"><span class="comment"># Convert all embeddings to numpy arrays</span></span><br><span class="line">all_embeddings = np.array(all_embeddings)</span><br><span class="line"><span class="built_in">print</span>(all_embeddings.shape)</span><br><span class="line"><span class="comment"># Store embedding vectors to an HDF5 file</span></span><br><span class="line">hdf5_filename = <span class="string">&#x27;/kaggle/working/multilabel-classification-dataset/embeddings.h5&#x27;</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(hdf5_filename, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> hdf5_file:</span><br><span class="line">    hdf5_file.create_dataset(<span class="string">&#x27;embeddings&#x27;</span>, data=all_embeddings)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Embeddings have been stored in the <span class="subst">&#123;hdf5_filename&#125;</span> file.&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">100</span>%|██████████| <span class="number">656</span>/<span class="number">656</span> [<span class="number">15</span>:<span class="number">11</span>&lt;<span class="number">00</span>:<span class="number">00</span>,  <span class="number">1.39</span>s/it]</span><br><span class="line">(<span class="number">20972</span>, <span class="number">768</span>)</span><br><span class="line">Embeddings have been stored <span class="keyword">in</span> the /kaggle/working/multilabel-classification-dataset/embeddings.h5 file.</span><br></pre></td></tr></table></figure><p>可以看到，现在我们的数据集中的text（也就是<code>'combined_text'</code>），被编译为了一个768维度的向量。一共有20972行text，所以嵌入矩阵为<code>(20972, 768)</code>。</p><h3 id="模型训练与测试阶段">模型训练与测试阶段</h3><h4 id="数据准备">数据准备</h4><ul><li><strong>数据集加载与分割</strong>：我们从CSV文件中加载了数据集，并提取了标签列。接着，使用XL-NET生成的嵌入向量作为特征，将数据集分割为训练集和测试集，保证了模型训练和评估的有效性和公正性。</li></ul><h4 id="多标签分类方法">多标签分类方法</h4><p>我们采用了三种不同的多标签分类方法：二元相关（Binary Relevance,BR）、分类器链（Classifier Chains, CC）和标签幂集（Label Powerset,LP）。每种方法都使用了随机森林分类器作为基学习器。</p><ul><li><strong>二元相关（BinaryRelevance）</strong>：这种方法将多标签问题分解为多个独立的二分类问题。我们首先训练了BR模型，并记录了训练时间。接着，我们在测试集上进行预测，并计算了准确度、精确度、召回率和F1分数（微观平均）。</li><li><strong>分类器链（ClassifierChains）</strong>：这种方法通过构建一个分类器链，使每个分类器在预测时考虑到之前分类器的输出。同样，我们训练了CC模型，记录了训练时间，并在测试集上进行了评估。</li><li><strong>标签幂集（LabelPowerset）</strong>：LP方法将多标签问题转换为单标签多类别问题。我们训练了LP模型，并对其进行了测试集上的性能评估。</li></ul><h4 id="性能评估">性能评估</h4><ul><li><p><strong>评估指标</strong>：为了全面评估每种方法的性能，我们计算了准确度、精确度、召回率和F1分数（均采用微观平均），评估指标详细说明如第三节所示。这些指标帮助我们理解不同方法在处理多标签分类任务时的效果和局限。</p></li><li><p><strong>训练时间和性能</strong>：每种方法的训练时间都被记录下来，以评估其在实际应用中的可行性。</p></li></ul><hr /><h4 id="代码部分-2">代码部分</h4><ul><li><strong>导入相关库</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skmultilearn.problem_transform <span class="keyword">import</span> BinaryRelevance, ClassifierChain, LabelPowerset</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><ul><li><strong>准备数据</strong></li></ul><p>提取标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_path = <span class="string">&quot;/kaggle/input/multilabel-classification-dataset/train.csv&quot;</span></span><br><span class="line">data = pd.read_csv(data_path)</span><br><span class="line"></span><br><span class="line">label_columns = data.columns[-<span class="number">6</span>:]  <span class="comment"># Extract the &#x27;labels&#x27; column</span></span><br><span class="line">y = data[label_columns].values</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(20972, 6)</span><br></pre></td></tr></table></figure><p>加载经过XL-NET嵌入后的隐向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load embedding vectors</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(<span class="string">&#x27;/kaggle/input/xlnet-embedding-for-multilabel-classification/embeddings.h5&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    embeddings = np.array(f[<span class="string">&#x27;embeddings&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(embeddings.shape)</span><br><span class="line"><span class="comment"># 确保标签和嵌入向量的行数相同</span></span><br><span class="line"><span class="keyword">assert</span> embeddings.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(20972, 768)</span><br></pre></td></tr></table></figure><p>用于后续测试，如果要让模型快速运行就把注释打开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST</span></span><br><span class="line"><span class="comment"># embeddings = embeddings[:1000]</span></span><br><span class="line"><span class="comment"># y = y[:1000]</span></span><br></pre></td></tr></table></figure><p>分割数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split the dataset into a training set and a test set.</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape, X_test.shape, y_train.shape, y_test.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(16777, 768) (4195, 768) (16777, 6) (4195, 6)</span><br></pre></td></tr></table></figure><ul><li>模型训练与测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Binary Relevance</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">br_classifier = BinaryRelevance(RandomForestClassifier())</span><br><span class="line">br_classifier.fit(X_train, y_train)</span><br><span class="line">br_training_time = time.time() - start_time</span><br><span class="line">br_predictions = br_classifier.predict(X_test)</span><br><span class="line">br_precision = precision_score(y_test, br_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">br_recall = recall_score(y_test, br_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">br_f1 = f1_score(y_test, br_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Training Time:&quot;</span>, br_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Accuracy =&quot;</span>, accuracy_score(y_test, br_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Precision (micro-average) =&quot;</span>, br_precision)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR Recall (micro-average) =&quot;</span>, br_recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BR F1 Score (micro-average) =&quot;</span>, br_f1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier Chains</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">cc_classifier = ClassifierChain(RandomForestClassifier())</span><br><span class="line">cc_classifier.fit(X_train, y_train)</span><br><span class="line">cc_training_time = time.time() - start_time</span><br><span class="line">cc_predictions = cc_classifier.predict(X_test)</span><br><span class="line">cc_precision = precision_score(y_test, cc_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">cc_recall = recall_score(y_test, cc_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">cc_f1 = f1_score(y_test, cc_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Training Time:&quot;</span>, cc_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Accuracy =&quot;</span>, accuracy_score(y_test, cc_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Precision (micro-average) =&quot;</span>, cc_precision)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC Recall (micro-average) =&quot;</span>, cc_recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CC F1 Score (micro-average) =&quot;</span>, cc_f1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label Powerset</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">lp_classifier = LabelPowerset(RandomForestClassifier())</span><br><span class="line">lp_classifier.fit(X_train, y_train)</span><br><span class="line">lp_training_time = time.time() - start_time</span><br><span class="line">lp_predictions = lp_classifier.predict(X_test)</span><br><span class="line">lp_precision = precision_score(y_test, lp_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">lp_recall = recall_score(y_test, lp_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">lp_f1 = f1_score(y_test, lp_predictions, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Training Time:&quot;</span>, lp_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Accuracy =&quot;</span>, accuracy_score(y_test, lp_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Precision (micro-average) =&quot;</span>, lp_precision)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP Recall (micro-average) =&quot;</span>, lp_recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP F1 Score (micro-average) =&quot;</span>, lp_f1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">===================================</span><br><span class="line">BR Training Time: 445.8087875843048</span><br><span class="line">BR Accuracy = 0.4476758045292014</span><br><span class="line">BR Precision (micro-average) = 0.8038496791934006</span><br><span class="line">BR Recall (micro-average) = 0.4978240302743614</span><br><span class="line">BR F1 Score (micro-average) = 0.6148632858144426</span><br><span class="line">===================================</span><br><span class="line">CC Training Time: 410.08831691741943</span><br><span class="line">CC Accuracy = 0.4786650774731824</span><br><span class="line">CC Precision (micro-average) = 0.8012065498419995</span><br><span class="line">CC Recall (micro-average) = 0.5277199621570482</span><br><span class="line">CC F1 Score (micro-average) = 0.6363221537759525</span><br><span class="line">===================================</span><br><span class="line">LP Training Time: 74.27938294410706</span><br><span class="line">LP Accuracy = 0.5349225268176401</span><br><span class="line">LP Precision (micro-average) = 0.7178777393310265</span><br><span class="line">LP Recall (micro-average) = 0.5888363292336802</span><br><span class="line">LP F1 Score (micro-average) = 0.646985446985447</span><br></pre></td></tr></table></figure><h4 id="结果分析">结果分析</h4><p>可以看到，LP不仅训练时间最短，而且Acc和F1都要更好。因此，我们可以继续探究使用支持向量机（SVM）作为基分类器的效果。</p><h3id="进一步探索--使用svm的标签幂集方法">进一步探索--使用SVM的标签幂集方法</h3><h4 id="实验设计">实验设计</h4><ul><li><strong>基分类器更换</strong>：鉴于LP方法的成功，我们决定用SVM替换原先的随机森林分类器，以进一步探索不同基分类器对多标签分类任务性能的影响。</li><li><strong>SVM配置</strong>：我们选择了线性核的SVM，并将其包装在<code>OneVsRestClassifier</code>中，以适应多类别问题。线性核是因其在处理高维数据时的有效性和计算效率而被选用。</li></ul><h4 id="训练和评估">训练和评估</h4><ul><li><p><strong>模型训练</strong>：使用LP方法结合SVM分类器训练模型，并记录了训练时间。</p></li><li><p><strong>性能评估</strong>：在测试集上评估了模型的准确度、精确度、召回率和F1分数（均采用微观平均）。这些指标有助于我们全面了解SVM在多标签分类任务中的表现。</p></li><li><p><strong>训练时间对比</strong>：与之前使用随机森林的LP方法相比，我们特别关注SVM版本的训练时间，以评估其在实际应用中的效率。</p><hr /></li></ul><h4 id="代码部分-3">代码部分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use SVM as the base classifier</span></span><br><span class="line">svm_classifier = OneVsRestClassifier(SVC(kernel=<span class="string">&#x27;linear&#x27;</span>))  <span class="comment"># The kernel function uses a linear function.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Label Powerset with SVM</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">lp_svm_classifier = LabelPowerset(svm_classifier)</span><br><span class="line">lp_svm_classifier.fit(X_train, y_train)</span><br><span class="line">lp_svm_training_time = time.time() - start_time</span><br><span class="line">lp_svm_predictions = lp_svm_classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Training Time:&quot;</span>, lp_svm_training_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Accuracy =&quot;</span>, accuracy_score(y_test, lp_svm_predictions))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Precision (micro-average) =&quot;</span>, precision_score(y_test, lp_svm_predictions, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM Recall (micro-average) =&quot;</span>, recall_score(y_test, lp_svm_predictions, average=<span class="string">&#x27;micro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LP-SVM F1 Score (micro-average) =&quot;</span>, f1_score(y_test, lp_svm_predictions, average=<span class="string">&#x27;micro&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">===================================</span><br><span class="line">LP-SVM Training Time: <span class="number">13640.821268558502</span></span><br><span class="line">LP-SVM Accuracy = <span class="number">0.5914183551847437</span></span><br><span class="line">LP-SVM Precision (micro-average) = <span class="number">0.7367712141620165</span></span><br><span class="line">LP-SVM Recall (micro-average) = <span class="number">0.7245033112582782</span></span><br><span class="line">LP-SVM F1 Score (micro-average) = <span class="number">0.7305857660751764</span></span><br></pre></td></tr></table></figure><h4 id="结果分析-1">结果分析</h4><p>可以看到，LP-SVM的训练时间比使用随机森林的LP长了184倍，但所有评估标准都比使用随机森林的LP好。显然，它是我们本篇中最好的模型。</p><h1 id="总结">总结</h1><p>本篇为《【NLP】多标签分类》的上篇，本文详细细探讨了多标签分类问题，聚焦于三种机器学习方法（BinaryRelevance, Classifier Chains, LabelPowerset），展示了每种方法的原理、优缺点，以及具体的实验评估和代码实现。本文还探讨了如何使用XL-NET做嵌入。实验结果表明，标签幂集方法配合随机森林分类器在训练时间和性能（准确度和F1分数）上表现良好。进一步探索使用SVM作为基分类器后，虽然训练时间增长，但所有评估标准均有所提升，显示出更好的性能。文章通过详细的实验步骤和评估方法，为选择适合特定多标签分类任务的方法提供了实证依据。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLP实战】基于Bert和双向LSTM的情感分类【下篇】</title>
      <link href="/article/4b6ff6ea27bb/"/>
      <url>/article/4b6ff6ea27bb/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>本文涉及的代码全由博主自己完成，可以随意拿去做参考。如对代码有不懂的地方请联系博主。</p><p>博主page：<a href="https://www.issey.top/">issey的博客 -愿无岁月可回首</a></p><p>本系列文章中不会说明环境和包如何安装，这些应该是最基础的东西，可以自己边查边安装。</p><p>许多函数用法等在代码里有详细解释，但还是希望各位去看它们的官方文档，我的代码还有很多可以改进的方法，需要的函数等在官方文档都有说明。</p><h1 id="简介">简介</h1><p>本系列将带领大家从数据获取、数据清洗，模型构建、训练，观察loss变化，调整超参数再次训练，并最后进行评估整一个过程。我们将获取一份公开竞赛中文数据，并一步步实验，到最后，我们的评估可以达到<strong>排行榜13</strong>位的位置。但重要的不是排名，而是我们能在其中学到很多。</p><p>本系列共分为三篇文章，分别是：</p><ul><li>上篇：数据获取，数据分割与数据清洗</li><li>中篇：模型构建，改进pytorch结构，开始第一次训练</li><li>下篇：测试与评估，绘图与过拟合，超参数调整</li></ul><p>本文为该系列第三篇文章，也是最后一篇。本文共分为两部分，在第一部分，我们将学习如何使用<code>pytorch lightning</code>保存模型的机制、如何读取模型与对测试集做测试。第二部分，我们将探讨前文遇到的过拟合问题，调整我们的超参数，进行第二轮训练，并对比两次训练的区别。我们还将基于<code>pytorch lightning</code>实现回调函数，保存训练过程中<code>val_loss</code>最小的模型。最后，将我们第二轮训练的<code>best model</code>进行评估，这一次，模型在测试集上的表现将达到排行榜第<strong>13</strong>位。</p><h1 id="第一部分">第一部分</h1><h2 id="关于pytorch-lightning保存模型的机制">关于pytorchlightning保存模型的机制</h2><p>官方文档：<ahref="https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html">Savingand loading checkpoints (basic) — PyTorch Lightning 2.0.1documentation</a></p><p>简单来说，每次用lightning进行训练时，他都会自动保存最近epoch训练出的model参数在<code>checkpoints</code>里。而<code>checkpoints</code>默认在<code>lightning_logs</code>目录下。</p><p><img src="https://img.issey.top/img/202304102333531.png" /></p><p>你还可以同时保存某次训练的参数，或者写<strong>回调函数</strong>改变它保存模型的机制（这个我们待会儿会用到）。当然你也可以设置不让它自动保存模型。这一切都在官方文档里。博主就不细讲这些细节了，建议读者自己做实验。</p><p>现在我们知道了重要的两件事：</p><ol type="1"><li>默认情况下，它会自动保存最近一次epoch训练结束后的模型。</li><li>我们只需要写回调函数，就可以改变它保存模型的机制。</li></ol><h2 id="关于如何读取保存好的模型">关于如何读取保存好的模型</h2><p>官方文档：<ahref="https://lightning.ai/docs/pytorch/stable/deploy/production_basic.html">Deploymodels into production (basic) — PyTorch Lightning 2.0.1documentation</a></p><p>根据文档，你还可以不用pytorchlightning，将模型读取到单纯的pytorch中，也可以使用。</p><p>感觉这部分讲的有点水？因为都在文档里，感觉没有需要逐一说明的地方。</p><p>现在，完善我们进行测试的代码。</p><h2 id="完善测试代码">完善测试代码</h2><p>有几点需要说明：我们在测试时还计算了常用的评估标准：acc，recall，pre，f1。这里博主将通常需要用到的评估标准写法逐一列出了。我是根据函数说明一点一点摸索出来的，所以一并写出来方便以后用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset  <span class="comment"># hugging-face dataset</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> torchmetrics.functional <span class="keyword">import</span> accuracy, recall, precision, f1_score  <span class="comment"># lightning中的评估</span></span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks.early_stopping <span class="keyword">import</span> EarlyStopping</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo：自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path, split</span>):</span><br><span class="line">        self.dataset = load_dataset(<span class="string">&#x27;csv&#x27;</span>, data_files=path, split=split)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        text = self.dataset[item][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        label = self.dataset[item][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    labels = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词并编码</span></span><br><span class="line">    data = token.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=sents,  <span class="comment"># 单个句子参与编码</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 当句子长度大于max_length时,截断</span></span><br><span class="line">        padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 一律补pad到max_length长度</span></span><br><span class="line">        max_length=<span class="number">200</span>,</span><br><span class="line">        return_tensors=<span class="string">&#x27;pt&#x27;</span>,  <span class="comment"># 以pytorch的形式返回，可取值tf,pt,np,默认为返回list</span></span><br><span class="line">        return_length=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment"># attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]  <span class="comment"># input_ids 就是编码后的词</span></span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]  <span class="comment"># pad的位置是0,其他位置是1</span></span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]  <span class="comment"># (如果是一对句子)第一个句子和特殊符号的位置是0,第二个句子的位置是1</span></span><br><span class="line">    labels = torch.LongTensor(labels)  <span class="comment"># 该批次的labels</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义模型，上游使用bert预训练，下游任务选择双向LSTM模型，最后加一个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTMClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTMClassifier, self).__init__()</span><br><span class="line">        self.drop = drop</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载bert中文模型,生成embedding层</span></span><br><span class="line">        self.embedding = BertModel.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">        <span class="comment"># 去掉移至gpu</span></span><br><span class="line">        <span class="comment"># 冻结上游模型参数(不进行预训练模型参数学习)</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 生成下游RNN层以及全连接层</span></span><br><span class="line">        self.lstm = nn.LSTM(input_size=<span class="number">768</span>, hidden_size=self.hidden_dim, num_layers=<span class="number">2</span>, batch_first=<span class="literal">True</span>,</span><br><span class="line">                            bidirectional=<span class="literal">True</span>, dropout=self.drop)</span><br><span class="line">        self.fc = nn.Linear(self.hidden_dim * <span class="number">2</span>, self.output_dim)</span><br><span class="line">        <span class="comment"># 使用CrossEntropyLoss作为损失函数时，不需要激活。因为实际上CrossEntropyLoss将softmax-log-NLLLoss一并实现的。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        embedded = self.embedding(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line">        embedded = embedded.last_hidden_state  <span class="comment"># 第0维才是我们需要的embedding,embedding.last_hidden_state = embedding[0]</span></span><br><span class="line">        out, (h_n, c_n) = self.lstm(embedded)</span><br><span class="line">        output = torch.cat((h_n[-<span class="number">2</span>, :, :], h_n[-<span class="number">1</span>, :, :]), dim=<span class="number">1</span>)</span><br><span class="line">        output = self.fc(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义pytorch lightning</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTMLighting</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTMLighting, self).__init__()</span><br><span class="line">        self.model = BiLSTMClassifier(drop, hidden_dim, output_dim)  <span class="comment"># 设置model</span></span><br><span class="line">        self.criterion = nn.CrossEntropyLoss()  <span class="comment"># 设置损失函数</span></span><br><span class="line">        self.train_dataset = MydataSet(<span class="string">&#x27;./data/archive/train_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">        self.val_dataset = MydataSet(<span class="string">&#x27;./data/archive/val_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">        self.test_dataset = MydataSet(<span class="string">&#x27;./data/archive/test_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        optimizer = optim.AdamW(self.parameters(), lr=lr)</span><br><span class="line">        <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):  <span class="comment"># forward(self,x)</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        train_loader = DataLoader(dataset=self.train_dataset, batch_size=batch_size, collate_fn=collate_fn,</span><br><span class="line">                                  shuffle=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> train_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids, attention_mask, token_type_ids, labels = batch  <span class="comment"># x, y = batch</span></span><br><span class="line">        y = one_hot(labels + <span class="number">1</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 将one_hot_labels类型转换成float</span></span><br><span class="line">        y = y.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># forward pass</span></span><br><span class="line">        y_hat = self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        y_hat = y_hat.squeeze()  <span class="comment"># 将[128, 1, 3]挤压为[128,3]</span></span><br><span class="line">        loss = self.criterion(y_hat, y)  <span class="comment"># criterion(input, target)</span></span><br><span class="line">        self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)  <span class="comment"># 将loss输出在控制台</span></span><br><span class="line">        <span class="keyword">return</span> loss  <span class="comment"># 必须把log返回回去才有用</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">val_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        val_loader = DataLoader(dataset=self.val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> val_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids, attention_mask, token_type_ids, labels = batch</span><br><span class="line">        y = one_hot(labels + <span class="number">1</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">        y = y.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># forward pass</span></span><br><span class="line">        y_hat = self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        y_hat = y_hat.squeeze()</span><br><span class="line">        loss = self.criterion(y_hat, y)</span><br><span class="line">        self.log(<span class="string">&#x27;val_loss&#x27;</span>, loss, prog_bar=<span class="literal">False</span>, logger=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        test_loader = DataLoader(dataset=self.test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> test_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids, attention_mask, token_type_ids, labels = batch</span><br><span class="line">        target = labels + <span class="number">1</span>  <span class="comment"># 用于待会儿计算acc和f1-score</span></span><br><span class="line">        y = one_hot(target, num_classes=<span class="number">3</span>)</span><br><span class="line">        y = y.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># forward pass</span></span><br><span class="line">        y_hat = self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        y_hat = y_hat.squeeze()</span><br><span class="line">        pred = torch.argmax(y_hat, dim=<span class="number">1</span>)</span><br><span class="line">        acc = (pred == target).<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line">        loss = self.criterion(y_hat, y)</span><br><span class="line">        self.log(<span class="string">&#x27;loss&#x27;</span>, loss)</span><br><span class="line">        <span class="comment"># task: Literal[&quot;binary&quot;, &quot;multiclass&quot;, &quot;multilabel&quot;],对应[二分类，多分类，多标签]</span></span><br><span class="line">        <span class="comment">#  average=None分别输出各个类别, 不加默认算平均</span></span><br><span class="line">        re = recall(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num, average=<span class="literal">None</span>)</span><br><span class="line">        pre = precision(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num, average=<span class="literal">None</span>)</span><br><span class="line">        f1 = f1_score(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num, average=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">log_score</span>(<span class="params">name, scores</span>):</span><br><span class="line">            <span class="keyword">for</span> i, score_class <span class="keyword">in</span> <span class="built_in">enumerate</span>(scores):</span><br><span class="line">                self.log(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>_class<span class="subst">&#123;i&#125;</span>&quot;</span>, score_class)</span><br><span class="line"></span><br><span class="line">        log_score(<span class="string">&quot;recall&quot;</span>, re)</span><br><span class="line">        log_score(<span class="string">&quot;precision&quot;</span>, pre)</span><br><span class="line">        log_score(<span class="string">&quot;f1&quot;</span>, f1)</span><br><span class="line">        self.log(<span class="string">&#x27;acc&#x27;</span>, accuracy(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num))</span><br><span class="line">        self.log(<span class="string">&#x27;avg_recall&#x27;</span>, recall(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num, average=<span class="string">&quot;weighted&quot;</span>))</span><br><span class="line">        self.log(<span class="string">&#x27;avg_precision&#x27;</span>, precision(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num, average=<span class="string">&quot;weighted&quot;</span>))</span><br><span class="line">        self.log(<span class="string">&#x27;avg_f1&#x27;</span>, f1_score(pred, target, task=<span class="string">&quot;multiclass&quot;</span>, num_classes=class_num, average=<span class="string">&quot;weighted&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    <span class="comment"># 加载之前训练好的最优模型参数</span></span><br><span class="line">    model = BiLSTMLighting.load_from_checkpoint(checkpoint_path=PATH,</span><br><span class="line">                                                drop=dropout, hidden_dim=rnn_hidden, output_dim=class_num)</span><br><span class="line">    trainer = Trainer(fast_dev_run=<span class="literal">False</span>)</span><br><span class="line">    result = trainer.test(model)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输出：也就是上一篇末尾提前剧透的截图。</p><p><img src="https://img.issey.top/img/202304101623410.png" /></p><h1 id="第二部分">第二部分</h1><h2id="第一次训练出的模型的过拟合问题">第一次训练出的模型的过拟合问题</h2><p>为什么提到之前的模型有过拟合问题呢？让我们打开tensorboard，观察<code>train_loss</code>和<code>val_loss</code>。</p><p><img src="https://img.issey.top/img/202304102357646.png" /></p><p>train_loss还没有收敛的趋势，但是val_loss已经出现了反弹的趋势。如果这还不算过拟合的预兆，博主做了第二个实验，我读取了第一次模型训练好的参数，并在次基础上继续训练，于是出现了以下的图像：</p><p><img src="https://img.issey.top/img/202304102359610.png" /></p><p>红色的线。可以看到，train_loss跟着橙色的线继续下降的，而val_loss直线上升，并且train_loss低于0.3时，val_loss高达0.9+。于是我们可以断定，过拟合了！</p><h2 id="如何解决过拟合">如何解决过拟合</h2><p>最简单的方式是调参，我将batch_size由128调整到了256，将drop从0.4调整到了0.5，再次进行训练。同时，为了防止第二次也过拟合，我加入了回调函数，<strong>这个回调函数将保存过拟合之前最好的一组模型</strong>。这个回调函数的作用极为重要。下面给出最终版本的train代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="comment"># 增加过拟合回调函数,提前停止,经过测试发现不太好用，因为可能会停止在局部最优值</span></span><br><span class="line">    early_stop_callback = EarlyStopping(</span><br><span class="line">        monitor=<span class="string">&#x27;val_loss&#x27;</span>,  <span class="comment"># 监控对象为&#x27;val_loss&#x27;</span></span><br><span class="line">        patience=<span class="number">4</span>,  <span class="comment"># 耐心观察4个epoch</span></span><br><span class="line">        min_delta=<span class="number">0.0</span>,  <span class="comment"># 默认为0.0，指模型性能最小变化量</span></span><br><span class="line">        verbose=<span class="literal">True</span>,  <span class="comment"># 在输出中显示一些关于early stopping的信息，如为何停止等</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 增加回调最优模型，这个比较好用</span></span><br><span class="line">    checkpoint_callback = ModelCheckpoint(</span><br><span class="line">        monitor=<span class="string">&#x27;val_loss&#x27;</span>,  <span class="comment"># 监控对象为&#x27;val_loss&#x27;</span></span><br><span class="line">        dirpath=<span class="string">&#x27;checkpoints/&#x27;</span>,  <span class="comment"># 保存模型的路径</span></span><br><span class="line">        filename=<span class="string">&#x27;model-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,  <span class="comment"># 最优模型的名称</span></span><br><span class="line">        save_top_k=<span class="number">1</span>,  <span class="comment"># 只保存最好的那个  </span></span><br><span class="line">        mode=<span class="string">&#x27;min&#x27;</span>  <span class="comment"># 当监控对象指标最小时</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Trainer可以帮助调试，比如快速运行、只使用一小部分数据进行测试、完整性检查等，</span></span><br><span class="line">    <span class="comment"># 详情请见官方文档https://lightning.ai/docs/pytorch/latest/debug/debugging_basic.html</span></span><br><span class="line">    <span class="comment"># auto自适应gpu数量</span></span><br><span class="line">    trainer = Trainer(max_epochs=epochs, log_every_n_steps=<span class="number">10</span>, accelerator=<span class="string">&#x27;gpu&#x27;</span>, devices=<span class="string">&quot;auto&quot;</span>, fast_dev_run=<span class="literal">False</span>,</span><br><span class="line">                      precision=<span class="number">16</span>, callbacks=[checkpoint_callback])</span><br><span class="line">    model = BiLSTMLighting(drop=dropout, hidden_dim=rnn_hidden, output_dim=class_num)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"><span class="comment"># todo:定义超参数</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"> epochs = <span class="number">30</span></span><br><span class="line">    dropout = <span class="number">0.5</span></span><br><span class="line">    rnn_hidden = <span class="number">768</span></span><br><span class="line">    rnn_layer = <span class="number">1</span></span><br><span class="line">    class_num = <span class="number">3</span></span><br><span class="line">    lr = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">    PATH = <span class="string">&#x27;PATH&#x27;</span></span><br><span class="line">    token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    train()</span><br><span class="line">    <span class="comment"># test()</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>把他加入到上面的代码就行了。</p><p>关于回调函数的说明在代码里。</p><p>在第二天早上，我拿到了这次训练的结果：</p><p><img src="https://img.issey.top/img/202304110009198.png" /></p><p>对比第一个模型：</p><p><img src="https://img.issey.top/img/202304110012202.png" /></p><p>好吧，这次还是过拟合了，而且trainloss居然低于了0.1，说明模型太复杂了。不过！由于我们的回调函数的存在，我们及时保存了val_loss最小时的模型。现在，将我们的模型路径换成bestmodel,再次对测试集进行评估，我们会得到以下结果：</p><p><img src="https://img.issey.top/img/202304110011051.png" /></p><p>现在，它在排行榜第13位。</p><p><img src="https://img.issey.top/img/202304110013539.png" /></p><h1 id="后记">后记</h1><p>终于写完了，一天肝完三篇文章。虽然前面实验时在边实验边记录，所以写的比较快。</p><p>好像也没什么要写成后记的，该说的也都说完了。这三篇文章，其实就是这次实验的后记（笑）。</p><p>歇一歇，累~</p><p>还有很多不知道和要改进的地方，继续努力吧。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLP实战】基于Bert和双向LSTM的情感分类【中篇】</title>
      <link href="/article/b9ce6b3f66fa/"/>
      <url>/article/b9ce6b3f66fa/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>本文涉及的代码全由博主自己完成，可以随意拿去做参考。如对代码有不懂的地方请联系博主。</p><p>相关代码以同步至github(2024/1/23):<ahref="https://github.com/iceissey/issey_Kaggle/tree/main/Bert_BiLSTM">issey_Kaggle/Bert_BiLSTM</a></p><p>博主page：<a href="https://www.issey.top/">issey的博客 -愿无岁月可回首</a></p><p>本系列文章中不会说明环境和包如何安装，这些应该是最基础的东西，可以自己边查边安装。</p><p>许多函数用法等在代码里有详细解释，但还是希望各位去看它们的官方文档，我的代码还有很多可以改进的方法，需要的函数等在官方文档都有说明。</p><h1 id="简介">简介</h1><p>本系列将带领大家从数据获取、数据清洗，模型构建、训练，观察loss变化，调整超参数再次训练，并最后进行评估整一个过程。我们将获取一份公开竞赛中文数据，并一步步实验，到最后，我们的评估可以达到<strong>排行榜13</strong>位的位置。但重要的不是排名，而是我们能在其中学到很多。</p><p>本系列共分为三篇文章，分别是：</p><ul><li>上篇：数据获取，数据分割与数据清洗</li><li>中篇：模型构建，改进pytorch结构，开始第一次训练</li><li>下篇：测试与评估，绘图与过拟合，超参数调整</li></ul><p>本文为该系列第二篇文章，在本文中，我们将学习如何用<code>pytorch</code>搭建我们需要的神经网络，如何用<code>pytorch lightning</code>改造我们的<code>trainer</code>，并开始在<code>GPU</code>环境我们第一次正式的训练。在这篇文章的末尾，我们的模型在测试集上的表现将达到排行榜<strong>28</strong>名的位置。</p><p>注意：本文不会写到训练好的模型读取与测试集测试，这将在下一篇文章详细说明。</p><h1 id="模型优化器与损失函数选择">模型、优化器与损失函数选择</h1><h2 id="神经网络的整体结构">神经网络的整体结构</h2><ul><li><p>预训练：我们将使用<code>Bert</code>对我们的text构建<code>Word Embedding</code>（词向量）。</p><p>关于什么是<code>word embedding</code>，这是nlp最基础的知识，<strong>在斋藤康毅的《深度学习进阶：自然语言处理》中，有详细说明，这本书很好，强烈推荐作为入门书籍</strong>。什么是<code>bert</code>，各位可以自行百度。前置知识太多，哪怕是简要说明，也要写一大堆。本文的重点在于实战而非理论学习，如果对这方面感兴趣，这里推荐一个社区：<code>hugging face</code>。</p></li><li><p>下游模型：<code>BiLSTM</code>(双向LSTM)。</p><p><code>lstm</code>是<code>RNN</code>的改进版，由于存在<code>梯度消失</code>和<code>梯度爆炸</code>问题，<code>RNN</code>模型的记忆很短，而<code>LSTM</code>的记忆较长。但<code>lstm</code>仍然存在<code>梯度消失</code>和<code>梯度爆炸</code>。近几年出现的<code>transformer</code>可以有效解决这个问题。<code>transformer</code>也是<code>bert</code>的前置知识之一。这里就不进行拓展了。感兴趣的读者可以尽情把<code>lstm</code>换成<code>transformer</code>，看看评估结果会不会更好。</p><p>关于<code>RNN</code>,在《深度学习进阶》一书中也有详细说明。</p></li></ul><h2 id="优化器选择">优化器选择</h2><p>选择<code>AdamW</code>作为本次训练的优化器。</p><p>关于<code>SGD</code>，<code>AdaGrad</code>，<code>Adam</code>优化器，在<strong>斋藤康毅的《深度学习入门：基于python的理论和实现》</strong>一书中有详细说明。<code>AdamW</code>是<code>Adam</code>的改进版本之一。</p><h2 id="损失函数选择">损失函数选择</h2><p>选择<code>Cross Entropy Loss</code>作为损失函数。<code>Cross Entropy Loss</code>实际上包含了<code>Softmax</code>层的实现。这里不进行展开讨论。关于<code>Cross Entropy Loss</code>的详细实现也在《深度学习入门》一书中有详细说明。</p><h1 id="需要导入的包和说明">需要导入的包和说明</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset  <span class="comment"># hugging-face dataset</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> torchmetrics.functional <span class="keyword">import</span> accuracy, recall, precision, f1_score  <span class="comment"># lightning中的评估</span></span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks.early_stopping <span class="keyword">import</span> EarlyStopping</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li>torch : pytorch</li><li>datasets : <strong>这个datasets指的是huggingface的datasets</strong>。关于hugging face请各位自行搜索教程。huggingface是nlp领域知名的社区。</li><li>transformer：huggingface的核心模块。<code>搜索：如何安装transformer</code></li><li>pytorch_lightning：更好的管理你的pytroch流程。在《深度学习进阶》一书中，提到过<code>Trainer</code>用于管理训练流程，但是pytorch本身并没有实现<code>Trianer</code>。pytorch_lightning专为此而生。除此之外，它还为记录train_loss,val_loss，评估等提供了便捷的方法。详细的待会儿用到了说。</li></ul><p><strong>需要加载的数据与模型</strong></p><ul><li>Bert数据包：bert-base-cased，为了方便的加载这个数据包，请在Pycharm中设置代理。如果连接不上，就自行搜索本地加载方法。</li></ul><p>现在，让我们开始写代码吧！</p><h1 id="第一部分搭建整体结构">第一部分：搭建整体结构</h1><h2 id="step1-定义dataset加载数据">step1: 定义DataSet，加载数据</h2><p>pytorch框架第一步：自定义数据集。如果这个有疑问，需要去看看pytorch基础。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset  <span class="comment"># hugging-face dataset</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path, split</span>):</span><br><span class="line">        self.dataset = load_dataset(<span class="string">&#x27;csv&#x27;</span>, data_files=path, split=split)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        text = self.dataset[item][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        label = self.dataset[item][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># load train data</span></span><br><span class="line">    train_dataset = MydataSet(<span class="string">&#x27;./data/archive/train_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(train_dataset.__len__())</span><br><span class="line">    <span class="built_in">print</span>(train_dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>解释一下load_dataset中的<code>split</code>，他是分割文档的意思，但是我们这里是从本地读取<code>csv</code>,文件格式并不是huggingface的标准。所以之后无论是验证集还是测试集，我们都统一填<code>'train'</code>。更多详情请访问官方文档。</p><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">55223</span><br><span class="line">(<span class="string">&#x27;我妈大早上去买了好几盒维C维E，说专家说了每天吃能预防肺炎还要去买金银花，一本正经的听专家讲座，还拿个小本子记笔记也是很认真了?&#x27;</span>, 1.0)</span><br></pre></td></tr></table></figure><h2id="step2装载dataloader定义批处理函数">step2:装载dataloader，定义批处理函数</h2><p>这个批处理函数主要做的事情是：使用<code>bert-base-chinese</code>对字典将我们的text进行编码，详细不展开拓展，请花时间去大致了解bert都做了些什么，bert如何使用。简单来说，bert每个模型自己有一个字典，我们映射text也是映射到它的字典上去。</p><p>如果字典上没有的字符，会映射成<code>[UNK]</code>。所以之前我们数据清洗时没有去除特殊字符。</p><p>其他的解释都在代码的注释里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo：自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    labels = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词并编码</span></span><br><span class="line">    data = token.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=sents,  <span class="comment"># 单个句子参与编码</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 当句子长度大于max_length时,截断</span></span><br><span class="line">        padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 一律补pad到max_length长度</span></span><br><span class="line">        max_length=<span class="number">200</span>,</span><br><span class="line">        return_tensors=<span class="string">&#x27;pt&#x27;</span>,  <span class="comment"># 以pytorch的形式返回，可取值tf,pt,np,默认为返回list</span></span><br><span class="line">        return_length=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment"># attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]  <span class="comment"># input_ids 就是编码后的词</span></span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]  <span class="comment"># pad的位置是0,其他位置是1</span></span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]  <span class="comment"># (如果是一对句子)第一个句子和特殊符号的位置是0,第二个句子的位置是1</span></span><br><span class="line">    labels = torch.LongTensor(labels)  <span class="comment"># 该批次的labels</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 定义超参数</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    epochs = <span class="number">30</span></span><br><span class="line">    <span class="comment"># load train data</span></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># todo: 加载字典和分词工具</span></span><br><span class="line">    token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    <span class="comment"># 装载训练集</span></span><br><span class="line">    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">True</span>,</span><br><span class="line">                              drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 检查一个批次是否编码成功</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">            labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># print(len(train_loader))</span></span><br><span class="line">        <span class="built_in">print</span>(input_ids[<span class="number">0</span>])  <span class="comment"># 第一句话分词后在bert-base-chinese字典中的word_to_id</span></span><br><span class="line">        <span class="built_in">print</span>(token.decode(input_ids[<span class="number">0</span>]))  <span class="comment"># 检查第一句话的id_to_word</span></span><br><span class="line">        <span class="built_in">print</span>(input_ids.shape)  <span class="comment"># 一个批次32句话，每句话被word_to_id成500维</span></span><br><span class="line">        <span class="comment"># print(attention_mask.shape)  # 对于使用者而言，不是重要的。含义上面有说明，感兴趣可以做实验测试</span></span><br><span class="line">        <span class="comment"># print(token_type_ids.shape)  # 对于使用者而言，不是重要的。含义上面有说明，感兴趣可以做实验测试</span></span><br><span class="line">        <span class="built_in">print</span>(labels)  <span class="comment"># 该批次的labels</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304080247611.png" /></p><p>因为没有指定随机种子，每次结果可能不一样。需要注意的是<code>input_ids.shape</code>需要与批处理函数中的规则对应。</p><h2id="step3生成层--预训练模块测试word-embedding">step3:生成层--预训练模块，测试wordembedding</h2><p>这里就直接上代码了，pytorch基础部分不讲解。其他地方都在代码里有说明。</p><p>注意我们冻结了上游参数，这样bert层就不会更新参数了。当然你也可以试试<code>微调</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># todo：自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义模型，上游使用bert预训练，下游任务选择双向LSTM模型，最后加一个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        <span class="comment"># todo: 加载bert中文模型</span></span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">        <span class="comment"># 冻结上游模型参数(不进行预训练模型参数学习)</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="comment"># bert预训练</span></span><br><span class="line">        embedding = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line">        <span class="comment"># print(embedding)</span></span><br><span class="line">        <span class="comment"># print(&quot;===============&quot;)</span></span><br><span class="line">        <span class="comment"># print(embedding[0])</span></span><br><span class="line">        <span class="comment"># print(embedding[0].shape)</span></span><br><span class="line">        embedding = embedding.last_hidden_state  <span class="comment"># 第0维才是我们需要的embedding,embedding.last_hidden_state 和 embedding[0] 效果是一样的。</span></span><br><span class="line">        <span class="comment"># print(&quot;--------------------------------&quot;)</span></span><br><span class="line">        <span class="built_in">print</span>(embedding)</span><br><span class="line">        <span class="built_in">print</span>(embedding.shape)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 定义超参数</span></span><br><span class="line">...</span><br><span class="line">    <span class="comment"># 检查一个批次是否编码成功</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">    model = Model()</span><br><span class="line">    <span class="comment"># 测试</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        model.forward(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304101540144.png" /></p><p>这里我们一个batch_size是128。注意<code>embedding.shape</code>的输出，是<code>[128,200,768]</code>。代表128句话，每句话200个词，每个词被构建成了768维度的<code>Word Embedding</code>(词向量)。接下来，我们就可以把它们丢进LSTM层了。</p><h2id="step4生成层--bilstm和全连接层测试forward">step4:生成层--BiLSTM和全连接层，测试forward</h2><blockquote><p>于4.13日修改forward output输出</p></blockquote><p>我们还在这部分选择了损失函数和优化器。</p><p>首先说明两个易错的点：</p><h3id="问题1使用cross-entropy-loss到底需不需要在forward时经过softmax层">问题1：使用CrossEntropy Loss到底需不需要在forward时经过softmax层？</h3><p>关于这个问题的讨论：</p><p><ahref="https://stackoverflow.com/questions/55675345/should-i-use-softmax-as-output-when-using-cross-entropy-loss-in-pytorch">python- Should I use softmax as output when using cross entropy loss inpytorch? - Stack Overflow</a></p><p>之前说了，<code>Cross Entropy Loss</code>本身已经实现了<code>Softmax</code>。所以forward时最后只需要经过全连接层就可以输出，当然你还可以再加一个<code>Relu</code>激活，我这里没有加。</p><h3id="问题2bilstm最后时刻的输出应该怎么取">问题2：bilstm最后时刻的输出应该怎么取？</h3><p><img src="https://img.issey.top/img/202304131806422.png" /></p><p>先来看看bilstm的图。我们需要注意的是，output第一层由<strong>正向lstm第一时刻状态</strong>和<strong>反向lstm最后时刻状态</strong>拼接而成；而output最后一层则由<strong>正向lstm最后时刻状态</strong>和<strong>反向lstm第一时刻状态</strong>拼接而成。所以我们在forward时简单取<code>output[:,-1,:]</code>并不是正确的。</p><p>下面，我们通过一个简单的实验证明以上结论。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset  <span class="comment"># hugging-face dataset</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo：自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义模型，上游使用bert预训练，下游任务选择双向LSTM模型，最后加一个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTM, self).__init__()</span><br><span class="line">        self.drop = drop</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载bert中文模型,生成embedding层</span></span><br><span class="line">        self.embedding = BertModel.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">        <span class="comment"># 冻结上游模型参数(不进行预训练模型参数学习)</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 生成下游RNN层以及全连接层</span></span><br><span class="line">        self.lstm = nn.LSTM(input_size=<span class="number">768</span>, hidden_size=self.hidden_dim, num_layers=<span class="number">1</span>, batch_first=<span class="literal">True</span>,</span><br><span class="line">                            bidirectional=<span class="literal">True</span>, dropout=self.drop)</span><br><span class="line">        self.fc = nn.Linear(self.hidden_dim * <span class="number">2</span>, self.output_dim)</span><br><span class="line">        <span class="comment"># 使用CrossEntropyLoss作为损失函数时，不需要激活。因为实际上CrossEntropyLoss将softmax-log-NLLLoss一并实现的。但是使用其他损失函数时还是需要加入softmax层的。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        embedded = self.embedding(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line">        embedded = embedded.last_hidden_state  <span class="comment"># 第0维才是我们需要的embedding,embedding.last_hidden_state = embedding[0]</span></span><br><span class="line">        out, (h_n, c_n) = self.lstm(embedded)</span><br><span class="line">        <span class="built_in">print</span>(out.shape)  <span class="comment"># [128, 200 ,1536]  因为是双向的，所以out是两个hidden_dim拼接而成。768*2 = 1536</span></span><br><span class="line">        <span class="comment"># h_n[-2, :, :] 为正向lstm最后一个隐藏状态。</span></span><br><span class="line">        <span class="comment"># h_n[-1, :, :] 为反向lstm最后一个隐藏状态</span></span><br><span class="line">        <span class="built_in">print</span>(out[:, -<span class="number">1</span>, :<span class="number">768</span>] == h_n[-<span class="number">2</span>, :, :])  <span class="comment"># 正向lstm最后时刻的输出在output最后一层</span></span><br><span class="line">        <span class="built_in">print</span>(out[:, <span class="number">0</span>, <span class="number">768</span>:] == h_n[-<span class="number">1</span>, :, :])  <span class="comment"># 反向lstm最后时刻的输出在output第一层</span></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304131819764.png" /></p><p>而我们现在同时需要正向lstm最后时刻的隐藏状态和反向lstm最后时刻的隐藏状态。<strong>于是，我们应该将两个hidden进行拼接作为输出。</strong></p><h3 id="生成层forward编写与测试">生成层，forward编写与测试</h3><p>修改刚才代码中的forward部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">    embedded = self.embedding(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line">    embedded = embedded.last_hidden_state  <span class="comment"># 第0维才是我们需要的embedding,embedding.last_hidden_state = embedding[0]</span></span><br><span class="line">    out, (h_n, c_n) = self.lstm(embedded)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)  <span class="comment"># [128, 200 ,1536]  因为是双向的，所以out是两个hidden_dim拼接而成。768*2 = 1536</span></span><br><span class="line">    <span class="comment"># h_n[-2, :, :] 为正向lstm最后一个隐藏状态。</span></span><br><span class="line">    <span class="comment"># h_n[-1, :, :] 为反向lstm最后一个隐藏状态</span></span><br><span class="line">    <span class="comment"># print(out[:, -1, :768] == h_n[-2, :, :])  # 正向lstm最后时刻的输出在output最后一层</span></span><br><span class="line">    <span class="comment"># print(out[:, 0, 768:] == h_n[-1, :, :])  # 反向lstm最后时刻的输出在output第一层</span></span><br><span class="line">    <span class="comment"># print(out[:, -1, :].shape)  # [128, 1536]  128句话，每句话的最后一个状态</span></span><br><span class="line">    output = torch.cat((h_n[-<span class="number">2</span>, :, :], h_n[-<span class="number">1</span>, :, :]), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    <span class="comment"># 检查是否拼接成功</span></span><br><span class="line">    <span class="built_in">print</span>(out[:, -<span class="number">1</span>, :<span class="number">768</span>] == output[:, :<span class="number">768</span>])</span><br><span class="line">    <span class="built_in">print</span>(out[:, <span class="number">0</span>, <span class="number">768</span>:] == output[:, <span class="number">768</span>:])</span><br><span class="line">    output = self.fc(output)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>在主函数中检查forward输出的shape。为了快速实验，此处把batch_size修改为10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># todo:定义超参数</span></span><br><span class="line">    batch_size = <span class="number">10</span></span><br><span class="line">    epochs = <span class="number">30</span></span><br><span class="line">    dropout = <span class="number">0.4</span></span><br><span class="line">    rnn_hidden = <span class="number">768</span></span><br><span class="line">    rnn_layer = <span class="number">1</span></span><br><span class="line">    class_num = <span class="number">3</span></span><br><span class="line">    lr = <span class="number">0.001</span></span><br><span class="line">...</span><br><span class="line">    out = model.forward(input_ids, attention_mask, token_type_ids)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304131848415.png" /></p><h2id="step5backward前置工作将labels进行one-hot">Step5:backward前置工作：将labels进行one-hot</h2><p>这件事留到现在才来做，其实最开始就可以做这件事。作为多分类模型的基础，我们需要把<span class="math display">\[\begin{split}&amp;class\_nums = 3,\\&amp;labels = [-1,-1,0,0,1,...]\end{split}\]</span> 改成: <span class="math display">\[\begin{split}&amp;class\_nums = 3,\\&amp;labels = [[1,0,0],[1,0,0],[0,1,0],[0,1,0],[0,0,1]...]\end{split}\]</span>Pytorch提供了<code>torch.nn.functional.one_hot</code>用于将上述功能，但是注意，one_hot的input中不应该包含负数。在上篇中，我们观察了类别为<code>-1，0，1</code>，只需要对每项加一，就可以变成我们需要的。<code>注意，对于更一般的情况，比如label为字符等，我们需要创建字典，将它们label to id得到对应的ids，再进行one-hot</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试labels2one-hot</span></span><br><span class="line">labels = torch.LongTensor([<span class="number">0</span>, -<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>])</span><br><span class="line">labels += <span class="number">1</span></span><br><span class="line">one_hot_labels = one_hot(labels,num_classes = <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(one_hot_labels)</span><br></pre></td></tr></table></figure><blockquote><p>思考：我们需要一次性对所有labels进行onehot还是对一个batch进行呢？</p><p>显然，对一个batch进行labels toonehot和对所有labels进行onehot效果是一样的。你可以选择先对所有labels做一个预处理，或者在批处理时一批一批的onehot。</p><p>考虑到内存等，这里我选择分批次处理。</p></blockquote><h2 id="step5backward测试">Step5:Backward测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    one_hot_labels = one_hot(labels+<span class="number">1</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 将one_hot_labels类型转换成float</span></span><br><span class="line">    one_hot_labels = one_hot_labels.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">    output = model.forward(input_ids, attention_mask, token_type_ids)  <span class="comment"># forward</span></span><br><span class="line">    <span class="comment"># output = output.squeeze()  # 将[128, 1, 3]挤压为[128,3]</span></span><br><span class="line">    loss = criterion(output, one_hot_labels)  <span class="comment"># 计算损失</span></span><br><span class="line">    <span class="built_in">print</span>(loss)</span><br><span class="line">    loss.backward()  <span class="comment"># backward,计算grad</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><p>跑了一个批次，没报错，并且观察到loss在逐渐变小即可。</p><p>loss变化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.0854</span>, grad_fn=&lt;DivBackward1&gt;)</span><br><span class="line">tensor(<span class="number">1.2250</span>, grad_fn=&lt;DivBackward1&gt;)</span><br><span class="line">tensor(<span class="number">1.0756</span>, grad_fn=&lt;DivBackward1&gt;)</span><br><span class="line">tensor(<span class="number">0.9926</span>, grad_fn=&lt;DivBackward1&gt;)</span><br><span class="line">tensor(<span class="number">0.8705</span>, grad_fn=&lt;DivBackward1&gt;)</span><br><span class="line">tensor(<span class="number">0.8499</span>, grad_fn=&lt;DivBackward1&gt;)</span><br></pre></td></tr></table></figure><p>ok，backward没问题。</p><p>但是很容易发现，计算速度太慢。于是下一步，我们需要将模型和数据放到GPU上运行。</p><h1 id="第二部分转移至gpu">第二部分：转移至GPU</h1><p>关于gpu环境的搭建等就不再赘述。</p><h2 id="检查gpu环境">检查gpu环境</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 设置GPU环境</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(device)</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304081614347.png" /></p><h2id="将cpu环境转换至gpu环境需要注意的地方">将cpu环境转换至gpu环境需要注意的地方</h2><ul><li><strong>检查GPU是否可用：</strong>首先需要检查系统中是否安装了CUDA以及GPU是否可用。可以使用<code>torch.cuda.is_available()</code>函数检查GPU是否可用。如果返回<code>True</code>，则说明GPU可以用于计算。</li><li><strong>将模型和数据移动到GPU：</strong>将模型和数据迁移到GPU需要使用<code>.to()</code>方法。例如，可以使用<code>model.to(device)</code>将模型移动到GPU环境。同样，将张量移动到GPU也需要使用相同的方法，例如<code>input.to(device)</code>。</li><li><strong>使用合适的数据类型：</strong>GPU只能使用特定的数据类型进行计算。在PyTorch中，如果需要将模型和数据移动到GPU，需要使用<code>FloatTensor()</code>或<code>LongTensor()</code>等数据类型。一般来说，<code>FloatTensor()</code>用于浮点数计算，<code>LongTensor()</code>用于整数计算。</li><li><strong>注意内存使用情况：</strong>在GPU环境下，需要注意内存使用情况。因为GPU的内存相对比较小，如果内存不足，程序可能会崩溃。在训练大型模型时，可以使用批处理(batching)的方法来减少内存占用。</li><li><strong>不要忘记将数据移回CPU：</strong>在使用GPU进行计算后，需要将结果数据移回CPU环境，以便保存或进一步操作。可以使用<code>.cpu()</code>方法将结果数据移回CPU环境。</li></ul><h2 id="转移模型与数据">转移模型与数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset  <span class="comment"># hugging-face dataset</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo：自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义模型，上游使用bert预训练，下游任务选择双向LSTM模型，最后加一个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTM, self).__init__()</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载bert中文模型,生成embedding层</span></span><br><span class="line">        self.embedding = BertModel.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">        <span class="comment"># 预处理模型需要转移至gpu</span></span><br><span class="line">        self.embedding.to(device)</span><br><span class="line">        <span class="comment"># 冻结上游模型参数(不进行预训练模型参数学习)</span></span><br><span class="line">         ...</span><br><span class="line">        <span class="comment"># 生成下游RNN层以及全连接层</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 设置GPU环境</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;device=&#x27;</span>, device)</span><br><span class="line">    <span class="comment"># todo:定义超参数</span></span><br><span class="line">...</span><br><span class="line">    <span class="comment"># load train data</span></span><br><span class="line">    train_dataset = MydataSet(<span class="string">&#x27;./data/archive/train_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    <span class="comment"># print(train_dataset.__len__())</span></span><br><span class="line">    <span class="comment"># print(train_dataset[0])</span></span><br><span class="line">    <span class="comment"># todo: 加载字典和分词工具</span></span><br><span class="line">    token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    <span class="comment"># 装载训练集</span></span><br><span class="line">    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = BiLSTM(drop=dropout, hidden_dim=rnn_hidden, output_dim=class_num)</span><br><span class="line">    <span class="comment"># 模型转移至gpu</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="comment"># 选择损失函数</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 选择优化器</span></span><br><span class="line">    optimizer = optim.AdamW(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 需要将所有数据转移到gpu</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        input_ids = input_ids.long().to(device)</span><br><span class="line">        attention_mask = attention_mask.long().to(device)</span><br><span class="line">        token_type_ids = token_type_ids.long().to(device)</span><br><span class="line">        labels = labels.long().to(device)</span><br><span class="line">        one_hot_labels = one_hot(labels+<span class="number">1</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 将one_hot_labels类型转换成float</span></span><br><span class="line">        one_hot_labels = one_hot_labels.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># print(one_hot_labels)</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        output = model.forward(input_ids, attention_mask, token_type_ids)  <span class="comment"># forward</span></span><br><span class="line">        <span class="comment"># output = output.squeeze()  # 将[128, 1, 3]挤压为[128,3]</span></span><br><span class="line">        loss = criterion(output, one_hot_labels)  <span class="comment"># 计算损失</span></span><br><span class="line">        <span class="built_in">print</span>(loss)</span><br><span class="line">        loss.backward()  <span class="comment"># backward,计算grad</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304081958310.png" /></p><p>可以看到损失在逐渐下降，并且速度提升了很多倍。并且device显示使用的cuda:0。</p><h1 id="第三部分pytorch-lightning-改造结构">第三部分：Pytorch lightning！改造结构</h1><blockquote><p>本来在这部分，我想完善epochs和batch的训练过程，并且记录loss和验证集的acc用于绘制图像，但在我写到一半是萌生了一个想法：有没有trainer？于是我去问了chat。它告诉我有个东西叫pytorch-lightning。里面就有trainer，可以便于我们管理我们的训练过程等。于是，我改变了这部分的目标。接下来，我们将学习如何使用pytorch-lightning改善我们结构，管理整理训练过程。</p><p>这个Lightning呢，最好是先去看看视频，然后跟着官方文档学。</p></blockquote><h2 id="参考教程">参考教程</h2><p>github：https://github.com/PyTorchLightning/pytorch-lightning</p><p>B站视频教学：<ahref="https://www.bilibili.com/video/BV1iz411q7Rt/?spm_id_from=333.337.search-card.all.click&amp;vd_source=747540861ba5c41c17852ccf069029f5">使用PyTorchLightning简化PyTorch代码_哔哩哔哩_bilibili</a></p><p><strong>官方文档</strong>（超级有用）：<ahref="https://lightning.ai/docs/pytorch/latest/levels/core_skills.html">Basicskills — PyTorch Lightning 2.1.0dev documentation</a></p><h2 id="改造结构">改造结构</h2><p>现在我们新创建一个文件，copy已经写好的代码，准备用pytorch-ligting去改造它，避免改造过程中失误。这里只放最终改造结果，其实博主看着文档和教程一步步改造，用的时间很多。但是也越来越熟练。</p><p>注意：改造时加入了交叉验证，便于后续观察过拟合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset  <span class="comment"># hugging-face dataset</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo:定义超参数</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">dropout = <span class="number">0.4</span></span><br><span class="line">rnn_hidden = <span class="number">768</span></span><br><span class="line">rnn_layer = <span class="number">1</span></span><br><span class="line">class_num = <span class="number">3</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo：自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MydataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path, split</span>):</span><br><span class="line">        self.dataset = load_dataset(<span class="string">&#x27;csv&#x27;</span>, data_files=path, split=split)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        text = self.dataset[item][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        label = self.dataset[item][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    labels = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词并编码</span></span><br><span class="line">    data = token.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=sents,  <span class="comment"># 单个句子参与编码</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 当句子长度大于max_length时,截断</span></span><br><span class="line">        padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 一律补pad到max_length长度</span></span><br><span class="line">        max_length=<span class="number">200</span>,</span><br><span class="line">        return_tensors=<span class="string">&#x27;pt&#x27;</span>,  <span class="comment"># 以pytorch的形式返回，可取值tf,pt,np,默认为返回list</span></span><br><span class="line">        return_length=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment"># attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]  <span class="comment"># input_ids 就是编码后的词</span></span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]  <span class="comment"># pad的位置是0,其他位置是1</span></span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]  <span class="comment"># (如果是一对句子)第一个句子和特殊符号的位置是0,第二个句子的位置是1</span></span><br><span class="line">    labels = torch.LongTensor(labels)  <span class="comment"># 该批次的labels</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义模型，上游使用bert预训练，下游任务选择双向LSTM模型，最后加一个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTMClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTMClassifier, self).__init__()</span><br><span class="line">        self.drop = drop</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载bert中文模型,生成embedding层</span></span><br><span class="line">        self.embedding = BertModel.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">        <span class="comment"># 去掉移至gpu</span></span><br><span class="line">        <span class="comment"># 冻结上游模型参数(不进行预训练模型参数学习)</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 生成下游RNN层以及全连接层</span></span><br><span class="line">        self.lstm = nn.LSTM(input_size=<span class="number">768</span>, hidden_size=self.hidden_dim, num_layers=<span class="number">2</span>, batch_first=<span class="literal">True</span>,</span><br><span class="line">                            bidirectional=<span class="literal">True</span>, dropout=self.drop)</span><br><span class="line">        self.fc = nn.Linear(self.hidden_dim * <span class="number">2</span>, self.output_dim)</span><br><span class="line">        <span class="comment"># 使用CrossEntropyLoss作为损失函数时，不需要激活。因为实际上CrossEntropyLoss将softmax-log-NLLLoss一并实现的。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        embedded = self.embedding(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line">        embedded = embedded.last_hidden_state  <span class="comment"># 第0维才是我们需要的embedding,embedding.last_hidden_state = embedding[0]</span></span><br><span class="line">        out, (h_n, c_n) = self.lstm(embedded)</span><br><span class="line">        output = torch.cat((h_n[-<span class="number">2</span>, :, :], h_n[-<span class="number">1</span>, :, :]), dim=<span class="number">1</span>)</span><br><span class="line">        output = self.fc(output)  </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 定义pytorch lightning</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTMLighting</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTMLighting, self).__init__()</span><br><span class="line">        self.model = BiLSTMClassifier(drop, hidden_dim, output_dim)  <span class="comment"># 设置model</span></span><br><span class="line">        self.criterion = nn.CrossEntropyLoss()  <span class="comment"># 设置损失函数</span></span><br><span class="line">        self.train_dataset = MydataSet(<span class="string">&#x27;./data/archive/train_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">        self.val_dataset = MydataSet(<span class="string">&#x27;./data/archive/val_clean.csv&#x27;</span>, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        optimizer = optim.AdamW(self.parameters(), lr=lr)</span><br><span class="line">        <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):  <span class="comment"># forward(self,x)</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids, attention_mask, token_type_ids, labels = batch  <span class="comment"># x, y = batch</span></span><br><span class="line">        y = one_hot(labels + <span class="number">1</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 将one_hot_labels类型转换成float</span></span><br><span class="line">        y = y.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># forward pass</span></span><br><span class="line">        y_hat = self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        <span class="comment"># y_hat = y_hat.squeeze()  # 将[128, 1, 3]挤压为[128,3]</span></span><br><span class="line">        loss = self.criterion(y_hat, y)  <span class="comment"># criterion(input, target)</span></span><br><span class="line">        self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)  <span class="comment"># 将loss输出在控制台</span></span><br><span class="line">        <span class="keyword">return</span> loss  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        train_loader = DataLoader(dataset=self.train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> train_loader</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_ids, attention_mask, token_type_ids, labels = batch</span><br><span class="line">        y = one_hot(labels + <span class="number">1</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">        y = y.to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># forward pass</span></span><br><span class="line">        y_hat = self.model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        <span class="comment"># y_hat = y_hat.squeeze()</span></span><br><span class="line">        loss = self.criterion(y_hat, y)</span><br><span class="line">        self.log(<span class="string">&#x27;val_loss&#x27;</span>, loss, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">val_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        val_loader = DataLoader(dataset=self.val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> val_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    <span class="comment"># Trainer可以帮助调试，比如快速运行、只使用一小部分数据进行测试、完整性检查等，</span></span><br><span class="line">    <span class="comment"># 详情请见官方文档https://lightning.ai/docs/pytorch/latest/debug/debugging_basic.html</span></span><br><span class="line">    <span class="comment"># auto自适应gpu数量</span></span><br><span class="line">    trainer = Trainer(max_epochs=<span class="number">2</span>, limit_train_batches=<span class="number">10</span>, limit_val_batches=<span class="number">5</span>, log_every_n_steps=<span class="number">1</span>,</span><br><span class="line">                      accelerator=<span class="string">&#x27;gpu&#x27;</span>, devices=<span class="string">&quot;auto&quot;</span>)</span><br><span class="line">    model = BiLSTMLighting(drop=dropout, hidden_dim=rnn_hidden, output_dim=class_num)</span><br><span class="line">    trainer.fit(model)</span><br></pre></td></tr></table></figure><p>注意这里<code>max_epochs=2,limit_train_batches=10, limit_val_batches=5</code>是在测试训练，不是正式训练。</p><h2 id="关于绘图工具">关于绘图工具</h2><p>这里把绘图工具也换成了<code>tensorbroad</code>，这是一款可以根据日志绘制准确率、loss等的工具。在安装了pytorch-lightning后，会自动安装。用法简单。</p><p>这里只贴启动代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=path/to/log/directory</span><br></pre></td></tr></table></figure><p>注意等号两边不要留空格。</p><p><img src="https://img.issey.top/img/202304101607743.png" /></p><h2 id="训练测试">训练测试</h2><p>上述代码trainer只用了一小部分数据集进行测试，先来看看控制台输出：</p><p><img src="https://img.issey.top/img/202304090034804.png" /></p><p>打开<code>transboard</code>的Web UI:</p><p><img src="https://img.issey.top/img/202304090035334.png" /></p><p>可以看到<code>train_loss_step</code>在按照我们期望的方式下降。</p><h1 id="第四部分正式训练">第四部分：正式训练</h1><p>该部分训练时间接近三小时，是本次实验的第一次训练。现在给出超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">epochs = <span class="number">10</span>  <span class="comment"># 刚才写的30</span></span><br><span class="line">dropout = <span class="number">0.4</span></span><br><span class="line">rnn_hidden = <span class="number">768</span></span><br><span class="line">rnn_layer = <span class="number">1</span></span><br><span class="line">class_num = <span class="number">3</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>不建议读者跟着一起训练，因为之后有改进版。只需要修改main，就可以正式训练.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    <span class="comment"># Trainer可以帮助调试，比如快速运行、只使用一小部分数据进行测试、完整性检查等，</span></span><br><span class="line">    <span class="comment"># 详情请见官方文档https://lightning.ai/docs/pytorch/latest/debug/debugging_basic.html</span></span><br><span class="line">    <span class="comment"># auto自适应gpu数量</span></span><br><span class="line">    trainer = Trainer(max_epochs=epochs, log_every_n_steps=<span class="number">10</span>, accelerator=<span class="string">&#x27;gpu&#x27;</span>, devices=<span class="string">&quot;auto&quot;</span>,fast_dev_run=<span class="literal">False</span>)</span><br><span class="line">    model = BiLSTMLighting(drop=dropout, hidden_dim=rnn_hidden, output_dim=class_num)</span><br><span class="line">    trainer.fit(model)</span><br></pre></td></tr></table></figure><hr /><p>too years later......</p><hr /><p>这篇文章没有讲到模型保存，代码里也没写，但是不用担心，pytorchlightning会帮我们自动保存，下篇会详细介绍如何保存训练过程中最好的模型，以及模型如何读取与测试。</p><p>第一次训练完毕。现在来看看我们tarin_loss和val_loss：</p><p><img src="https://img.issey.top/img/202304101620819.png" /></p><p><img src="https://img.issey.top/img/202304101621690.png" /></p><p>可以看到train loss依然在下降，但是valloss却已经开始上升。说明训练到后来，模型过拟合了，这也是为什么我们要接着调整超参数并进行第二次训练的原因。但这些是下一篇文章的内容。这里先剧透一下，这次训练的模型拿去做测试，评估结果如下:</p><h2 id="第一次训练的测试结果">第一次训练的测试结果</h2><p><img src="https://img.issey.top/img/202304101623410.png" /></p><p>这个结果在竞赛排行榜上排在28名的位置（评估标准为f1score）。</p><p><img src="https://img.issey.top/img/202304101624733.png" /></p><p>关于如何测试，如何解决过拟合等，将在下一篇文章中说明。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLP实战】基于Bert和双向LSTM的情感分类【上篇】</title>
      <link href="/article/ceedbae09481/"/>
      <url>/article/ceedbae09481/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>最近自己找了个实验做，写了很多实验记录和方法，现在我将它们整理成文章，希望能对不熟悉NLP的伙伴们起到些许帮助。如有疑问请及时联系作者。</p><p>博主page：<a href="https://www.issey.top/">issey的博客 -愿无岁月可回首</a></p><p>本系列文章中不会说明环境和包如何安装，这些应该是最基础的东西，可以自己边查边安装。</p><p>许多函数用法等在代码里有详细解释，但还是希望各位去看它们的官方文档，我的代码还有很多可以改进的方法，需要的函数等在官方文档都有说明。</p><h1 id="简介">简介</h1><p>本系列将带领大家从数据获取、数据清洗、模型构建、训练，观察loss变化，调整超参数再次训练，并最后进行评估整一个过程。我们将获取一份公开竞赛中文数据，并一步步实验，到最后，我们的评估可以达到<strong>排行榜13</strong>位的位置。但重要的不是排名，而是我们能在其中学到很多。</p><p>本系列将分为三篇文章，分别是：</p><ul><li>上篇：数据获取，数据分割与数据清洗</li><li>中篇：模型构建，改进pytorch结构，开始第一次训练</li><li>下篇：测试与评估，绘图与过拟合，超参数调整</li></ul><p>本文为该系列第一篇文章，在本文中，我们将一同观察原始数据，进行数据清洗。样本是很重要的一个部分，学会观察样本并剔除一些符合特殊条件的样本，对模型在学习时有很大的帮助。</p><h1 id="数据获取与提取">数据获取与提取</h1><p>数据来源：<ahref="https://www.kaggle.com/datasets/liangqingyuan/chinese-text-multi-classification">WeibonCoV Data | Kaggle</a></p><p>竞赛官网：<ahref="https://www.datafountain.cn/competitions/423/datasets">疫情期间网民情绪识别竞赛 - DataFountain</a></p><p>关于kaggle如何下载数据，本文不再赘述。</p><p>为了把<code>数据分割</code>也作为我们实验的一部分，假设我们现在拿到的<code>nCoV_100k_train.labled.csv</code>就是我们爬取到的原始数据。</p><p>先来看看我们用到的数据长什么样。</p><p><img src="https://img.issey.top/img/202304071830190.png" /></p><p>思考：</p><ul><li>我们只需要text和情感倾向的列，其他列都不需要。</li><li>分割数据时，训练集：测试集：验证集 =6：2：2。这只是博主自己选择的比例，各位可以自行调整。</li></ul><p>编写代码。这部分比较简单，就不一步步运行了，但是各位应该逐行运行观察变化，写文章不能像<code>jupyter notebook</code>那样一行行运行，为了方便起见，文章涉及的代码都将以块状给出。但是运行实际上很多都是逐行调整的。</p><p><code>数据获取.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./data/archive/nCoV_100k_train.labled.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="comment"># 只要text和标签</span></span><br><span class="line">df = df[[<span class="string">&#x27;微博中文内容&#x27;</span>, <span class="string">&#x27;情感倾向&#x27;</span>]]</span><br><span class="line">df = df.rename(columns=&#123;<span class="string">&#x27;微博中文内容&#x27;</span>: <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;情感倾向&#x27;</span>: <span class="string">&#x27;label&#x27;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 分割数据集,储存.0.6/0.2/0.2</span></span><br><span class="line">train, test = train_test_split(df, test_size=<span class="number">0.2</span>)</span><br><span class="line">train, val = train_test_split(train, test_size=<span class="number">0.25</span>)</span><br><span class="line"><span class="built_in">print</span>(train)</span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"><span class="built_in">print</span>(val)</span><br><span class="line">train.to_csv(<span class="string">&#x27;./data/archive/train.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br><span class="line">val.to_csv(<span class="string">&#x27;./data/archive/val.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br><span class="line">test.to_csv(<span class="string">&#x27;./data/archive/test.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304101336907.png" /></p><p>运行结束后，这三个文件就是我们需要的文件。</p><h1 id="数据清洗">数据清洗</h1><p>我的清洗思路来源于这篇：<ahref="https://www.kaggle.com/code/mohamedabdelmohsen/emotion-analysis-and-classification-using-lstm-93">Emotionanalysis and Classification using LSTM 93% | Kaggle</a></p><p>该部分需要的库：</p><ul><li><strong>seaborn</strong>:一个适合数据分析的绘图库，需要matplotlib作为前置库</li></ul><h2 id="读取数据查看数据">读取数据，查看数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo:读取数据</span></span><br><span class="line">df_train = pd.read_csv(<span class="string">&#x27;./data/archive/train.csv&#x27;</span>)</span><br><span class="line">df_test = pd.read_csv(<span class="string">&#x27;./data/archive/test.csv&#x27;</span>)</span><br><span class="line">df_val = pd.read_csv(<span class="string">&#x27;./data/archive/val.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出前5行</span></span><br><span class="line"><span class="built_in">print</span>(df_train.head())</span><br><span class="line"><span class="built_in">print</span>(df_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df_test.head())</span><br><span class="line"><span class="built_in">print</span>(df_test.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df_val.head())</span><br><span class="line"><span class="built_in">print</span>(df_val.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://img.issey.top/img/202304071843310.png" /></p><h2 id="清洗训练集">清洗训练集</h2><h3 id="观察数据分布">观察数据分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># todo: 清洗Train</span></span><br><span class="line"><span class="comment"># 观察数据是否平衡</span></span><br><span class="line"><span class="built_in">print</span>(df_train.label.value_counts())</span><br><span class="line"><span class="built_in">print</span>(df_train.label.value_counts() / df_train.shape[<span class="number">0</span>] * <span class="number">100</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">sns.countplot(x=<span class="string">&#x27;label&#x27;</span>, data=df_train)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304071845138.png" /></p><p><img src="https://img.issey.top/img/202304071845498.png" /></p><p>可以发现，-2.0，9.0，10.0都<strong>只有一个样本</strong>，当作异常数据处理，我选择直接丢掉不要。</p><p>另外，这个样本分布略微存在分布不平衡<strong>（imbalance）</strong>的情况，至于要不要用<code>smote</code>等方法过采样，暂时先不进行讨论，我们暂时保持数据不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df_train[df_train.label &gt; <span class="number">5.0</span>])</span><br><span class="line"><span class="built_in">print</span>(df_train[(df_train.label &lt; -<span class="number">1.1</span>)])</span><br><span class="line"><span class="comment"># 丢掉异常数据</span></span><br><span class="line">df_train.drop(df_train[(df_train.label &lt; -<span class="number">1.1</span>) | (df_train.label &gt; <span class="number">5</span>)].index, inplace=<span class="literal">True</span>, axis=<span class="number">0</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_train.label.value_counts())</span><br><span class="line">sns.countplot(x=<span class="string">&#x27;label&#x27;</span>, data=df_train)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304071919138.png" /></p><p><img src="https://img.issey.top/img/202304071920000.png" /></p><h3 id="去除空数据">去除空数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 观察是否有空行</span></span><br><span class="line"><span class="built_in">print</span>(df_train.isnull().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 删除空行数据</span></span><br><span class="line">df_train.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_train.isnull().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304071928151.png" /></p><h3 id="去除重复数据">去除重复数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看重复数据</span></span><br><span class="line"><span class="built_in">print</span>(df_train.duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># print(df_train[df_train.duplicated()==True])</span></span><br><span class="line"><span class="comment"># 删除重复数据</span></span><br><span class="line">index = df_train[df_train.duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_train.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_train.duplicated().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304071948763.png" /></p><p><strong>然后我们还需要去除text一样但是label不一样的数据。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们还需要关心的重复数据是text一样但是label不一样的数据。</span></span><br><span class="line"><span class="built_in">print</span>(df_train[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="built_in">print</span>(df_train[df_train[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>])</span><br><span class="line"><span class="comment"># 查看例子</span></span><br><span class="line"><span class="built_in">print</span>(df_train[df_train[<span class="string">&#x27;text&#x27;</span>] == df_train.iloc[<span class="number">856</span>][<span class="string">&#x27;text&#x27;</span>]])</span><br><span class="line"><span class="built_in">print</span>(df_train[df_train[<span class="string">&#x27;text&#x27;</span>] == df_train.iloc[<span class="number">3096</span>][<span class="string">&#x27;text&#x27;</span>]])</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304071953989.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去掉text一样但是label不一样的数据</span></span><br><span class="line">index = df_train[df_train[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_train.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line"><span class="built_in">print</span>(df_train[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())  <span class="comment"># 0</span></span><br></pre></td></tr></table></figure><h3 id="关于去除停用词">关于去除停用词</h3><p>去不去除停用词和构建<code>word embedding</code>选择的方法有关，去查了一下，使用Bert构建时，不需要去除停用词处理，否则还会丢失上下文。于是这里没有进一步去除停用词。</p><p>问题解答：<ahref="https://stackoverflow.com/questions/63633534/is-it-necessary-to-do-stopwords-removal-stemming-lemmatization-for-text-classif">nlp- Is it necessary to do stopwords removal ,Stemming/Lemmatization fortext classification while using Spacy,Bert? - Stack Overflow</a></p><p><img src="https://img.issey.top/img/202304072024715.png" /></p><h3 id="关于特殊符号">关于特殊符号</h3><p>观察我们现在的数据：</p><p><img src="https://img.issey.top/img/202304072057132.png" /></p><p>很容易发现里面有特殊字符。</p><p>待会儿用到的bert，它会用到一个中文字典，这个字典是它自己有的，如果出现字典里没有的字符，它会自动替换成<code>[UNK]</code>,所以不用管。</p><h3 id="储存清洗后的数据集">储存清洗后的数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_train.to_csv(<span class="string">&#x27;./data/archive/train_clean.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h2 id="清洗测试集">清洗测试集</h2><p>整体步骤和清洗训练集的一样。这里为了巩固处理思路，自己还是详细做一遍吧。</p><h3 id="观察数据分布-1">观察数据分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 观察数据是否平衡</span></span><br><span class="line"><span class="built_in">print</span>(df_test.label.value_counts())</span><br><span class="line"><span class="built_in">print</span>(df_test.label.value_counts() / df_test.shape[<span class="number">0</span>] * <span class="number">100</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">sns.countplot(x=<span class="string">&#x27;label&#x27;</span>, data=df_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输出就不放了，放个图。</p><p><img src="https://img.issey.top/img/202304101431629.png" /></p><p>没有特殊label，不用进行去除的操作。</p><p>哦对，执行时可以把上面清洗train的代码注释了，用不着重新跑。</p><h3 id="去除空数据-1">去除空数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 观察是否有空行</span></span><br><span class="line"><span class="built_in">print</span>(df_test.isnull().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 删除空行数据</span></span><br><span class="line">df_test.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_test.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_test.isnull().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304072154340.png" /></p><h3 id="去除重复数据并储存">去除重复数据(并储存)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看重复数据</span></span><br><span class="line"><span class="built_in">print</span>(df_test.duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># print(df_test[df_test.duplicated()==True])</span></span><br><span class="line"><span class="comment"># 删除重复数据</span></span><br><span class="line">index = df_test[df_test.duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_test.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_test.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_test.duplicated().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304072157586.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重复数据是text一样但是label不一样的数据。</span></span><br><span class="line"><span class="built_in">print</span>(df_test[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="built_in">print</span>(df_test[df_test[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>])</span><br><span class="line"><span class="comment"># 查看例子</span></span><br><span class="line"><span class="comment"># print(df_test[df_test[&#x27;text&#x27;] == df_test.iloc[2046][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># print(df_test[df_test[&#x27;text&#x27;] == df_test.iloc[3132][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># 去掉text一样但是label不一样的数据</span></span><br><span class="line">index = df_test[df_test[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_test.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_test.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line"><span class="built_in">print</span>(df_test[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())  <span class="comment"># 0</span></span><br><span class="line"><span class="comment"># print(df_test)</span></span><br><span class="line"><span class="comment"># 检查形状与编号</span></span><br><span class="line"><span class="built_in">print</span>(df_test.tail())</span><br><span class="line"><span class="built_in">print</span>(df_test.shape)</span><br><span class="line">df_test.to_csv(<span class="string">&#x27;./data/archive/test_clean.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304072203568.png" /></p><p>有的注释可以打开自己看着调。</p><h2 id="清洗验证集">清洗验证集</h2><h3 id="观察数据分布-2">观察数据分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 观察数据是否平衡</span></span><br><span class="line"><span class="built_in">print</span>(df_val.label.value_counts())</span><br><span class="line"><span class="built_in">print</span>(df_val.label.value_counts() / df_val.shape[<span class="number">0</span>] * <span class="number">100</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">sns.countplot(x=<span class="string">&#x27;label&#x27;</span>, data=df_val)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304072227428.png" /></p><p><img src="https://img.issey.top/img/202304101432583.png" /></p><p>有三个取值我们需要剔除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 丢掉异常数据</span></span><br><span class="line">df_val.drop(df_val[(df_val.label == <span class="string">&#x27;4&#x27;</span>) |</span><br><span class="line">                   (df_val.label == <span class="string">&#x27;-&#x27;</span>) |</span><br><span class="line">                   (df_val.label == <span class="string">&#x27;·&#x27;</span>)].index, inplace=<span class="literal">True</span>, axis=<span class="number">0</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_val.label.value_counts())</span><br><span class="line">sns.countplot(x=<span class="string">&#x27;label&#x27;</span>, data=df_val)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304101430080.png" /></p><h3 id="去除空行">去除空行</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 观察是否有空行</span></span><br><span class="line"><span class="built_in">print</span>(df_val.isnull().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 删除空行数据</span></span><br><span class="line">df_val.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_val.isnull().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><h3 id="去除重复数据并储存-1">去除重复数据(并储存)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看重复数据</span></span><br><span class="line"><span class="built_in">print</span>(df_val.duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># print(df_val[df_val.duplicated()==True])</span></span><br><span class="line"><span class="comment"># 删除重复数据</span></span><br><span class="line">index = df_val[df_val.duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_val.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_val.duplicated().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304072237376.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重复数据是text一样但是label不一样的数据。</span></span><br><span class="line"><span class="built_in">print</span>(df_val[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># print(df_val[df_val[&#x27;text&#x27;].duplicated() == True])</span></span><br><span class="line"><span class="comment"># 查看例子</span></span><br><span class="line"><span class="comment"># print(df_val[df_val[&#x27;text&#x27;] == df_val.iloc[1817][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># print(df_val[df_val[&#x27;text&#x27;] == df_val.iloc[2029][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># 去掉text一样但是label不一样的数据</span></span><br><span class="line">index = df_val[df_val[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_val.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line"><span class="built_in">print</span>(df_val[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())  <span class="comment"># 0</span></span><br><span class="line"><span class="comment"># print(df_val)</span></span><br><span class="line"><span class="comment"># 检查形状与编号</span></span><br><span class="line"><span class="built_in">print</span>(df_val.tail())</span><br><span class="line"><span class="built_in">print</span>(df_val.shape)</span><br><span class="line">df_val.to_csv(<span class="string">&#x27;./data/archive/val_clean.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202304072241938.png" /></p><h1 id="总结">总结</h1><p>到此为止，我们已经清洗好了数据。让我们来看看在本次清洗时，忽略了哪些在其他实验中可以继续改进的地方：</p><ul><li>本次清洗没有去除停用词，因为使用<code>bert</code>时去除停用词可能会丢失上下文。</li><li>本次清洗没有去除特殊字符，因为<code>bert</code>会自动将未知字符转化为<code>[UKN]</code>。</li><li>本次没有对样本进行<code>过采样/欠采样</code>来解决<strong>imbalance</strong>问题，这个问题留到评估模型后再考虑要不要讨论。</li></ul><p>下一篇文章中，我们将会使用Pytorch搭建Bert和双向LSTM实现多分类。</p><h1 id="代码汇总">代码汇总</h1><p><code>数据清洗.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo:读取数据</span></span><br><span class="line">df_train = pd.read_csv(<span class="string">&#x27;./data/archive/train.csv&#x27;</span>)</span><br><span class="line">df_test = pd.read_csv(<span class="string">&#x27;./data/archive/test.csv&#x27;</span>)</span><br><span class="line">df_val = pd.read_csv(<span class="string">&#x27;./data/archive/val.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出前5行</span></span><br><span class="line"><span class="comment"># print(df_train.head())</span></span><br><span class="line"><span class="comment"># print(df_train.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(df_test.head())</span></span><br><span class="line"><span class="comment"># print(df_test.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(df_val.head())</span></span><br><span class="line"><span class="comment"># print(df_val.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 清洗Train</span></span><br><span class="line"><span class="comment"># 观察数据是否平衡</span></span><br><span class="line"><span class="comment"># print(df_train.label.value_counts())</span></span><br><span class="line"><span class="comment"># print(df_train.label.value_counts() / df_train.shape[0] * 100)</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(8, 4))</span></span><br><span class="line"><span class="comment"># sns.countplot(x=&#x27;label&#x27;, data=df_train)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># print(df_train[df_train.label &gt; 5.0])</span></span><br><span class="line"><span class="comment"># print(df_train[(df_train.label &lt; -1.1)])</span></span><br><span class="line"><span class="comment"># 丢掉异常数据</span></span><br><span class="line">df_train.drop(df_train[(df_train.label &lt; -<span class="number">1.1</span>) | (df_train.label &gt; <span class="number">5</span>)].index, inplace=<span class="literal">True</span>, axis=<span class="number">0</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_train.label.value_counts())</span></span><br><span class="line"><span class="comment"># sns.countplot(x=&#x27;label&#x27;, data=df_train)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察是否有空行</span></span><br><span class="line"><span class="comment"># print(df_train.isnull().sum())</span></span><br><span class="line"><span class="comment"># 删除空行数据</span></span><br><span class="line">df_train.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_train.isnull().sum())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看重复数据</span></span><br><span class="line"><span class="comment"># print(df_train.duplicated().sum())</span></span><br><span class="line"><span class="comment"># print(df_train[df_train.duplicated()==True])</span></span><br><span class="line"><span class="comment"># 删除重复数据</span></span><br><span class="line">index = df_train[df_train.duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_train.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_train.duplicated().sum())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还需要关心的重复数据是text一样但是label不一样的数据。</span></span><br><span class="line"><span class="comment"># print(df_train[&#x27;text&#x27;].duplicated().sum())</span></span><br><span class="line"><span class="comment"># print(df_train[df_train[&#x27;text&#x27;].duplicated() == True])</span></span><br><span class="line"><span class="comment"># 查看例子</span></span><br><span class="line"><span class="comment"># print(df_train[df_train[&#x27;text&#x27;] == df_train.iloc[856][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># print(df_train[df_train[&#x27;text&#x27;] == df_train.iloc[3096][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># 去掉text一样但是label不一样的数据</span></span><br><span class="line">index = df_train[df_train[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_train.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_train.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line"><span class="comment"># print(df_train[&#x27;text&#x27;].duplicated().sum())  # 0</span></span><br><span class="line"><span class="comment"># print(df_train)</span></span><br><span class="line"><span class="comment"># 检查形状与编号</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======train-clean======&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df_train.tail())</span><br><span class="line"><span class="built_in">print</span>(df_train.shape)</span><br><span class="line">df_train.to_csv(<span class="string">&#x27;./data/archive/train_clean.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 清洗test</span></span><br><span class="line"><span class="comment"># 观察数据是否平衡</span></span><br><span class="line"><span class="comment"># print(df_test.label.value_counts())</span></span><br><span class="line"><span class="comment"># print(df_test.label.value_counts() / df_test.shape[0] * 100)</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(8, 4))</span></span><br><span class="line"><span class="comment"># sns.countplot(x=&#x27;label&#x27;, data=df_test)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># 观察是否有空行</span></span><br><span class="line"><span class="comment"># print(df_test.isnull().sum())</span></span><br><span class="line"><span class="comment"># 删除空行数据</span></span><br><span class="line">df_test.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_test.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_test.isnull().sum())</span></span><br><span class="line"><span class="comment"># 查看重复数据</span></span><br><span class="line"><span class="comment"># print(df_test.duplicated().sum())</span></span><br><span class="line"><span class="comment"># print(df_test[df_test.duplicated()==True])</span></span><br><span class="line"><span class="comment"># 删除重复数据</span></span><br><span class="line">index = df_test[df_test.duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_test.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_test.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_test.duplicated().sum())</span></span><br><span class="line"><span class="comment"># 重复数据是text一样但是label不一样的数据。</span></span><br><span class="line"><span class="comment"># print(df_test[&#x27;text&#x27;].duplicated().sum())</span></span><br><span class="line"><span class="comment"># print(df_test[df_test[&#x27;text&#x27;].duplicated() == True])</span></span><br><span class="line"><span class="comment"># 查看例子</span></span><br><span class="line"><span class="comment"># print(df_test[df_test[&#x27;text&#x27;] == df_test.iloc[2046][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># print(df_test[df_test[&#x27;text&#x27;] == df_test.iloc[3132][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># 去掉text一样但是label不一样的数据</span></span><br><span class="line">index = df_test[df_test[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_test.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_test.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line"><span class="comment"># print(df_test[&#x27;text&#x27;].duplicated().sum())  # 0</span></span><br><span class="line"><span class="comment"># print(df_test)</span></span><br><span class="line"><span class="comment"># 检查形状与编号</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======test-clean======&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df_test.tail())</span><br><span class="line"><span class="built_in">print</span>(df_test.shape)</span><br><span class="line">df_test.to_csv(<span class="string">&#x27;./data/archive/test_clean.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 清洗验证集</span></span><br><span class="line"><span class="comment"># 观察数据是否平衡</span></span><br><span class="line"><span class="comment"># print(df_val.label.value_counts())</span></span><br><span class="line"><span class="comment"># print(df_val.label.value_counts() / df_val.shape[0] * 100)</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(8, 4))</span></span><br><span class="line"><span class="comment"># sns.countplot(x=&#x27;label&#x27;, data=df_val)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># 丢掉异常数据</span></span><br><span class="line">df_val.drop(df_val[(df_val.label == <span class="string">&#x27;4&#x27;</span>) |</span><br><span class="line">                   (df_val.label == <span class="string">&#x27;-&#x27;</span>) |</span><br><span class="line">                   (df_val.label == <span class="string">&#x27;·&#x27;</span>)].index, inplace=<span class="literal">True</span>, axis=<span class="number">0</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_val.label.value_counts())</span></span><br><span class="line"><span class="comment"># sns.countplot(x=&#x27;label&#x27;, data=df_val)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察是否有空行</span></span><br><span class="line"><span class="comment"># print(df_val.isnull().sum())</span></span><br><span class="line"><span class="comment"># 删除空行数据</span></span><br><span class="line">df_val.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_val.isnull().sum())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看重复数据</span></span><br><span class="line"><span class="comment"># print(df_val.duplicated().sum())</span></span><br><span class="line"><span class="comment"># print(df_val[df_val.duplicated()==True])</span></span><br><span class="line"><span class="comment"># 删除重复数据</span></span><br><span class="line">index = df_val[df_val.duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_val.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(df_val.duplicated().sum())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复数据是text一样但是label不一样的数据。</span></span><br><span class="line"><span class="built_in">print</span>(df_val[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># print(df_val[df_val[&#x27;text&#x27;].duplicated() == True])</span></span><br><span class="line"><span class="comment"># 查看例子</span></span><br><span class="line"><span class="comment"># print(df_val[df_val[&#x27;text&#x27;] == df_val.iloc[1817][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># print(df_val[df_val[&#x27;text&#x27;] == df_val.iloc[2029][&#x27;text&#x27;]])</span></span><br><span class="line"><span class="comment"># 去掉text一样但是label不一样的数据</span></span><br><span class="line">index = df_val[df_val[<span class="string">&#x27;text&#x27;</span>].duplicated() == <span class="literal">True</span>].index</span><br><span class="line">df_val.drop(index, axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_val.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line"><span class="built_in">print</span>(df_val[<span class="string">&#x27;text&#x27;</span>].duplicated().<span class="built_in">sum</span>())  <span class="comment"># 0</span></span><br><span class="line"><span class="comment"># print(df_val)</span></span><br><span class="line"><span class="comment"># 检查形状与编号</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======val-clean======&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df_val.tail())</span><br><span class="line"><span class="built_in">print</span>(df_val.shape)</span><br><span class="line">df_val.to_csv(<span class="string">&#x27;./data/archive/val_clean.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记15——多分类混淆矩阵、F1-score指标详解与代码实现（含数据）</title>
      <link href="/article/f7fe9040ab51/"/>
      <url>/article/f7fe9040ab51/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>参考文章</strong></p><ul><li><ahref="https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839">4.4.2分类模型评判指标（一）- 混淆矩阵(ConfusionMatrix)_进击的橘子猫的博客-CSDN博客_混淆矩阵</a></li></ul><h1 id="前言">前言</h1><p>之前在逻辑回归的文章中简单提到过F1-score，但并没有详细对其进行说明和代码实现。这里补一下。</p><h1 id="混淆矩阵简介">混淆矩阵简介</h1><p><code>混淆矩阵</code>（又称<code>误差矩阵</code>）是评判模型结果的指标，属于模型评估的一部分。混淆矩阵多用于判断分类器的优劣，适用于分类型数据模型。如分类树、逻辑回归、线性判别分析等方法。</p><blockquote><p>除了混淆矩阵外，常见的分类型模型判别标准还有<code>ROC曲线</code>和<code>AUC面积</code>，本篇不对另外两种进行拓展。</p></blockquote><h1 id="二分类混淆矩阵">二分类混淆矩阵</h1><blockquote><p>为了便于理解，我们从二分类混淆模型开始，引出四个基本指标（又称一级指标），并进一步引出其他指标。</p></blockquote><p><img src="https://img.issey.top/img/202210312306116.png" /></p><h2 id="一级指标">一级指标</h2><ul><li><code>TP(True Positive，真阳性)：样本的真实类别是正类，并且模型预测的结果也是正类。</code></li><li><code>FP(False Positive，假阳性)：样本的真实类别是负类，但是模型将其预测成为正类。</code></li><li><code>TN(True Negative，真阴性)：样本的真实类别是负类，并且模型将其预测成为负类。</code></li><li><code>FN(False Negative，假阴性)：样本的真实类别是正类，但是模型将其预测成为负类。</code></li></ul><h2 id="二级指标">二级指标</h2><p>通过混淆矩阵统计出的一级指标，可以进一步衍生出以下二级指标。</p><h3 id="准确率accuracy">准确率（Accuracy）</h3><p>含义：分类模型所有判断正确的结果占总观测值的比重。<code>（准确率是针对整个模型）</code></p><p>公式： <span class="math display">\[Acc = \frac{TP+TN}{TP+TN+FP+FN}\]</span></p><blockquote><p>注意别把准确率和精确率搞混了。</p></blockquote><h3 id="精确率precision">精确率（Precision）</h3><p>含义：在模型预测为正类的所有结果中，模型预测正确的比重。</p><p>公式： <span class="math display">\[Pc = \frac{TP}{TP+FP}\]</span></p><h3 id="召回率recall">召回率（Recall）</h3><blockquote><p>召回率又称<code>灵敏度（Sensitivity）</code>。</p></blockquote><p>含义：在真实值为正类的所有结果中，模型预测正确的比重。</p><p>公式： <span class="math display">\[Rc = \frac{TP}{TP+FN}\]</span></p><h3 id="特异度specificity">特异度（Specificity）</h3><p>含义：在真实值为负类的所有结果中，模型预测正确的比重。</p><p>公式： <span class="math display">\[Sc = \frac{TN}{TN+FP}\]</span></p><blockquote><p>相对于前三个二级指标，特异度用的比较少。</p></blockquote><h2 id="三级指标f-score">三级指标（F-score）</h2><p>通过二级指标可以引出三级指标<code>F Score</code>。</p><p>F-Score是可以综合考虑<code>精确度（Precision）</code>和<code>召回率（Recall）</code>的调和值，公式如下：<span class="math display">\[F~Score = (1+\beta^2)\frac{Precision\timesRecall}{\beta^2Precision+Recall}\]</span></p><ul><li>当我们认为精确度更重要，调整<spanclass="math inline">\(\beta&lt;1\)</span>。</li><li>当我们认为召回率更重要，调整<spanclass="math inline">\(\beta&gt;1\)</span>。</li><li>当<span class="math inline">\(\beta =1\)</span>时，精确度和召回率权重相同。<code>此时称为F1-Score或F1-Measure</code>。</li></ul><h3 id="f1-score">F1-score</h3><p>公式（即<span class="math inline">\(\beta=1\)</span>）: <spanclass="math display">\[F1~Score = \frac{2Precision\times Recall}{Precision+Recall}\]</span></p><h1 id="多分类混淆矩阵">多分类混淆矩阵</h1><blockquote><p>现在将二分类拓展至多分类混淆矩阵。为了便于理解，这里举一个具体的示例。</p></blockquote><p><img src="https://img.issey.top/img/202210312326990.png" /></p><h2 id="准确率accuracy-1">准确率（Accuracy）</h2><p>准确率是对于整体而言的，指分类模型所有预测正确的结果占总观测值的比重。可以很容易看出，对于多分类混淆矩阵，所有预测正确的结果就是<code>对角线之和</code>。</p><p>于是这个示例的准确率为： <span class="math display">\[\begin{split}Acc &amp; = \frac{23+25+30}{23+3+2+4+25+6+5+2+30} \\&amp; = \frac{78}{100} \\&amp; = 0.78\end{split}\]</span></p><h2 id="精确率precision-1">精确率（Precision）</h2><p>精确率是对于单个类别而言。所以这里需要把<spanclass="math inline">\(C1,C2,C3\)</span>的精确度都算出来。这里就只举例<spanclass="math inline">\(C1\)</span>的精确度如何计算。</p><p>某类的精确率：在模型预测为本类的所有结果中，模型预测正确的比重。<code>(分母为对行求和)</code></p><p>令<span class="math inline">\(C1\)</span>的精确率为<spanclass="math inline">\(Pc_1\)</span> ： <span class="math display">\[\begin{split}Pc_1 = &amp; \frac{23}{23+3+2} \\&amp; = 0.821\end{split}\]</span></p><h2 id="召回率recall-1">召回率（Recall）</h2><p>某类的召回率：在真实值为本类的所有结果中，模型预测正确的比重。<code>（分母为对列求和）</code></p><p>令<span class="math inline">\(C1\)</span>的召回率为<spanclass="math inline">\(Rc_1\)</span> ： <span class="math display">\[\begin{split}Rc_1 = &amp; \frac{23}{23+4+5} \\&amp; = 0.719\end{split}\]</span></p><h2 id="特异度specificity-1">特异度（Specificity）</h2><blockquote><p>一般都用不着，不过这里还是算一遍吧。</p></blockquote><p>要计算这个东西，需要把上面的图统计为二分类混淆矩阵的形式才好解释。</p><p><img src="https://img.issey.top/img/202211011424133.png" /></p><p>特异度：在真实值为负类的所有结果中，模型预测正确的比重。</p>令<span class="math inline">\(C1\)</span>的特异度为<spanclass="math inline">\(Sc_1\)</span> ： $$<span class="math display">\[\begin{split}Sc_1 &amp; = \frac{TN}{TN+FP} \\&amp; = \frac{63}{5+63} \\&amp; = 0.93\end{split}\]</span><p>$$</p><h2 id="f1-score-1">F1-score</h2><p><span class="math display">\[\begin{split}F1~Score_1 &amp;= \frac{2Pc_1Rc_1}{Pc_1+Rc_1} \\&amp; = \frac{2\times0.821\times0.719 }{0.821+0.719} \\&amp; = 0.767\end{split}\]</span></p><h1 id="示例与代码实现python">示例与代码实现（Python）</h1><blockquote><p>前景题要：现在已经通过某种多分类器（例如softmax分类器）求出了测试集的预测结果。现在需要对预测结果进行评估。</p></blockquote><ul><li>y_predict：样本集的预测结果，维度(1,N)。N代表参加测试的有N个样本。</li><li>y_true：样本集的真实标签，维度(1,N)。</li></ul><p>数据如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_true is :    [0 1 3 2 3 0 2 2 3 3 3 0 1 4 4 0 1 3 2 2 1 3 2 0 2 4 1 0 1 0 4 3 3 3 2 1 0 3 0]</span><br><span class="line">y_predict is : [0 1 3 0 2 0 2 2 1 2 3 0 0 4 4 0 1 4 2 2 0 3 2 1 2 4 3 1 1 3 4 3 0 2 2 3 2 2 1]</span><br></pre></td></tr></table></figure><h2 id="step1统计混淆矩阵">step1：统计混淆矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">statistics_confusion</span>(<span class="params">y_true,y_predict</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">     统计混淆矩阵</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">    :param y_true: 真实label </span></span><br><span class="line"><span class="string">    :param y_predict: 预测label</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    confusion = np.zeros((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y_true.shape[<span class="number">0</span>]):</span><br><span class="line">        confusion[y_predict[i]][y_true[i]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> confusion</span><br></pre></td></tr></table></figure><p>输出:</p><p><img src="https://img.issey.top/img/202211011130582.png" /></p><h2 id="step2计算二级指标">step2：计算二级指标</h2><h3 id="准确率accuracy-2">准确率（Accuracy）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_Acc</span>(<span class="params">confusion</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算准确率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param confusion: 混淆矩阵</span></span><br><span class="line"><span class="string">    :return: Acc</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(confusion.diagonal())/np.<span class="built_in">sum</span>(confusion)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Acc is 0.5641025641025641</span><br></pre></td></tr></table></figure><h3 id="精确率precision-2">精确率（Precision）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_Pc</span>(<span class="params">confusion</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算每类精确率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param confusion: 混淆矩阵</span></span><br><span class="line"><span class="string">    :return: Pc</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> confusion.diagonal()/np.<span class="built_in">sum</span>(confusion,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pc is [0.5        0.42857143 0.58333333 0.57142857 0.8       ]</span><br></pre></td></tr></table></figure><p>每列对应一个类的精确率。</p><h3 id="召回率recall-2">召回率（Recall）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_Rc</span>(<span class="params">confusion</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算每类召回率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param confusion: 混淆矩阵</span></span><br><span class="line"><span class="string">    :return: Rc</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> confusion.diagonal()/np.<span class="built_in">sum</span>(confusion,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Rc is [0.44444444 0.42857143 0.875      0.36363636 1.        ]</span><br></pre></td></tr></table></figure><p>每列对应一个类的召回率。</p><h2 id="step3计算f1-score">step3：计算F1-score</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_F1score</span>(<span class="params">PC,RC</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算F1 score</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param PC: 精准率</span></span><br><span class="line"><span class="string">    :param RC: 召回率</span></span><br><span class="line"><span class="string">    :return: F1 score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*np.multiply(PC,RC)/(PC+RC)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F1-score is [0.47058824 0.42857143 0.7        0.44444444 0.88888889]</span><br></pre></td></tr></table></figure><p>每列对应一个类的F1 score。</p><p>为了让结果和代码更加美观，可以打一下包。</p><h2 id="完整代码">完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Myreport</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__confusion = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__statistics_confusion</span>(<span class="params">self,y_true,y_predict</span>):</span><br><span class="line">        self.__confusion = np.zeros((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y_true.shape[<span class="number">0</span>]):</span><br><span class="line">            self.__confusion[y_predict[i]][y_true[i]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cal_Acc</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(self.__confusion.diagonal()) / np.<span class="built_in">sum</span>(self.__confusion)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cal_Pc</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__confusion.diagonal() / np.<span class="built_in">sum</span>(self.__confusion, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cal_Rc</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__confusion.diagonal() / np.<span class="built_in">sum</span>(self.__confusion, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cal_F1score</span>(<span class="params">self,PC,RC</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * np.multiply(PC, RC) / (PC + RC)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">report</span>(<span class="params">self,y_true,y_predict,classNames</span>):</span><br><span class="line">        self.__statistics_confusion(y_true,y_predict)</span><br><span class="line">        Acc = self.__cal_Acc()</span><br><span class="line">        Pc = self.__cal_Pc()</span><br><span class="line">        Rc = self.__cal_Rc()</span><br><span class="line">        F1score = self.__cal_F1score(Pc,Rc)</span><br><span class="line">        <span class="built_in">str</span> = <span class="string">&quot;Class Name\t\tprecision\t\trecall\t\tf1-score\n&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classNames)):</span><br><span class="line">           <span class="built_in">str</span> += <span class="string">f&quot;<span class="subst">&#123;classNames[i]&#125;</span>   \t\t\t<span class="subst">&#123;<span class="built_in">format</span>(Pc[i],<span class="string">&#x27;.2f&#x27;</span>)&#125;</span>   \t\t\t<span class="subst">&#123;<span class="built_in">format</span>(Rc[i],<span class="string">&#x27;.2f&#x27;</span>)&#125;</span>&quot;</span> \</span><br><span class="line">                  <span class="string">f&quot;   \t\t\t<span class="subst">&#123;<span class="built_in">format</span>(F1score[i],<span class="string">&#x27;.2f&#x27;</span>)&#125;</span>\n&quot;</span></span><br><span class="line">        <span class="built_in">str</span> += <span class="string">f&quot;accuracy is <span class="subst">&#123;<span class="built_in">format</span>(Acc,<span class="string">&#x27;.2f&#x27;</span>)&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">myreport = Myreport()</span><br><span class="line"><span class="built_in">print</span>(myreport.report(y_true = y_true,y_predict=y_predict,classNames=[<span class="string">&#x27;C1&#x27;</span>,<span class="string">&#x27;C2&#x27;</span>,<span class="string">&#x27;C3&#x27;</span>,<span class="string">&#x27;C4&#x27;</span>,<span class="string">&#x27;C5&#x27;</span>]))</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://img.issey.top/img/202211011326796.png" /></p><h1id="使用sklearn对比计算结果是否正确">使用sklearn对比计算结果是否正确</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 结果评估</span></span><br><span class="line">myreport = Myreport()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=====自己实现的结果=====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(myreport.report(y_true = y_true,y_test=y_predict,classNames=[<span class="string">&#x27;C1&#x27;</span>,<span class="string">&#x27;C2&#x27;</span>,<span class="string">&#x27;C3&#x27;</span>,<span class="string">&#x27;C4&#x27;</span>,<span class="string">&#x27;C5&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=====使用sklrean的结果=====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_true, y_predict, target_names=[<span class="string">&#x27;C1&#x27;</span>,<span class="string">&#x27;C2&#x27;</span>,<span class="string">&#x27;C3&#x27;</span>,<span class="string">&#x27;C4&#x27;</span>,<span class="string">&#x27;C5&#x27;</span>]))</span><br></pre></td></tr></table></figure><h2 id="结果对比">结果对比</h2><p><img src="https://img.issey.top/img/202211011327061.png" /></p><p>只能说一模一样。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型评估 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记14——【Softmax多分类 2】代码具体实现与应用（含数据集）</title>
      <link href="/article/602d6fb8bb9d/"/>
      <url>/article/602d6fb8bb9d/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>前置文章</strong></p><ul><li><ahref="https://www.issey.top/article/324b71593ade/">机器学习笔记13——【Softmax多分类1】完整流程与详细公式推导 | issey的博客</a></li></ul><h1 id="前言">前言</h1><p>在阅读本文之前，请确保已经对Softmax多分类器的原理、相关公式推导、具体流程有一定了解。本文将不再具体介绍这些内容，若对这些内容还不了解，请先阅读本文的前置文章。</p><p>本文为softmax多分类器下篇，共分为两个部分：</p><p><strong>第一部分：</strong>对softmax分类器具体代码实现，并将<code>鸢尾花数据集</code>作为示例，进行模型训练和预测。</p><p><strong>第二部分：</strong>使用sklrean实现softmax分类器，并与自己实现的softmax分类器作结果对比。<code>（但好像sklrean只有逻辑回归的包，但是达到了多分类的效果，所以一般说sklrean里的softmax就是逻辑回归）</code></p><h1 id="关于代码">关于代码</h1><p>关于本文代码实现的一些说明：</p><p>本篇文章涉及的部分<strong>已经在本系列以前的文章中具体介绍并实现过模块</strong>例如：训练集测试集拆分、Z-score标准化、测试结果评估等将不再手动实现。</p><p>另外，因为没有找到合适的参考文章，所以这是一次在没有任何参考，仅通过个人理解和公式推导的前提下实现的代码。如果有不足的地方还请指出。</p><h1 id="第一部分">第一部分</h1><blockquote><p>在该部分，将会对softmax分类器进行具体实现，并完成鸢尾花多分类示例。</p></blockquote><h2 id="softmax分类器相关公式与步骤">Softmax分类器相关公式与步骤</h2><p>回顾上篇文章我们总结的softmax相关公式与步骤。</p><h3 id="相关公式">相关公式</h3><p>1.<strong>样本特征的加权组合</strong>，我们选用<code>线性加权组合</code>：<span class="math display">\[z_k = w_k^Tx+b_k = (\sum_{i=1}^Mw_{k,i}x_i)+b_k\]</span> 2.<strong>softmax激活函数：</strong> <spanclass="math display">\[softmax(z_k) = a_k = \frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}},~~~~k =0,1,...,K-1\]</span> 3.<strong>交叉熵损失函数：</strong> <spanclass="math display">\[L(\hat y,y) = -\sum_{k=1}^Ky_kloga_k\]</span> 4.<strong>损失函数求偏导：</strong> <spanclass="math display">\[\begin{split}&amp; \frac{\partial L(\hat y,y)}{\partial w_k} = (a_k-y_k)x \\&amp; \frac{\partial L(\hat y,y)}{\partial b_k} =a_k-y_k\end{split}\]</span></p><blockquote><p>其实本质是对<spanclass="math inline">\(z_k\)</span>求偏导，这里还是也贴出来吧，不过代码实现时没有用到<span class="math display">\[\frac{\partial L(\hat y,y)}{\partial z_k} = a_k-y_k\]</span></p></blockquote><h3 id="梯度下降步骤">梯度下降步骤</h3><p>给定训练集X，训练集X共分为K类:</p><ol type="1"><li><p>随机初始化模型未知参数<span class="math inline">\(\theta\)</span>，本篇文章中为随机初始化权重向量<spanclass="math inline">\(w_k,b_k,k=1,2,...,K-1\)</span>。其中，<spanclass="math inline">\(w_k\)</span>为向量，<spanclass="math inline">\(b_k\)</span> 为标量。</p></li><li><p>梯度下降算法迭代更新模型参数直至收敛，每一轮具体流程如下：</p><ol type="1"><li>先通过正向传播求得本轮训练样本预测值。</li><li>反向传播更新模型参数：</li></ol><p><span class="math display">\[\begin{split}&amp; w_k = w_k - a\frac{\partial J}{\partial w_k} =w_k-a\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k)x,\quad k = 0,1,...,K-1 \\&amp; b_k = b_k -a\frac{\partial J}{\partial b_k} =b_k-a\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k),\quad k = 0,1,...,K-1\end{split}\]</span></p><p>​注意：上述公式省去了上标<code>i</code>，上标<code>i</code>表示这是第<code>i</code>个样本。</p></li></ol><h2 id="数据集获取">数据集获取</h2><p>鸢尾花数据集的两种获取方式:</p><ol type="1"><li>sklrean自带鸢尾花数据集，不需要额外下载。</li><li>鸢尾花数据集下载地址：<ahref="https://www.kaggle.com/datasets/uciml/iris">Iris Species |Kaggle</a></li></ol><p>我专门查看了两个数据集，没有区别。</p><p><img src="https://img.issey.top/img/202210261308840.png" /></p><h2 id="从零开始实现softmax多分类器">从零开始实现softmax多分类器</h2><blockquote><p>为了使代码更便于调用，层次更加清晰，本次将会以面向对象的形式实现模型。（之前好几次都是直接写的函数）如果觉得分开写的太乱，可以看最后给出的组合起来的完整代码。</p></blockquote><h3 id="导入数据">导入数据</h3><p>为了便于编写代码过程中对各模块作测试，这里先导入鸢尾花数据集，我采用第一种导入数据集的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    iris = datasets.load_iris() <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    dataSet = iris.data <span class="comment"># 特征集</span></span><br><span class="line">    target = iris.target <span class="comment"># label集</span></span><br></pre></td></tr></table></figure><h3 id="初始框架">初始框架</h3><p>先把softmax类的框架搭出来，之后的函数一个个加进来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxModel</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,random_state = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型</span></span><br><span class="line"><span class="string">        :param random_state: 指定随机种子，默认为None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__random_state = random_state</span><br><span class="line">        self.__theta = <span class="literal">None</span> <span class="comment"># 模型参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="step1将label向量化">step1:将label向量化</h3><p>根据上一篇的推导，我们在进行softmax分类时，需要先将<spanclass="math inline">\(y = c_k\)</span>的形式换成<spanclass="math inline">\((0,..,1,...,0)\)</span>的形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__labelTransform</span>(<span class="params">self,Y,classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将label向量化</span></span><br><span class="line"><span class="string">    :param Y: label集合</span></span><br><span class="line"><span class="string">    :param classes: 共有多少类</span></span><br><span class="line"><span class="string">    :return: 向量化后的label集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    vec_Y = []</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> Y:</span><br><span class="line">        vec_label = np.zeros(classes)</span><br><span class="line">        vec_label[y] = <span class="number">1</span></span><br><span class="line">        vec_Y.append(vec_label)</span><br><span class="line">    <span class="keyword">return</span> vec_Y</span><br></pre></td></tr></table></figure><p>测试：</p><p><img src="https://img.issey.top/img/202210261838957.png" /></p><p>只截取了部分。可以看到label成功变成了我们需要的。</p><h3id="step2根据训练集初始化模型参数">step2:根据训练集初始化模型参数</h3><blockquote><p>这部分需要写的比较杂，不好单独抽成函数，所以大部分细节就在代码说。另外，如有疑问的地方，请配合上一篇文章一起阅读。</p></blockquote><ul><li>初始化W：维度为（K，M）。一共有K个<spanclass="math inline">\(w_k\)</span>(K为类别数)，每一个<spanclass="math inline">\(w_k\)</span>都是向量，维度与单个样本<spanclass="math inline">\(x\)</span>相同。</li><li>初始化时B一共有K个，每一个<spanclass="math inline">\(b_k\)</span>都是标量。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxModel</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,random_state = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型</span></span><br><span class="line"><span class="string">        :param random_state: 指定随机种子，默认为None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__random_state = random_state</span><br><span class="line">        self.__theta = <span class="literal">None</span> <span class="comment"># 模型参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__labelTransform</span>(<span class="params">self,Y,classes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将label向量化</span></span><br><span class="line"><span class="string">        :param Y: label集合</span></span><br><span class="line"><span class="string">        :param classes: 共有多少类</span></span><br><span class="line"><span class="string">        :return: 向量化后的label集</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        vec_Y = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> Y:</span><br><span class="line">            vec_label = <span class="built_in">list</span>(np.zeros(classes))</span><br><span class="line">            vec_label[y] = <span class="number">1</span></span><br><span class="line">            vec_Y.append(vec_label)</span><br><span class="line">        vec_Y = np.array(vec_Y)</span><br><span class="line">        <span class="keyword">return</span> vec_Y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init_theta</span>(<span class="params">self,classes,feature_nums,random_state</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型参数</span></span><br><span class="line"><span class="string">        :param classes: 一共有多少类，即K</span></span><br><span class="line"><span class="string">        :param feature_nums: 样本特征数目</span></span><br><span class="line"><span class="string">        :param random_state: 随机种子</span></span><br><span class="line"><span class="string">        :return: 模型参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        theta = &#123;&#125;</span><br><span class="line">        np.random.seed(random_state)</span><br><span class="line">        theta[<span class="string">&#x27;W&#x27;</span>] = np.random.randn(classes,feature_nums) <span class="comment"># 关于randn的用法请自行查找</span></span><br><span class="line">        theta[<span class="string">&#x27;B&#x27;</span>] = np.random.randn(classes)</span><br><span class="line">        <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self,X,Y,classes = <span class="literal">None</span>,learning_rate = <span class="number">1e-3</span>,num_iters = <span class="number">100</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练softmax模型，此时采用的批量梯度下降算法。</span></span><br><span class="line"><span class="string">        :param X: 样本集</span></span><br><span class="line"><span class="string">        :param Y: 标签集，请确保类别标号从0开始</span></span><br><span class="line"><span class="string">        :param classes: 共有多少个类别，如果为None，则会根据Y自动调整</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率，默认0.001</span></span><br><span class="line"><span class="string">        :param num_iters: 最大迭代次数，默认100次</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置模型类别数</span></span><br><span class="line">        <span class="keyword">if</span> classes == <span class="literal">None</span>:</span><br><span class="line">            self.__classes = np.<span class="built_in">max</span>(Y) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__classes = classes</span><br><span class="line">        <span class="comment"># 将label向量化</span></span><br><span class="line">        Y = self.__labelTransform(Y,self.__classes)</span><br><span class="line">        <span class="comment"># 根据样本调整W的维度，并进行初始化</span></span><br><span class="line">        sample_nums,feature_nums = X.shape <span class="comment"># 行为样本个数，列为特征数</span></span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="keyword">if</span> self.__theta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.__theta = self.__init_theta(self.__classes,feature_nums,self.__random_state)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.__classes):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;w_<span class="subst">&#123;k&#125;</span> is <span class="subst">&#123;self.__theta[<span class="string">&#x27;W&#x27;</span>][k]&#125;</span>,b_<span class="subst">&#123;k&#125;</span> is <span class="subst">&#123;self.__theta[<span class="string">&#x27;B&#x27;</span>][k]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    iris = datasets.load_iris() <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    dataSet = iris.data <span class="comment"># 特征集</span></span><br><span class="line">    target = iris.target <span class="comment"># label集</span></span><br><span class="line">    <span class="comment"># print(dataSet)</span></span><br><span class="line">    model = SoftmaxModel(random_state=<span class="number">10</span>)</span><br><span class="line">    model.train(dataSet,target,learning_rate = <span class="number">1e-3</span>,num_iters = <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://img.issey.top/img/202210261916050.png" /></p><h3 id="step3对特征进行加权组合">step3:对特征进行加权组合</h3><p>公式： <span class="math display">\[z_k = w_k^Tx+b_k = (\sum_{i=1}^Mw_{k,i}x_i)+b_k\]</span> 函数变量说明：</p><ul><li>X：样本集，维度为(N，M)，代表N个样本，每个样本M个特征。</li><li>W：权值，维度为（K，M）。</li><li>B：维度为（1，K）。</li><li>Z：维度为（N，K），第<code>i</code>行代表由第<code>i</code>个样本线性组合输出的<spanclass="math inline">\(z\)</span>。</li></ul><p>矩阵乘法： <span class="math display">\[A(N,M)\cdot B(M,K) = C(N,K)\]</span> 所以<span class="math inline">\(X\cdotW^T\)</span>即可得没有加B的Z。</p><p>矩阵加法，两个尺寸一样的才可以加。所以需要把B按行复制N次，得到B（N，K），然后与Z相加即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__linear_combination</span>(<span class="params">self,X,theta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对样本特征进行线性加权组合</span></span><br><span class="line"><span class="string">    :param X: 特征集</span></span><br><span class="line"><span class="string">    :param theta: 模型参数</span></span><br><span class="line"><span class="string">    :return: Z,加权后的输出</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Z = np.dot(X,theta[<span class="string">&#x27;W&#x27;</span>].T)+np.tile(theta[<span class="string">&#x27;B&#x27;</span>],(X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><h3 id="step4softmax激活函数">step4:softmax激活函数</h3><p>下一步应该是对Z进行softmax激活，得到每一个后验概率<spanclass="math inline">\(P(y=c_k|x,\theta)\)</span>。（后验概率就是所谓的预测概率）</p><p>公式： <span class="math display">\[softmax(z_k) = a_k = \frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}},~~~~k =0,1,...,K-1\]</span> 函数变量说明：</p><ul><li>A：维度（N，K），第<code>i</code>行代表第<code>i</code>个样本对应的后验概率分布<spanclass="math inline">\(a^i\)</span>,第k列对应该样本对第k类的后验概率<spanclass="math inline">\(a_k\)</span>。</li><li>Z：维度（N，K），上一步已经说过Z的含义。</li></ul><p>要想从Z求到A,根据公式，要先对整个矩阵Z求<spanclass="math inline">\(expZ =exp(Z)\)</span>,然后分母为<code>expZ</code>按行求和，分子为<code>expZ[i,k]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__cal_softmax</span>(<span class="params">self,Z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过softmax激活，计算后验概率</span></span><br><span class="line"><span class="string">    :param Z: 隐藏层输出</span></span><br><span class="line"><span class="string">    :return: A,后验概率矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    expZ = np.exp(Z)</span><br><span class="line">    A = np.zeros_like(expZ)</span><br><span class="line">    denominator = np.<span class="built_in">sum</span>(expZ,axis=<span class="number">1</span>) <span class="comment"># 计算分母</span></span><br><span class="line">    N = expZ.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        A[i] = expZ[i]/denominator[i]</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><p>我们可以检测一下后验概率计算的对不对，因为一个样本对应的后验分布之和为1，那么有多少样本，所有后验概率之和就应该是多少：</p><p><img src="https://img.issey.top/img/202210262031175.png" /></p><blockquote><p>聪明的小伙伴已经想到了，到这一步就完成了正向传播。现在只需要知道模型参数，我们就可以对某个未知分类的样本作预测。</p></blockquote><h3 id="step5计算交叉熵损失函数">step5:计算交叉熵损失函数</h3><p>其实这一步在训练时用不到，不过既然有公式还是实现了吧，然后来对刚才我们计算的后验概率求一个代价函数。</p><p><code>注：代价函数为损失函数求平均。</code></p><p>公式： <span class="math display">\[\begin{split}&amp; L(\hat y,y) = -\sum_{k=1}^Ky_kloga_k \\&amp; J = \frac{1}{N}\sum_{i=0}^{N-1}L(\hat y,y) =-\frac{1}{N}\sum_{i=0}^{N-1}\sum_{k=1}^Ky_kloga_k\end{split}\]</span> 函数变量说明：</p><p>A和Y都是维度为（N，K）的矩阵，也就是说只需要<strong>一一对应</strong>求出<spanclass="math inline">\(-y^i_kloga^i_k\)</span>,然后对整个矩阵求和，然后取相反数，最后除N就行了。</p><p><code>一般log都是以2为底。</code></p><p>现在先测试一下我们需要的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">B = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">C = -np.log2(np.array([[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>],[<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>]]))</span><br><span class="line"><span class="built_in">print</span>(np.multiply(A,B))</span><br><span class="line"><span class="built_in">print</span>(C)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210262052395.png" /></p><p>ok,没问题。现在实现该部分的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_Costfunction</span>(<span class="params">self,A,Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算代价函数</span></span><br><span class="line"><span class="string">    :param A: 后验概率矩阵（预测概率矩阵）</span></span><br><span class="line"><span class="string">    :param Y: 真实label矩阵</span></span><br><span class="line"><span class="string">    :return: 代价</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    J = -np.<span class="built_in">sum</span>(np.multiply(Y,np.log2(A)))</span><br><span class="line">    J /= Y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210262101376.png" /></p><h3 id="step6单轮模型参数迭代">step6：单轮模型参数迭代</h3><p>每一轮参数迭代公式： <span class="math display">\[\begin{split}&amp; w_k = w_k - a\frac{\partial J}{\partial w_k} =w_k-a\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k)x,\quad k = 0,1,...,K-1 \\&amp; b_k = b_k -a\frac{\partial J}{\partial b_k} =b_k-a\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k),\quad k = 0,1,...,K-1\end{split}\]</span> 函数变量说明：</p><ul><li><spanclass="math inline">\(\alpha\)</span>：超参数，这个就不多说了，我们需要手动调整。</li><li>A：维度（N，K），含义与之前一样，后验概率矩阵。</li><li>Y：维度（N，K），含义与之前一样，真实label矩阵。</li><li>X：维度（N，M）。N个样本，一个样本M个特征。</li><li>W：维度（K，M），权值矩阵。K对应<spanclass="math inline">\(z_k\)</span>，M对应<spanclass="math inline">\(x\)</span>的特征。</li><li>B：维度（1，K），标量。K对应<spanclass="math inline">\(z_k\)</span>。</li></ul><blockquote><p>​ 这一步想矩阵化运算有点绕，所以我得分三步说明：</p></blockquote><p><strong>1.只看单个样本<spanclass="math inline">\(x\)</span>:</strong> <span class="math display">\[w_k = w_k-\alpha(a_k-y_k)x\]</span></p><ul><li><span class="math inline">\(w_k\)</span>：维度（1，M）</li><li><span class="math inline">\(a_k-y_k\)</span>：标量</li><li><span class="math inline">\(x\)</span>:维度（1,M）</li></ul><p>所以只看一个样本时，为<span class="math inline">\(a_k-y_k\)</span>（标量）去乘x里每一个特征，然后再乘学习率。</p><p><strong>2.拓展到N个样本X的平均<spanclass="math inline">\(w_k\)</span>：</strong> <spanclass="math display">\[w_k = w_k-\alpha\frac{1}{N}(A_k-Y_k)^TX\]</span> 现在<spanclass="math inline">\(A_k-Y_k\)</span>维度应该是<spanclass="math inline">\((N,1)\)</span>。也就是<code>每个类别对应的那一列的预测值与真实值相减</code>。然后将其倒置变成<spanclass="math inline">\((1,N)\)</span>去乘<spanclass="math inline">\(X(N,M)\)</span>,就可以得到<spanclass="math inline">\((1,M)\)</span>维度,不过这时求得的是所有样本对应的<spanclass="math inline">\(w_k\)</span>之和，所以还要对<spanclass="math inline">\((1,M)\)</span>除个N，才是<spanclass="math inline">\(w_k\)</span>的均值。</p><p><strong>3.拓展到W</strong> <span class="math display">\[W = W-a\frac{1}{N}(A-Y)^TX\]</span> 在草稿本上自己算一下，应该可以想明白为什么求和符号消失了。</p><p>同理， <span class="math display">\[B = B-a\frac{1}{N}\sum_{axis=0}(A-Y)\]</span> <span class="math inline">\(axis=0\)</span> 代表对列求和。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__gradient_iteration</span>(<span class="params">self,W,B,X,Y,A,learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一轮梯度迭代</span></span><br><span class="line"><span class="string">    :param W: W</span></span><br><span class="line"><span class="string">    :param B: B</span></span><br><span class="line"><span class="string">    :param X: X</span></span><br><span class="line"><span class="string">    :param Y: Y</span></span><br><span class="line"><span class="string">    :param A: A</span></span><br><span class="line"><span class="string">    :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">    :return: 更新后的W,B</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    A_Y = A-Y</span><br><span class="line">    W = W-(learning_rate/X.shape[<span class="number">0</span>])*np.dot(A_Y.T,X)</span><br><span class="line">    B = B-(learning_rate/X.shape[<span class="number">0</span>])*np.<span class="built_in">sum</span>(A_Y,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> W,B</span><br></pre></td></tr></table></figure><h3id="step7梯度下降整合模型训练模块代码">step7：梯度下降，整合模型训练模块代码</h3><blockquote><p>线性代数给我推麻了，哎。不过最后好歹还是推出来了。</p></blockquote><p>截止目前，训练过程的代码已经可以进行整合了，我们来测试一下效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxModel</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,random_state = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型</span></span><br><span class="line"><span class="string">        :param random_state: 指定随机种子，默认为None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__random_state = random_state</span><br><span class="line">        self.__theta = <span class="literal">None</span> <span class="comment"># 模型参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__labelTransform</span>(<span class="params">self,Y,classes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将label向量化</span></span><br><span class="line"><span class="string">        :param Y: label集合</span></span><br><span class="line"><span class="string">        :param classes: 共有多少类</span></span><br><span class="line"><span class="string">        :return: 向量化后的label集</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        vec_Y = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> Y:</span><br><span class="line">            vec_label = <span class="built_in">list</span>(np.zeros(classes))</span><br><span class="line">            vec_label[y] = <span class="number">1</span></span><br><span class="line">            vec_Y.append(vec_label)</span><br><span class="line">        vec_Y = np.array(vec_Y)</span><br><span class="line">        <span class="keyword">return</span> vec_Y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init_theta</span>(<span class="params">self,classes,feature_nums,random_state</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型参数</span></span><br><span class="line"><span class="string">        :param classes: 一共有多少类，即K</span></span><br><span class="line"><span class="string">        :param feature_nums: 样本特征数目</span></span><br><span class="line"><span class="string">        :param random_state: 随机种子</span></span><br><span class="line"><span class="string">        :return: 模型参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        theta = &#123;&#125;</span><br><span class="line">        np.random.seed(random_state)</span><br><span class="line">        theta[<span class="string">&#x27;W&#x27;</span>] = np.random.randn(classes,feature_nums) <span class="comment"># 关于randn的用法请自行查找</span></span><br><span class="line">        theta[<span class="string">&#x27;B&#x27;</span>] = np.random.randn(classes)</span><br><span class="line">        <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__linear_combination</span>(<span class="params">self,X,theta</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        对样本特征进行线性加权组合</span></span><br><span class="line"><span class="string">        :param X: 特征集</span></span><br><span class="line"><span class="string">        :param theta: 模型参数</span></span><br><span class="line"><span class="string">        :return: Z,加权后的输出</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Z = np.dot(X,theta[<span class="string">&#x27;W&#x27;</span>].T)+np.tile(theta[<span class="string">&#x27;B&#x27;</span>],(X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cal_softmax</span>(<span class="params">self,Z</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        通过softmax激活，计算后验概率</span></span><br><span class="line"><span class="string">        :param Z: 隐藏层输出</span></span><br><span class="line"><span class="string">        :return: A,后验概率矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        expZ = np.exp(Z)</span><br><span class="line">        A = np.zeros_like(expZ)</span><br><span class="line">        denominator = np.<span class="built_in">sum</span>(expZ,axis=<span class="number">1</span>) <span class="comment"># 计算分母</span></span><br><span class="line">        N = expZ.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            A[i] = expZ[i]/denominator[i]</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_Costfunction</span>(<span class="params">self,A,Y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算代价函数</span></span><br><span class="line"><span class="string">        :param A: 后验概率矩阵（预测概率矩阵）</span></span><br><span class="line"><span class="string">        :param Y: 真实label矩阵</span></span><br><span class="line"><span class="string">        :return: 代价</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        J = -np.<span class="built_in">sum</span>(np.multiply(Y,np.log2(A)))</span><br><span class="line">        J /= Y.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__gradient_iteration</span>(<span class="params">self,W,B,X,Y,A,learning_rate</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        一轮梯度迭代</span></span><br><span class="line"><span class="string">        :param W: W</span></span><br><span class="line"><span class="string">        :param B: B</span></span><br><span class="line"><span class="string">        :param X: X</span></span><br><span class="line"><span class="string">        :param Y: Y</span></span><br><span class="line"><span class="string">        :param A: A</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">        :return: 更新后的W,B</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        A_Y = A-Y</span><br><span class="line">        W = W-(learning_rate/X.shape[<span class="number">0</span>])*np.dot(A_Y.T,X)</span><br><span class="line">        B = B-(learning_rate/X.shape[<span class="number">0</span>])*np.<span class="built_in">sum</span>(A_Y,axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> W,B</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self,X,Y,classes = <span class="literal">None</span>,learning_rate = <span class="number">0.001</span>,num_iters = <span class="number">100</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练softmax模型，此时采用的批量梯度下降算法。</span></span><br><span class="line"><span class="string">        :param X: 样本集</span></span><br><span class="line"><span class="string">        :param Y: 标签集，请确保类别标号从0开始</span></span><br><span class="line"><span class="string">        :param classes: 共有多少个类别，如果为None，则会根据Y自动调整</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率，默认0.001</span></span><br><span class="line"><span class="string">        :param num_iters: 最大迭代次数，默认100次</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1.设置模型类别数</span></span><br><span class="line">        <span class="keyword">if</span> classes == <span class="literal">None</span>:</span><br><span class="line">            self.__classes = np.<span class="built_in">max</span>(Y) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__classes = classes</span><br><span class="line">        <span class="comment"># 2.将label向量化</span></span><br><span class="line">        Y = self.__labelTransform(Y,self.__classes)</span><br><span class="line">        <span class="comment"># 3.根据样本调整W的维度，并进行初始化</span></span><br><span class="line">        sample_nums,feature_nums = X.shape <span class="comment"># 行为样本个数，列为特征数</span></span><br><span class="line">        <span class="keyword">if</span> self.__theta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.__theta = self.__init_theta(self.__classes,feature_nums,self.__random_state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度下降更新参数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">            <span class="comment"># 4.对样本特征集线性加权组合</span></span><br><span class="line">            Z = self.__linear_combination(X, self.__theta)</span><br><span class="line">            <span class="comment"># 5.通过softmax激活函数，计算后验概率矩阵</span></span><br><span class="line">            A = self.__cal_softmax(Z)</span><br><span class="line">            self.__theta[<span class="string">&#x27;W&#x27;</span>],self.__theta[<span class="string">&#x27;B&#x27;</span>] = self.__gradient_iteration(self.__theta[<span class="string">&#x27;W&#x27;</span>],self.__theta[<span class="string">&#x27;B&#x27;</span>],X,Y,A,learning_rate)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;第<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>次迭代后的代价:<span class="subst">&#123;self.cal_Costfunction(A,Y)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    iris = datasets.load_iris() <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    dataSet = iris.data <span class="comment"># 特征集</span></span><br><span class="line">    target = iris.target <span class="comment"># label集</span></span><br><span class="line">    <span class="comment"># print(dataSet)</span></span><br><span class="line">    model = SoftmaxModel(random_state=<span class="number">10</span>)</span><br><span class="line">    model.train(dataSet,target,learning_rate = <span class="number">0.5</span>,num_iters = <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210262258797.png" /></p><p><img src="https://img.issey.top/img/202210262258183.png" /></p><p>前几次代价跳的比较厉害，这是学习率设置的原因。小了吧又下降的太慢了。这是我从很多学习率里选的一个比较合适的。看这情况后面还是收敛了。</p><p><img src="https://img.issey.top/img/202210262314322.png" /></p><h3 id="step8完成预测模块">step8：完成预测模块</h3><p>这个部分就简单了，正向传播之后多加一个判断就行了：<code>谁的概率最大选谁</code>。</p><p>哦对，注意，预测完毕后返回的<spanclass="math inline">\(Y_{predict}\)</span>应该是没有进行label向量化的状态。即如果<spanclass="math inline">\(y_{pre} = c_k\)</span>，那么<spanclass="math inline">\(y_{pre}\)</span>的值应该是<spanclass="math inline">\(k-1\)</span>而不是一个向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getTheta</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取训练好的模型参数，方便下次不用重新训练</span></span><br><span class="line"><span class="string">    :return: theta,模型参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> self.__theta</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X,theta = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对指定样本集进行预测</span></span><br><span class="line"><span class="string">    :param X: 需要进行预测的样本</span></span><br><span class="line"><span class="string">    :param theta: 方便不用每一次都训练，所以也可以直接传模型参数，默认为None</span></span><br><span class="line"><span class="string">    :return: 预测结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> theta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        theta = self.__theta</span><br><span class="line">    Z = self.__linear_combination(X,theta)</span><br><span class="line">    A = self.__cal_softmax(Z)</span><br><span class="line">    y_predict = np.argmax(A,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y_predict</span><br></pre></td></tr></table></figure><p>OK,到此最基本的softmax分类器就实现了，这是整个类的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxModel</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,random_state = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型</span></span><br><span class="line"><span class="string">        :param random_state: 指定随机种子，默认为None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__random_state = random_state</span><br><span class="line">        self.__theta = <span class="literal">None</span> <span class="comment"># 模型参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__labelTransform</span>(<span class="params">self,Y,classes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将label向量化</span></span><br><span class="line"><span class="string">        :param Y: label集合</span></span><br><span class="line"><span class="string">        :param classes: 共有多少类</span></span><br><span class="line"><span class="string">        :return: 向量化后的label集</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        vec_Y = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> Y:</span><br><span class="line">            vec_label = <span class="built_in">list</span>(np.zeros(classes))</span><br><span class="line">            vec_label[y] = <span class="number">1</span></span><br><span class="line">            vec_Y.append(vec_label)</span><br><span class="line">        vec_Y = np.array(vec_Y)</span><br><span class="line">        <span class="keyword">return</span> vec_Y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init_theta</span>(<span class="params">self,classes,feature_nums,random_state</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型参数</span></span><br><span class="line"><span class="string">        :param classes: 一共有多少类，即K</span></span><br><span class="line"><span class="string">        :param feature_nums: 样本特征数目</span></span><br><span class="line"><span class="string">        :param random_state: 随机种子</span></span><br><span class="line"><span class="string">        :return: 模型参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        theta = &#123;&#125;</span><br><span class="line">        np.random.seed(random_state)</span><br><span class="line">        theta[<span class="string">&#x27;W&#x27;</span>] = np.random.randn(classes,feature_nums) <span class="comment"># 关于randn的用法请自行查找</span></span><br><span class="line">        theta[<span class="string">&#x27;B&#x27;</span>] = np.random.randn(classes)</span><br><span class="line">        <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__linear_combination</span>(<span class="params">self,X,theta</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        对样本特征进行线性加权组合</span></span><br><span class="line"><span class="string">        :param X: 特征集</span></span><br><span class="line"><span class="string">        :param theta: 模型参数</span></span><br><span class="line"><span class="string">        :return: Z,加权后的输出</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Z = np.dot(X,theta[<span class="string">&#x27;W&#x27;</span>].T)+np.tile(theta[<span class="string">&#x27;B&#x27;</span>],(X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cal_softmax</span>(<span class="params">self,Z</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        通过softmax激活，计算后验概率</span></span><br><span class="line"><span class="string">        :param Z: 隐藏层输出</span></span><br><span class="line"><span class="string">        :return: A,后验概率矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        expZ = np.exp(Z)</span><br><span class="line">        A = np.zeros_like(expZ)</span><br><span class="line">        denominator = np.<span class="built_in">sum</span>(expZ,axis=<span class="number">1</span>) <span class="comment"># 计算分母</span></span><br><span class="line">        N = expZ.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            A[i] = expZ[i]/denominator[i]</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_Costfunction</span>(<span class="params">self,A,Y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算代价函数</span></span><br><span class="line"><span class="string">        :param A: 后验概率矩阵（预测概率矩阵）</span></span><br><span class="line"><span class="string">        :param Y: 真实label矩阵</span></span><br><span class="line"><span class="string">        :return: 代价</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        J = -np.<span class="built_in">sum</span>(np.multiply(Y,np.log2(A)))</span><br><span class="line">        J /= Y.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__gradient_iteration</span>(<span class="params">self,W,B,X,Y,A,learning_rate</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        一轮梯度迭代</span></span><br><span class="line"><span class="string">        :param W: W</span></span><br><span class="line"><span class="string">        :param B: B</span></span><br><span class="line"><span class="string">        :param X: X</span></span><br><span class="line"><span class="string">        :param Y: Y</span></span><br><span class="line"><span class="string">        :param A: A</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">        :return: 更新后的W,B</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        A_Y = A-Y</span><br><span class="line">        W = W-(learning_rate/X.shape[<span class="number">0</span>])*np.dot(A_Y.T,X)</span><br><span class="line">        B = B-(learning_rate/X.shape[<span class="number">0</span>])*np.<span class="built_in">sum</span>(A_Y,axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> W,B</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self,X,Y,classes = <span class="literal">None</span>,learning_rate = <span class="number">0.001</span>,num_iters = <span class="number">100</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练softmax模型，此时采用的批量梯度下降算法。</span></span><br><span class="line"><span class="string">        :param X: 样本集</span></span><br><span class="line"><span class="string">        :param Y: 标签集，请确保类别标号从0开始</span></span><br><span class="line"><span class="string">        :param classes: 共有多少个类别，如果为None，则会根据Y自动调整</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率，默认0.001</span></span><br><span class="line"><span class="string">        :param num_iters: 最大迭代次数，默认100次</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1.设置模型类别数</span></span><br><span class="line">        <span class="keyword">if</span> classes == <span class="literal">None</span>:</span><br><span class="line">            self.__classes = np.<span class="built_in">max</span>(Y) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__classes = classes</span><br><span class="line">        <span class="comment"># 2.将label向量化</span></span><br><span class="line">        Y = self.__labelTransform(Y,self.__classes)</span><br><span class="line">        <span class="comment"># 3.根据样本调整W的维度，并进行初始化</span></span><br><span class="line">        sample_nums,feature_nums = X.shape <span class="comment"># 行为样本个数，列为特征数</span></span><br><span class="line">        <span class="keyword">if</span> self.__theta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.__theta = self.__init_theta(self.__classes,feature_nums,self.__random_state)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;初始参数:&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.__classes):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;w<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;self.__theta[<span class="string">&#x27;W&#x27;</span>][k]&#125;</span>,b<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;self.__theta[<span class="string">&#x27;B&#x27;</span>][k]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度下降更新参数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">            <span class="comment"># 4.对样本特征集线性加权组合</span></span><br><span class="line">            Z = self.__linear_combination(X, self.__theta)</span><br><span class="line">            <span class="comment"># 5.通过softmax激活函数，计算后验概率矩阵</span></span><br><span class="line">            A = self.__cal_softmax(Z)</span><br><span class="line">            self.__theta[<span class="string">&#x27;W&#x27;</span>],self.__theta[<span class="string">&#x27;B&#x27;</span>] = self.__gradient_iteration(self.__theta[<span class="string">&#x27;W&#x27;</span>],self.__theta[<span class="string">&#x27;B&#x27;</span>],X,Y,A,learning_rate)</span><br><span class="line">            <span class="comment"># print(f&quot;第&#123;i+1&#125;次迭代后的代价:&#123;self.cal_Costfunction(A,Y)&#125;&quot;)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;结果参数:&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.__classes):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;w<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;self.__theta[<span class="string">&#x27;W&#x27;</span>][k]&#125;</span>,b<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;self.__theta[<span class="string">&#x27;B&#x27;</span>][k]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getTheta</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取训练好的模型参数，方便下次不用重新训练</span></span><br><span class="line"><span class="string">        :return: theta,模型参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.__theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X,theta = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        对指定样本集进行预测</span></span><br><span class="line"><span class="string">        :param X: 需要进行预测的样本</span></span><br><span class="line"><span class="string">        :param theta: 方便不用每一次都训练，所以也可以直接传模型参数，默认为None</span></span><br><span class="line"><span class="string">        :return: 预测结果</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> theta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            theta = self.__theta</span><br><span class="line">        Z = self.__linear_combination(X,theta)</span><br><span class="line">        A = self.__cal_softmax(Z)</span><br><span class="line">        y_predict = np.argmax(A,axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> y_predict</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2id="使用自己实现的softmax多分类器完成鸢尾花多分类">使用自己实现的softmax多分类器完成鸢尾花多分类</h2><p>这部分的内容就不多解释了，还是老套路：</p><ol type="1"><li>分割训练集和测试集（这次选择7：3）</li><li>对训练集Z-score标准化，<code>并用训练集标准化时计算的参数去标准化测试集</code>（可别一起丢进去标准化）</li><li>训练集丢进去训练，训练结束把测试集丢进去测试</li><li>对测试集的测试结果进行评估</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 结果评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 拆分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="comment"># 数据标准化</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    random_state = <span class="number">10</span></span><br><span class="line">    iris = datasets.load_iris() <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    dataSet = iris.data <span class="comment"># 特征集</span></span><br><span class="line">    target = iris.target <span class="comment"># label集-</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(dataSet, target, train_size=<span class="number">0.7</span>, random_state=random_state)</span><br><span class="line">    <span class="comment"># 使用sklearn进行Z-score标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    X_train = scaler.fit_transform(X_train)  <span class="comment"># 标准化训练集X</span></span><br><span class="line">    <span class="comment"># 标准化测试集x,只有训练集才fit_transform，测试集是transform</span></span><br><span class="line">    X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = SoftmaxModel(random_state=random_state)</span><br><span class="line">    model.train(X_train,y_train,learning_rate = <span class="number">0.5</span>,num_iters = <span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_predict =  model.predict(X_test)</span><br><span class="line"></span><br><span class="line">    class_names = [<span class="string">&#x27;第一类&#x27;</span>, <span class="string">&#x27;第二类&#x27;</span>, <span class="string">&#x27;第三类&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test, y_predict, target_names=class_names))</span><br></pre></td></tr></table></figure><p>评估结果：</p><p><img src="https://img.issey.top/img/202210270029022.png" /></p><p>至少通过评估结果来看，自己实现的softmax模型没什么奇怪的问题。不过可能在一些细节上还有很多改进空间。</p><h1 id="第二部分">第二部分</h1><h2id="使用skrean中的softmax其实是逻辑回归">使用skrean中的softmax（其实是逻辑回归）</h2><blockquote><p>怪，我发现sklearn里没有softmax，只有逻辑回归的包，但是它那个逻辑回归的包又可以作多分类。因为原本的逻辑回归只能作二分类，所以我觉得它实现的逻辑回归可能就是softmax。因此调用的是LogisticRegression。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 结果评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="comment"># 数据标准化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    random_state = <span class="number">10</span></span><br><span class="line">    iris = datasets.load_iris() <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    dataSet = iris.data <span class="comment"># 特征集</span></span><br><span class="line">    target = iris.target <span class="comment"># label集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(dataSet, target, train_size=<span class="number">0.7</span>, random_state=random_state)</span><br><span class="line">    <span class="comment"># 使用sklearn进行Z-score标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    X_train = scaler.fit_transform(X_train)  <span class="comment"># 标准化训练集X</span></span><br><span class="line">    <span class="comment"># 标准化测试集x,只有训练集才fit_transform，测试集是transform</span></span><br><span class="line">    X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line">    model = LogisticRegression()</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    y_predict = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">    class_names = [<span class="string">&#x27;第一类&#x27;</span>, <span class="string">&#x27;第二类&#x27;</span>, <span class="string">&#x27;第三类&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test, y_predict, target_names=class_names))</span><br></pre></td></tr></table></figure><h2 id="结果对比">结果对比</h2><p><img src="https://img.issey.top/img/202210270052701.png" /></p><p>看到这个结果，虽然我知道是数据集太小的原因，但我内心十分不服，于是把自己实现的分类器的迭代次数改成了1000，再跑了一次。</p><p>自己实现的分类器迭代次数改成迭代1000次后：</p><p><img src="https://img.issey.top/img/202210270053689.png" /></p><p>不得不说，这个数据集实在太小了，只有150个，分出来测试集才45个样本，所以预测结果才会这么好。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记13——【Softmax多分类 1】完整流程与详细公式推导</title>
      <link href="/article/324b71593ade/"/>
      <url>/article/324b71593ade/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>相关文章</strong></p><ul><li>【1】<ahref="https://blog.csdn.net/qq_52466006/article/details/126037505?spm=1001.2014.3001.5502">【机器学习笔记1】一元线性回归模型及预测_TwilightSparkle.的博客-CSDN博客_一元线性回归模型预测</a></li><li>【2】<ahref="https://blog.csdn.net/qq_52466006/article/details/126113034?spm=1001.2014.3001.5502">【机器学习笔记4】逻辑回归模型_TwilightSparkle.的博客-CSDN博客_逻辑回归模型</a></li><li>【3】<ahref="https://blog.csdn.net/weixin_44441131/article/details/119819025">对数损失和交叉熵损失_InceptionZ的博客-CSDN博客_深度学习对数和交叉熵的区别是什么</a></li></ul><p><strong>参考文章</strong></p><ul><li>【1】<ahref="https://blog.csdn.net/bqw18744018044/article/details/83120425">【深度学习】：超详细的Softmax求导_BQW_的博客-CSDN博客_softmax导数</a></li></ul><h1 id="前言">前言</h1><p>早在写逻辑回归二分类时就提到过多分类的问题，时隔两个月终于可以把当初这个坑填上了。softmax本应该属于深度学习的笔记，但归为机器学习的笔记，也没什么不妥。好了，闲话不多说。</p><p>本篇为softmax多分类器的原理与公式详细推导，关于softmax的具体代码实现和应用将在下一章说明。</p><h1 id="softmax多分类器简介">softmax多分类器简介</h1><p>softmax多分类器，一种基于softmax函数的分类器，它可以预测一个样本属于每个样本的概率。softmax一般用于神经网络的输出层，叫做softmax层。</p><p>不过不必担心，本篇文章对神经网络没有要求，即使没学过神经网络，也可以学会softmax，并在下一篇文章中学会自己实现softmax代码并完成一个具体的训练和预测示例。</p><h1id="如何利用softmax对样本进行分类">如何利用softmax对样本进行分类</h1><h2 id="问题引入">问题引入</h2><p>如何使用训练集训练一个基于softmax函数的多分类模型，并使用该模型对测试集进行预测？</p><blockquote><p>好吧，这个问题引入写的有点草率了，想了一会儿没啥好写的，就是多分类问题。</p></blockquote><h2 id="明确变量与集合">明确变量与集合</h2><p>为了在接下来的推导过程中不搞混各变量的含义，我们需要先明确涉及的变量和集合。</p><ol type="1"><li>设样本集含有<code>N</code>个样本,令样本集为<code>X</code>,则<spanclass="math inline">\(X =\{x^1,x^2,x^3...,x^N\}\)</span>。注意这里使用的上标，这样写是为了后续方便表示样本的特征。</li><li>设任意一个样本含有<code>M</code>个特征，令任意一个样本为<spanclass="math inline">\(x\)</span>,则<span class="math inline">\(x =(x_1,x_2,x_3,...,x_M)\)</span>。则第<code>i</code>个样本的第<code>j</code>个特征表示为<spanclass="math inline">\(x_j^i\)</span>。</li><li>设样本集共分为<code>K</code>类。记分类集为<code>C</code>,则<spanclass="math inline">\(C = \{c_1,c_2,...,c_K\}\)</span>。</li><li>设样本集对应的label集为<code>Y</code>,每一个样本对应一个<code>y</code>,则<spanclass="math inline">\(Y=\{y_1,y_2,y_3,..,y_N\}\)</span>。其中，<spanclass="math inline">\(y_i\)</span> 的值为<span class="math inline">\(0\sim K-1\)</span> 中的一个整数。</li></ol><h2 id="进一步处理">进一步处理</h2><h3 id="对label向量化">对label向量化</h3><p>回顾二分类问题，样本的label要么是0，要么是1。现在变成多分类了，那么样本的label就变成了<spanclass="math inline">\(0 \sim K-1\)</span>中任意一个整数。这就不便于我们计算了，所以需要将label进行向量化：</p><p>若第i个样本属于第K个分类，即<spanclass="math inline">\(y_i=c_k\)</span>。那么对其进行向量化后： <spanclass="math display">\[y_i = (0,...,1,...,0)\]</span> 仅在第K-1的位置上值为1，其他位置均为0。</p><p><strong>示例</strong></p><p>假设共有5个分类，样本<span class="math inline">\(x^i\)</span>属于第3类，则： <span class="math display">\[y_i = (0,0,1,0,0)\]</span></p><h3 id="对样本特征进行加权组合">对样本特征进行加权组合</h3><p>在进入softmax层之前，不得不提到特征的加权组合。在神经网络中，这一步的处理其实叫做<code>隐藏层</code>，不过这里就不对隐藏层做拓展了。</p><p>为了使softmax拥有一套完整的流程，我们这里对样本特征进行最简单的<code>线性加权组合</code>：</p><hr /><p>现在有任意一个样本<span class="math inline">\(x =(x_1,x_2,x_3,...,x_M)\)</span> 。<spanclass="math inline">\(x_i\)</span>表示该样本的第<spanclass="math inline">\(i\)</span>个特征。令 <span class="math display">\[z_k = w_k^Tx+b_k = (\sum_{i=1}^Mw_{k,i}x_i)+b_k\]</span> 现在多出了几个参数，下面对它们进行说明：</p><ol type="1"><li><span class="math inline">\(z\)</span>：对样本<spanclass="math inline">\(x\)</span>的特征进行线性加权组合后得到的输出，<spanclass="math inline">\(z = (z_1,z_2,...,z_K)\)</span>。有几个分类就有就有几个<span class="math inline">\(z_k\)</span>。</li><li><spanclass="math inline">\(z_k\)</span>：表示对样本的特征进行权值向量为<spanclass="math inline">\(w_k\)</span>的线性变换后得到的值，为<code>标量</code>。</li><li><span class="math inline">\(w_k\)</span>：第k个权值向量，维度与<spanclass="math inline">\(x\)</span>一样，<spanclass="math inline">\((1,N)\)</span>。</li></ol><blockquote><p>将<span class="math inline">\(b_k\)</span>放在<spanclass="math inline">\(w_k\)</span> 里：</p><p>只需要将<span class="math inline">\(w_k\)</span>加一列，即<spanclass="math inline">\(w_k = (...,b_k)\)</span>，</p><p>然后再往<span class="math inline">\(x\)</span>加一列，即<spanclass="math inline">\(x = (...,1)\)</span> 。</p><p>就可以将<span class="math inline">\(z_k\)</span> 的表达式变为:<spanclass="math inline">\(z_k = w_k^Tx\)</span></p></blockquote><p>图示（不含<span class="math inline">\(b_k\)</span>的情况）：</p><p><img src="https://img.issey.top/img/202210252102719.png" /></p><h2 id="softmax激活函数">softmax激活函数</h2><p>激活函数也是神经网络里的称呼，直接叫softmax函数也没问题。</p><p>简单来说，softmax预测分类是作用在<spanclass="math inline">\(z_k\)</span>上的，这也是为什么之前我们需要对样本特征进行加权组合得到<spanclass="math inline">\(z_k\)</span>。接下来给出公式： <spanclass="math display">\[a_k = P(y=c_k|x,\theta) = \frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}},k =0,1,...,K-1\]</span></p><blockquote><p>稍微解释一下这个公式：在模型参数<spanclass="math inline">\(\theta\)</span>固定，且样本已知的情况下，该样本属于第K个分类的概率等于后面那一坨公式。(本文中<spanclass="math inline">\(\theta\)</span>即各权值向量<spanclass="math inline">\(w_k\)</span>)</p></blockquote><h3id="通过softmax激活函数得到各分类预测概率">通过softmax激活函数得到各分类预测概率</h3><p>将一个样本对应的<span class="math inline">\(a_k\)</span>全部算出来，这些<span class="math inline">\(a_k\)</span>就是该样本属于各分类的预测概率。</p><hr /><p><strong>示例</strong></p><p>假设一个样本<spanclass="math inline">\(x\)</span>的特征进行加权组合后得到的<spanclass="math inline">\(z\)</span>为： <span class="math display">\[z = (0.6,1.1,-1.5,1.2,3.2,-1.1)\]</span> 经过softmax激活后： <span class="math display">\[a = (0.055,0.090,0.0067,0.10,0.74,010)\]</span> 比如其中的0.74表示：在已知权值<spanclass="math inline">\(w\)</span> 的情况下，某一个样本<spanclass="math inline">\(x\)</span> 属于第5个分类的概率为0.74。</p><blockquote><p>那么如果这是在做测试集的测试，根据“属于谁的概率最大就选谁”的原则，我们可以说x的预测分类为第5个分类，即<spanclass="math inline">\(label = 4\)</span>。不过在训练集训练模型时，我们只求到x属于每个类别的概率。</p></blockquote><hr /><p>在神经网络中，将上述过程描述为<code>神经网络的正向传播</code>。</p><h2 id="总结softmax的正向传播流程">总结softmax的正向传播流程</h2><ol type="1"><li>固定模型参数<span class="math inline">\(\theta\)</span>,在本篇文章中，<span class="math inline">\(\theta\)</span>即各权值向量<span class="math inline">\(w_k\)</span>。</li><li>对于一个样本<span class="math inline">\(x =(x_1,x_2,...,x_M)\)</span>，先对它的特征做加权组合得到<spanclass="math inline">\(z =(z_1,..,z_K)\)</span>。在本篇文章中，我们仅做了一次线性加权组合。而在神经网络中，可能会对x的特征做多次变换，即隐藏层不止一层。</li><li>对<span class="math inline">\(z\)</span> 进行softmax激活得到<spanclass="math inline">\(a = (a_1,...a_K)\)</span>，<spanclass="math inline">\(a_k\)</span> 表示样本x属于第k类的概率。</li></ol><h3 id="引出新的问题">引出新的问题</h3><p>我们进行正向传播时，是在模型参数<spanclass="math inline">\(\theta\)</span>已知的情况下进行。而我们现在已知的只有训练集，怎么根据训练集得到这个模型参数<spanclass="math inline">\(\theta\)</span> 呢？</p><p>答案是迭代。我们可以随机初始化模型参数<spanclass="math inline">\(\theta\)</span>，然后通过某种算法一轮一轮的更新<spanclass="math inline">\(\theta\)</span> ,直到<spanclass="math inline">\(\theta\)</span> 收敛。</p><p>在softmax分类器中，我们使用<strong>梯度下降算法</strong>对<spanclass="math inline">\(\theta\)</span>进行优化。关于梯度下降算法请见相关文章【1】，在这里就不多说了。</p><h1 id="softmax损失函数">softmax损失函数</h1><p>损失函数之前的文章已经提到过N次了，不过这里还是说一下。损失函数是用来衡量label预测值和真实值之间的某种差距的函数。常见的损失函数有0-1损失函数、对数损失函数、平方损失函数等等。每种模型需要选择的损失函数不同。如果损失函数选择不当，会对优化过程造成很大影响，详情请见相关文章【2】。</p><h2 id="损失函数和代价函数的区别">损失函数和代价函数的区别</h2><p>说来惭愧，我一直都没真正搞清楚损失函数和代价函数的区别，所以之前写文章可能存在这两个词汇混用的问题。这次专门区分一下两者的区别。</p><h3 id="损失函数">损失函数</h3><p>损失函数（Loss Function）是定义在<strong>单个样本</strong>上的，描述单个样本的label预测值与真实值的某种差距。一般记为<spanclass="math inline">\(L(\hat y,y)\)</span> 。</p><h3 id="代价函数">代价函数</h3><p>代价函数（Cost Function）是定义在<strong>整个训练集</strong>上的，是所有样本误差的平均，也就是<strong>损失函数的平均</strong>。一般记为<spanclass="math inline">\(J(...)\)</span>。<code>“...”指代的一般为模型未知参数，有许多种写法。</code><span class="math display">\[J(...) = \frac{1}{N}\sum_{i=0}^{N-1}L(\hat y,y)\]</span> 其中，N为本次参加训练的样本个数。</p><blockquote><p>我们进行梯度下降时使用的是<code>代价函数求偏导</code>，而对代价函数求偏导其实只需要对损失函数求偏导，然后求个平均就是代价函数求偏导。所以接下来的求偏导过程写的是对损失函数求偏导。不过在最后的总结时，会对损失函数求偏导的结果求平均变成代价函数求偏导的结果。</p></blockquote><h2 id="softmax的损失函数表达式">softmax的损失函数表达式</h2><p>softmax分类器选用的是<code>交叉熵损失函数</code>。有的文章也说是选用<code>对数损失函数</code>。这是因为：</p><blockquote><p>对数损失函数(Log loss function)和交叉熵损失函数(Cross-entroy lossfunction)表达式本质式一样的。不过这两种损失函数对应的上一层结构不同，对数损失函数经常对应的是Sigmoid函数的输出，用于二分类问题；而交叉熵损失函数经常对应的是Softmax函数的输出，用于多分类问题。详情请见相关文章【3】。</p></blockquote><p><span class="math display">\[L(\hat y,y) = -\sum_{k=1}^Ky_kloga_k\]</span></p><p>注，log通常以2为底，有的文章也会使用ln。</p><p>其中：</p><ul><li><span class="math inline">\(\hat y\)</span>:单个训练样本label预测值，$y = (a_1,...,a_k) <spanclass="math inline">\(; 例如：\)</span>y =(0.055,0.090,0.0067,0.10,0.74,010)$</li><li><span class="math inline">\(y\)</span>:单个训练样本label真实值,<spanclass="math inline">\(y = (y_1,...,y_k)\)</span>; 例如：<spanclass="math inline">\(y = (0,0,0,0,1,0)\)</span></li><li><span class="math inline">\(y_k\)</span>:单个训练样本属于第k类的真实值，取值0或1</li><li><spanclass="math inline">\(a_k\)</span>：单个训练样本属于第k类的预测值，为一个概率值</li></ul><h1 id="对代价函数进行梯度下降">对代价函数进行梯度下降</h1><h2 id="梯度下降算法步骤">梯度下降算法步骤</h2><p>回顾梯度下降算法的步骤：</p><ol type="1"><li>初始化模型未知参数<span class="math inline">\(W =(w_1,..,w_K)\)</span>，其中每一个<spanclass="math inline">\(w_k\)</span>为向量，维度与样本x相同。K的值与类别数相同。</li><li>重复进行以下公式直到收敛：</li></ol><p><span class="math display">\[w_k = w_k - a\frac{\partial J}{\partial w_k},\quad k = 0,1,...,K-1\]</span></p><p>对上述公式进行说明：</p><p><strong>为什么没有<spanclass="math inline">\(b_k\)</span>?</strong></p><p><span class="math inline">\(b_k\)</span>被融合到向量<spanclass="math inline">\(w_k\)</span>里了。最后总结结果时会单独把<spanclass="math inline">\(b_k\)</span> 提出来。</p><p><strong><span class="math inline">\(a\)</span>是什么？</strong></p><p>学习率，是需要人工调整的超参数。它控制的是梯度下降中每一步的跨度大小。关于学习率更多说明请见相关文章【1】。</p><p><strong>这个公式里明明是对代价函数J求偏导，为什么下面推导的是损失函数求偏导？</strong></p><p>刚才在区分代价函数和损失函数时有说过，对求得的损失函数偏导求平均就是代价函数求偏导的结果。</p><hr /><h2 id="损失函数求偏导的详细推导">损失函数求偏导的详细推导</h2><p>此部分参考了文章：参考文章【1】</p><p>之前提到的softmax损失函数： <span class="math display">\[L(\hat y,y) = -\sum_{k=1}^Ky_kloga_k~~~~~(1)\]</span> 其中， <span class="math display">\[\begin{split}&amp; a_k = \frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}}~~~~~~(2) \\&amp; z_k = w_k^Tx = \sum_{i=1}^Mw_{k,i}x_i~~~(3)\\&amp; k = 0,1,...,K-1\end{split}\]</span> 现在需要求：<span class="math inline">\(\frac{\partial L(\haty,y)}{\partial w_k}\)</span></p><p>根据<code>链式求导法则</code>,有： <span class="math display">\[\frac{\partial L(\hat y,y)}{\partial w_k} = \frac{\partial L(\haty,y)}{\partial z_k}·\frac{\partial z_k}{\partial w_k}~~~~(4)\]</span></p><hr /><p><strong>1 求解<span class="math inline">\(\frac{\partialz_k}{\partial w_k}\)</span></strong> <span class="math display">\[\begin{split}\frac{\partial z_k}{\partial w_k} &amp; = \frac{\partial(w_k^Tx)}{\partial w_k} = x ~~~~(5)\end{split}\]</span></p><hr /><p><strong>2 求解<span class="math inline">\(\frac{\partial L(\haty,y)}{\partial z_k}\)</span></strong></p><blockquote><p>这也是整个softmax最关键的一步，很多教程推完这里就结束了。</p></blockquote><p>根据（2）式，<span class="math inline">\(a_j\)</span> 均包含<spanclass="math inline">\(z\)</span>的所有分量,所以要求<spanclass="math inline">\(z_k\)</span> 的偏导，所以对于<spanclass="math inline">\(z_k\)</span>的偏导，每一个<spanclass="math inline">\(a_j\)</span> 都有贡献： <spanclass="math display">\[\frac{\partial L(\hat y,y)}{\partial z_k} = \sum_{j=1}^K[\frac{\partialL(\hat y,y)}{\partial a_j}\frac{\partial a_j}{\partial z_k}]~~~~(6)\]</span> <strong>2.1 求解<span class="math inline">\(\frac{\partialL(\hat y,y)}{\partial a_j}\)</span></strong> <spanclass="math display">\[\begin{split}\frac{\partial L(\hat y,y)}{\partial a_j} &amp; = \frac{\partial(-\sum_{j=1}^Ky_jloga_j)}{\partial a_j} = - \frac{y_j}{a_j}~~~~(7)\end{split}\]</span></p><p><strong>2.2 求解<span class="math inline">\(\frac{\partiala_j}{\partial z_k}\)</span></strong></p><p>刚才说过，每一个<span class="math inline">\(a_j\)</span> 都要对<spanclass="math inline">\(z_k\)</span> 求导，那自然有两种情况：<spanclass="math inline">\(j = k\)</span> 和<span class="math inline">\(j\neq k\)</span> 。现在对这两种情况分开讨论。</p><strong>2.2.1 当<span class="math inline">\(j \neq k\)</span></strong><span class="math display">\[\begin{split}\frac{\partial a_j}{\partial z_k}&amp;=  \frac{\partial(\frac{e^{z_j}}{\sum_{i=1}^Ke^{z_i}})}{\partialz_k} \\&amp; = -e^{z_j}\frac{1}{(\sum_{i=1}^Ke^{z_i})^2}e^{z_k} \\&amp; =-\frac{e^{z_j}}{\sum_{i=1}^Ke^{z_i}}·\frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}}\\&amp; = -a_ja_k ~~~~~(8)\end{split}\]</span> <strong>2.2.2 当<spanclass="math inline">\(j=k\)</span></strong> $$<span class="math display">\[\begin{split}\frac{\partial a_j}{\partial z_k} &amp;=  \frac{\partial a_k}{\partialz_k}\\&amp;=\frac{\partial(\frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}})}{\partial z_k}\\&amp; =\frac{e^{z_k}\sum_{i=1}^Ke^{z_i}-(e^{z_k})^2}{(\sum_{i=1}^Ke^{z_i})^2}\\&amp; =\frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}}(1-\frac{e^{z_k}}{\sum_{i=1}^Ke^{z_i}})\\&amp; = a_k(1-a_k)~~~~(9)\end{split}\]</span><span class="math display">\[**2.2.3 将（7）、（8）、（9）代入（6）求解**\]</span><span class="math display">\[\begin{split}\frac{\partial L(\hat y,y)}{\partial z_k} &amp; =\sum_{j=1}^K[\frac{\partial L(\hat y,y)}{\partial a_j}\frac{\partiala_j}{\partial z_k}] \\&amp; = \sum_{j=1}^K[-\frac{y_j}{a_j}\frac{\partial a_j}{\partial z_k}]\\&amp; = -\frac{y_k}{a_k}\frac{\partial a_k}{\partialz_k}+\sum_{j=1,j\neq k}^K[-\frac{y_j}{a_j}\frac{\partial a_j}{\partialz_k}] \\&amp; = -\frac{y_k}{a_k}a_k(1-a_k)+\sum_{j=1,j\neqk}^K[-\frac{y_j}{a_j}·-a_ja_k] \\&amp; = y_k(a_k-1)+\sum_{j=1,j\neq k}^Ky_ja_k \\&amp; = -y_k+y_ka_k+\sum_{j=1,j\neq k}^Ky_ja_k \\&amp; = -y_k+a_k\sum_{j=1}^Ky_j~~~~(10)\end{split}\]</span><p>$$ ok,到这里已经初步求出了损失函数对<spanclass="math inline">\(z_k\)</span>的偏导。不过我们还可以对它进一步优化，所以我们现在考虑y的特点，前面说过：</p><blockquote><p><span class="math inline">\(y\)</span>:单个训练样本label真实值,<spanclass="math inline">\(y = (y_1,...,y_k)\)</span>; 例如：<spanclass="math inline">\(y = (0,0,0,0,1,0)\)</span></p></blockquote><p>也就是说向量y中只有一个值为1,所以<spanclass="math inline">\(\sum_{j=1}^Ky_j = 1\)</span> 。</p><p>于是(9)式将变为: <span class="math display">\[\frac{\partial L(\hat y,y)}{\partial z_k} = a_k-y_k~~~~(11)\]</span> 其中，</p><ul><li><span class="math inline">\(a_k\)</span>:单个训练样本属于第k类的预测值，为一个概率值。</li><li><span class="math inline">\(y_k\)</span>:单个训练样本属于第k类的真实值，取值0或1</li></ul><blockquote><p>到此为止，softmax最核心的偏导已经推导完毕。多么简约美妙的一个公式！</p></blockquote><hr /><p><strong>3.将（5）、（11）代入（4）求解L对<spanclass="math inline">\(w_k\)</span>的偏导</strong> <spanclass="math display">\[\begin{split}\frac{\partial L(\hat y,y)}{\partial w_k}&amp; = \frac{\partial L(\haty,y)}{\partial z_k}·\frac{\partial z_k}{\partial w_k} \\&amp; = (a_k-y_k)x\end{split}\]</span> 3.1 取出<span class="math inline">\(b_k\)</span></p><p>最开始说了，我们计算时先把<span class="math inline">\(b_k\)</span>融进<span class="math inline">\(w_k\)</span> 了。现在将它取出来：</p><p>因为把<span class="math inline">\(b_k\)</span>融合进<spanclass="math inline">\(w_k\)</span> 后，<spanclass="math inline">\(w_k\)</span>最后一个分量为<spanclass="math inline">\(b_k\)</span> 。而<spanclass="math inline">\(x\)</span>最后一个分量为1。所以 <spanclass="math inline">\(z_k\)</span>对<spanclass="math inline">\(w_k\)</span> 求偏导后，<spanclass="math inline">\(b_k\)</span>那个分量对应的偏导就是1。所以 <spanclass="math display">\[\frac{\partial L(\hat y,y)}{\partial b_k} =a_k-y_k\]</span></p><h3 id="结果总结">结果总结</h3><p>好了，现在来总结一下softmax损失函数求偏导的结果： <spanclass="math display">\[\begin{split}&amp; \frac{\partial L(\hat y,y)}{\partial w_k} = (a_k-y_k)x \\&amp; \frac{\partial L(\hat y,y)}{\partial b_k} =a_k-y_k\end{split}\]</span></p><h2 id="更新权重">更新权重</h2><p>还记得梯度下降的步骤吗？</p><p>梯度下降更新权重时，是用的<code>代价函数的偏导</code>更新的，我们刚才求导的只是<code>损失函数的偏导</code>。所以还需要对它们求平均。<span class="math display">\[\begin{split}&amp; \frac{\partial J}{\partial w_k} =\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k)x \\&amp; \frac{\partial J}{\partial b_k} =\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k)\end{split}\]</span> 其中，N为本次参加训练的样本个数。另外，上述式子省去了上标<spanclass="math inline">\(i\)</span>，上标<spanclass="math inline">\(i\)</span>表示这是第i个样本。</p><blockquote><p>顺带一提，像上面这种根据推出的结果反过去更新权重的流程，在神经网络中被称为<code>反向传播</code>。</p></blockquote><h1 id="总结">总结</h1><h2 id="softmax多分类模型的训练流程">softmax多分类模型的训练流程</h2><p>给定训练集X，训练集X共分为K类:</p><ol type="1"><li><p>随机初始化模型未知参数<span class="math inline">\(\theta\)</span>，本篇文章中为随机初始化权重向量<spanclass="math inline">\(w_k,b_k,k=1,2,...,K-1\)</span>。其中，<spanclass="math inline">\(w_k\)</span>为向量，<spanclass="math inline">\(b_k\)</span> 为标量。</p></li><li><p>梯度下降算法迭代更新模型参数直至收敛，每一轮具体流程如下：</p><ol type="1"><li>先通过正向传播求得本轮训练样本预测值。</li><li>反向传播更新模型参数：</li></ol><p><span class="math display">\[\begin{split}&amp; w_k = w_k - a\frac{\partial J}{\partial w_k} =w_k-a\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k)x,\quad k = 0,1,...,K-1 \\&amp; b_k = b_k -a\frac{\partial J}{\partial b_k} =b_k-a\frac{1}{N}\sum_{i=0}^{N-1}(a_k-y_k),\quad k = 0,1,...,K-1\end{split}\]</span></p><p>​注意：上述公式省去了上标<code>i</code>，上标<code>i</code>表示这是第<code>i</code>个样本。</p></li></ol><h2 id="softmax多分类模型的预测流程">softmax多分类模型的预测流程</h2><p>经过训练后，模型参数已经确定。现在只需要用这些参数对需要预测的数据跑一遍正向传播就行。</p><p>不过可能在求出预测值后，需要加一步判断：哪个类别的概率最大，就认为该样本属于哪一类。</p><blockquote><p>到此，整个softmax分类器说明完毕。</p></blockquote><h1id="softmax多分类器的具体代码实现和应用">softmax多分类器的具体代码实现和应用</h1><p>关于softmax的具体代码实现和应用将在下一篇文章介绍。在下一篇文章中，将会以经典的鸢尾花数据集为例，对其进行softmax多分类的手动代码实现。</p><p>下篇文章：<ahref="https://www.issey.top/article/602d6fb8bb9d/">机器学习笔记14——【Softmax多分类2】代码具体实现与应用（含数据集） | issey的博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【matlab图像处理笔记5】【图像变换】（四）图像的正交变换</title>
      <link href="/article/79566dd9cf96/"/>
      <url>/article/79566dd9cf96/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>本系列其他文章</strong></p><ol type="1"><li><ahref="https://blog.csdn.net/qq_52466006/article/details/127352334?spm=1001.2014.3001.5501">【matlab图像处理笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法_TwilightSparkle.的博客-CSDN博客</a></li><li><ahref="https://blog.csdn.net/qq_52466006/article/details/127355405?spm=1001.2014.3001.5501">【matlab图像处理笔记3】【图像变换】（二）图像的形态学变换_TwilightSparkle.的博客-CSDN博客</a></li><li><ahref="https://blog.csdn.net/qq_52466006/article/details/127399857?spm=1001.2014.3001.5501">【matlab图像处理笔记4】【图像变换】（三）图像的霍夫变换_TwilightSparkle.的博客-CSDN博客</a></li></ol><p><strong>相关文章</strong></p><ul><li><ahref="https://blog.csdn.net/qq_52466006/article/details/127290165?spm=1001.2014.3001.5501">【从FT到DFT和FFT】（一）从三角函数正交性到傅里叶变换的详细公式推导_TwilightSparkle.的博客-CSDN博客</a></li><li><ahref="https://blog.csdn.net/qq_52466006/article/details/127329471?spm=1001.2014.3001.5501">【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换_TwilightSparkle.的博客-CSDN博客</a></li><li><ahref="https://blog.csdn.net/qq_52466006/article/details/127337890?spm=1001.2014.3001.5501">【从FT到DFT和FFT】（三）从离散傅里叶变换到快速傅里叶变换_TwilightSparkle.的博客-CSDN博客</a></li></ul><p><strong>参考文章</strong></p><ol type="1"><li><ahref="https://blog.csdn.net/dazhuan0429/article/details/85774692">如何理解图像傅里叶变换的频谱图_双子的孤独的博客-CSDN博客_图像傅里叶变换频谱图</a></li></ol><h1 id="前言">前言</h1><p>本文是现阶段图像变换笔记的最后一篇，将介绍图像的正交变换。</p><h1 id="图像正交变换简介">图像正交变换简介</h1><p>正交变换是数字图像处理的一种有效工具。通过正交变换，我们可以将图像从时域变换到频域进行后续处理。正交变换在图像增强、图像复原、图像压缩、图像特征处理等方面都经常使用。</p><p>常用的正交变换有:离散傅里叶变换、离散余弦变换、K-L变换、小波变换等。本篇仅介绍傅里叶变换和离散余弦变换的相关内容，K-L变换和小波变换将会在后续文章补齐（希望吧）。</p><h1 id="离散傅里叶变换">离散傅里叶变换</h1><p>图像处理中的离散傅里叶变换其实指的是<strong>二维快速离散傅里叶变换</strong>。关于傅里叶原理与推导在之前的文章已经详细写过，详情请见"相关文章"。这里默认已经对傅里叶变换/二维离散傅里叶变换有一定了解。</p><h2id="对图像进行离散傅里叶变换的作用">对图像进行离散傅里叶变换的作用</h2><p>可以将空间域（二维灰度数表）的图像转换到频域（频率数表），使得可以更方便的处理图像，也更有利于进行频域滤波等操作。</p><h2 id="二维离散傅里叶变换">二维离散傅里叶变换</h2><p><strong>公式：</strong> <span class="math display">\[\begin{split}&amp; F(u,v) =\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)e^{-i2\pi(\frac{ux}{M}+\frac{vy}{N})}\\&amp; f(u,v) =\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}F(x,y)e^{i2\pi(\frac{ux}{M}+\frac{vy}{N})}\end{split}\]</span>不过实际上要使用二维离散傅里叶变换，是使用的对其进行<strong>分治</strong>的基础上得到的<strong>快速二维傅里叶变换</strong>。</p><h2 id="频谱图">频谱图</h2><p>学习图像离散傅里叶变换，一个不得不提的就是对频谱图的理解。光有公式是不够的，还得看懂变换后的频谱图大概代表什么意思。频谱图的内容较多，我在学习时找到了很详细的文章。请见参考教程【1】。推荐先去看频谱图，这里虽然占的篇幅很少，实际上是因为自己太懒了不想重述了。</p><p><code>二维频谱图中每一点都是一个与之一一对应的二维正弦/余弦波。</code></p><h2 id="示例">示例</h2><p>matlab中使用fft2进行二维（快速）离散傅里叶变换。</p><p>使用文档：<ahref="https://ww2.mathworks.cn/help/matlab/ref/fft2.html?searchHighlight=fft2&amp;s_tid=srchtitle_fft2_1">二维快速傅里叶变换- MATLAB fft2 - MathWorks 中国</a></p><p><img src="https://img.issey.top/img/202210221749449.png" /></p><p>将时域图转化为频域图:</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all; </span><br><span class="line">load imdemos saturn2;</span><br><span class="line"><span class="built_in">figure</span>,imshow(saturn2);</span><br><span class="line">title(<span class="string">&#x27;原图&#x27;</span>);</span><br><span class="line">X = fft2(saturn2);</span><br><span class="line"><span class="built_in">figure</span>,imshow(<span class="built_in">log</span>(<span class="built_in">abs</span>(X)),[]);</span><br><span class="line">title(<span class="string">&#x27;2d-fft&#x27;</span>);</span><br><span class="line">colormap(jet(<span class="number">64</span>)),colorbar;</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210221753338.png" /></p><p>在频域去阈值后再逆变换</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all; </span><br><span class="line">load imdemos saturn2;</span><br><span class="line"><span class="built_in">figure</span>,imshow(saturn2);</span><br><span class="line">title(<span class="string">&#x27;原图&#x27;</span>);</span><br><span class="line">X = fft2(saturn2);</span><br><span class="line"><span class="built_in">figure</span>,imshow(<span class="built_in">log</span>(<span class="built_in">abs</span>(X)),[]);</span><br><span class="line">title(<span class="string">&#x27;2d-fft&#x27;</span>);</span><br><span class="line">colormap(jet(<span class="number">64</span>)),colorbar;</span><br><span class="line">X(<span class="built_in">abs</span>(X)&lt;<span class="number">10</span>)=<span class="number">0</span>;</span><br><span class="line">Y = <span class="built_in">real</span>(ifft2(X)/<span class="number">255</span>);</span><br><span class="line"><span class="built_in">figure</span>(),imshow(Y,[]);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210221804187.png" /></p><h1 id="离散余弦变换">离散余弦变换</h1><h2 id="简介">简介</h2><p>离散余弦变换(Discrete Cosine Transform,DCT)是以一组不同频率和幅值的余弦函数和来近似一幅图像，实际上是<strong>傅立叶变换的实数部分</strong>。离散余弦变换有一个重要的性质，即对于一幅图像，其大部分可视化信息都集中在少数的变换系数上。因此，离散余弦变换经常用于图像压缩，例如国际压缩标准的JPEG格式中就采用了离散余弦变换。</p><h2 id="基本原理">基本原理</h2><p>在傅立叶变换过程中，若被展开的函数是实偶函数，则其傅立叶变换中只包含余弦项，基于傅立叶变换的这一特点，人们提出了离散余弦变换。<strong>DCT变换先将图像函数变换成偶函数形式，再对其进行二维离散傅立叶变换，故DCT变换可以看成是一种简化的傅立叶变换</strong>。</p><p><strong>二维离散余弦公式：</strong></p><p>对于<span class="math inline">\((M,N)\)</span> 的序列矩阵<spanclass="math inline">\(f(x,y)\)</span>，二维离散余弦变换定义如下：</p><p>DCT: <span class="math display">\[F(u,v) =a(u)a(v)\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)cos\frac{(2x+1)u\pi}{2M}cos\frac{(2y+1)u\pi}{2N}\]</span> 其中,<spanclass="math inline">\(u=0,1,...,M-1,v=0,1,...,N-1\)</span> <spanclass="math display">\[a(u) =\left\{\begin{aligned}\frac{1}{\sqrt M},u=0\\\sqrt \frac{2}{M},u\neq0\\\end{aligned}\right.,a(v) =\left\{\begin{aligned}\frac{1}{\sqrt N},v=0\\\sqrt \frac{2}{N},v\neq0\\\end{aligned}\right.\]</span> IDCT: <span class="math display">\[f(x,y) =\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}a(u)a(v)F(u,v)cos\frac{(2x+1)u\pi}{2M}cos\frac{(2y+1)u\pi}{2N}\]</span> 其中,<spanclass="math inline">\(x=0,1,...,M-1,y=0,1,...,N-1\)</span> <spanclass="math display">\[a(u) =\left\{\begin{aligned}\frac{1}{\sqrt M},u=0\\\sqrt \frac{2}{M},u\neq0\\\end{aligned}\right.,a(v) =\left\{\begin{aligned}\frac{1}{\sqrt N},v=0\\\sqrt \frac{2}{N},v\neq0\\\end{aligned}\right.\]</span></p><h2 id="示例-1">示例</h2><p>matlab中使用dct2进行二维离散余弦变换。</p><p>参考文档：<ahref="https://ww2.mathworks.cn/help/images/ref/dct2.html?s_tid=doc_ta">二维离散余弦变换- MATLAB dct2 - MathWorks 中国</a></p><p><img src="https://img.issey.top/img/202210221955920.png" /></p><p>正变换：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all; </span><br><span class="line">load imdemos saturn2;</span><br><span class="line"><span class="built_in">figure</span>,imshow(saturn2);</span><br><span class="line">title(<span class="string">&#x27;原图&#x27;</span>);</span><br><span class="line">X = dct2(saturn2);</span><br><span class="line"><span class="built_in">figure</span>,imshow(<span class="built_in">log</span>(<span class="built_in">abs</span>(X)),[]);</span><br><span class="line">title(<span class="string">&#x27;2d-dct&#x27;</span>);</span><br><span class="line">colormap(jet(<span class="number">64</span>)),colorbar;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210222001885.png" /></p><p>在频域设置阈值并逆变换：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all; </span><br><span class="line">load imdemos saturn2;</span><br><span class="line"><span class="built_in">figure</span>,imshow(saturn2);</span><br><span class="line">title(<span class="string">&#x27;原图&#x27;</span>);</span><br><span class="line">X = dct2(saturn2);</span><br><span class="line"><span class="built_in">figure</span>,imshow(<span class="built_in">log</span>(<span class="built_in">abs</span>(X)),[]);</span><br><span class="line">title(<span class="string">&#x27;2d-dct&#x27;</span>);</span><br><span class="line">colormap(jet(<span class="number">64</span>)),colorbar;</span><br><span class="line">X(<span class="built_in">abs</span>(X)&lt;<span class="number">10</span>)=<span class="number">0</span>;</span><br><span class="line">Y = idct2(X)/<span class="number">255</span>;</span><br><span class="line"><span class="built_in">figure</span>(),imshow(Y);</span><br><span class="line">title(<span class="string">&#x27;2d-idct&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210222003662.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【matlab图像处理笔记4】【图像变换】（三）图像的霍夫变换</title>
      <link href="/article/fdf5a5acea45/"/>
      <url>/article/fdf5a5acea45/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>本系列其他文章</strong></p><ol type="1"><li><ahref="https://www.issey.top/article/34b6e799e503/">【matlab图像处理笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法| issey的博客</a></li><li><ahref="https://www.issey.top/article/2358738631df/">【matlab图像处理笔记3】【图像变换】（二）图像的形态学变换| issey的博客</a></li></ol><p><strong>参考教程/推荐文章</strong></p><ol type="1"><li><a href="https://www.cnblogs.com/php-rearch/p/6760683.html">霍夫变换- 疯狂奔跑 - 博客园 (cnblogs.com)</a></li><li><ahref="https://blog.csdn.net/poem_qianmo/article/details/26977557">【OpenCV入门教程之十四】OpenCV霍夫变换：霍夫线变换，霍夫圆变换合辑_浅墨_毛星云的博客-CSDN博客</a></li><li><ahref="https://blog.csdn.net/weixin_57440207/article/details/122647052?ops_request_misc=%7B%22request%5Fid%22%3A%22166609911516782390589403%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=166609911516782390589403&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-122647052-null-null.142%5Ev59%5Econtrol,201%5Ev3%5Econtrol_2&amp;utm_term=霍夫变换&amp;spm=1018.2226.3001.4187">第16章：霍夫变换_李淳罡Lichungang的博客-CSDN博客_霍夫变换</a></li><li><ahref="https://blog.csdn.net/didi_ya/article/details/111149724?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=霍夫变换matlab&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-111149724.142%5Ev59%5Econtrol,201%5Ev3%5Econtrol_2&amp;spm=1018.2226.3001.4187">霍夫变换（HoughTransformation）基本思想及MATLAB相关函数_wendy_ya的博客-CSDN博客_hough变换的基本思想</a></li></ol><h1 id="前言">前言</h1><p>本篇将介绍图像变换中的霍夫变换，该文章不会对霍夫变换做太过于详细的推导，将更注重于霍夫变换的理解与应用。</p><p>本篇文章主要介绍<strong>霍夫变换直线检测</strong>。</p><h1 id="霍夫变换概述">霍夫变换概述</h1><p>霍夫变换是一种在图像中寻找直线、圆形以及其他简单形状的方法，广义上的霍夫变换可以找到你想要的任何你可以描述的特征。</p><p>霍夫变换采用类似于投票的方式来获取当前图像内的形状集合，该变换由PaulHough(霍夫)于1962年首次提出。最初的霍夫变换只能用于检测直线，经过发展后，霍夫变换不仅能够识别直线，还能识别其他简单的图形结构，常见的有圆、椭圆等。实际上，只要是能够用一个参数方程表示的对象，都适合用霍夫变换来检测。</p><h1 id="霍夫变换直线检测原理">霍夫变换直线检测原理</h1><h2 id="从笛卡尔坐标系到霍夫空间">从笛卡尔坐标系到霍夫空间</h2><p><strong>直线变为点</strong></p><p>在笛卡尔坐标系中，存在一条直线<spanclass="math inline">\(y=k_0x+b_0\)</span></p><p><img src="https://img.issey.top/img/202210182122181.png" /></p><p>将<span class="math inline">\(y=k_0x+b_0\)</span>写为关于<spanclass="math inline">\((k,b)\)</span>的函数： <spanclass="math display">\[b_0 = -k_0x+y\]</span>变换后的空间称为霍夫空间。此时笛卡尔坐标系中的直线将表示为霍夫空间中的一个点：</p><p><img src="https://img.issey.top/img/202210182123065.png" /></p><p>同时也可以从霍夫空间逆变换为笛卡尔空间。</p><hr /><p><strong>点变为直线</strong></p><p>在笛卡尔坐标系中，存在一个点<spanclass="math inline">\((x_0,y_0)\)</span>，</p><p><img src="https://img.issey.top/img/202210182126421.png" /></p><p>通过该点的直线可表示为：<span class="math inline">\(y_0 =kx_0+b\)</span> 。变换后为<span class="math inline">\(b =-kx_0+y_0\)</span>。此时笛卡尔坐标系中的一个点将表示为霍夫空间中的一条直线：</p><p><img src="https://img.issey.top/img/202210182127518.png" /></p><h2 id="两点一线的霍夫空间形式">两点一线的霍夫空间形式</h2><p>现在在笛卡尔坐标系中，有两个点：</p><p><img src="https://img.issey.top/img/202210182152082.png" /></p><p>我们知道，两点确定一条直线。现在来思考这两个点在霍夫空间将以什么形式表示这条直线。在霍夫空间中，这两个点对应不同的直线。那么在霍夫空间中，这两条直线的交点，就是笛卡尔坐标系中对应的直线。</p><p><img src="https://img.issey.top/img/202210182157189.png" /></p><h2 id="寻找共线的点">寻找共线的点</h2><p>现在我们将点位增多，并开始在这些点中寻找共线的点以及对应的直线。</p><p><img src="https://img.issey.top/img/202210182201190.png" /></p><p>那么根据霍夫空间交点即笛卡尔空间共线的规则，右图的交点都说明共线。但是右图的交点为什么无视了两个呢？</p><p><img src="https://img.issey.top/img/202210182207513.png" /></p><p>这是因为<strong>霍夫变换后处理的基本方式是：选择由尽可能多直线汇成的点。<code>通常情况下，我们需要设置一个阈值，当霍夫坐标系内交于某点的曲线达到了阈值，就认为在对应的极坐标系内存在(检测到)一条直线。</code></strong></p><p>现在逆变换回笛卡尔坐标系，看看这两个橙色的交点代表的直线：</p><p><img src="https://img.issey.top/img/202210182209960.png" /></p><p>可以看到我们成功找到了共线的点对应的直线，但此时的霍夫变换还存在问题。</p><h2 id="直角坐标系存在的问题">直角坐标系存在的问题</h2><p>考虑下图的情况，即共线的直线垂直于x轴，此时直线的斜率k为无穷大，截距b无法取值。因此，下图的垂线无法映射至霍夫空间。为了解决该问题，我们需要将直角坐标系换位极坐标系。</p><p><img src="https://img.issey.top/img/202210182212867.png" /></p><h2 id="极坐标参数空间下的霍夫变换">极坐标参数空间下的霍夫变换</h2><p><img src="https://img.issey.top/img/202210182217423.png" /></p><p>在极坐标中的直线可以表示为： <span class="math display">\[r = xcos\theta+ysin\theta\]</span> 现在将极坐标的点映射至霍夫空间，霍夫空间的参数变为<spanclass="math inline">\(r,\theta\)</span> ，对比图如下:</p><p><img src="https://img.issey.top/img/202210182219995.png" /></p><p>同样的规则，交点为共线。</p><h1 id="matlab霍夫变换直线检测示例">matlab霍夫变换直线检测示例</h1><p>现在使用matlab实现的霍夫变换做一个具体的示例。</p><h2 id="检测步骤">检测步骤</h2><ol type="1"><li><p>对原图像进行<strong>边缘检测</strong>同时二值化。<code>（关于边缘检测的相关内容将在之后的文章中更新）</code></p><p><strong>二值化以后，我们就可以通过找非零点的坐标确定数据点的位置。即将像素图像变成笛卡尔坐标系的坐标集合。</strong></p></li><li><p>对二值化后的图像进行霍夫变换。</p></li><li><p>在霍夫空间中寻找满足条件的交点。</p></li><li><p>在笛卡尔坐标系，将霍夫变换中找到的交点变成直线，<strong>再以线段的形式绘制出来</strong>。<code>这一步待会儿会特别说明一下</code></p></li></ol><h2 id="示例以及代码">示例以及代码</h2><h3 id="原图">原图</h3><p><img src="https://img.issey.top/img/202210182255641.jpg" /></p><h3 id="边缘检测">边缘检测</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&#x27;example3.jpg&#x27;</span>);</span><br><span class="line">I = rgb2gray(I); <span class="comment">% 灰度化</span></span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&#x27;原图&#x27;</span>);</span><br><span class="line">BW = edge(I,<span class="string">&#x27;prewitt&#x27;</span>); <span class="comment">% 边缘检测</span></span><br><span class="line"><span class="built_in">figure</span>();imshow(BW);title(<span class="string">&#x27;边缘检测&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210182254657.png" /></p><h3 id="对二值图像霍夫变换">对二值图像霍夫变换</h3><p>matlab中使用hough函数对二值图像进行霍夫变换。</p><p>函数用法：<ahref="https://ww2.mathworks.cn/help/images/ref/hough.html?searchHighlight=hough&amp;s_tid=srchtitle_hough_1">Hough变换 - MATLAB hough - MathWorks 中国</a></p><p><img src="https://img.issey.top/img/202210182300155.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 霍夫变换</span></span><br><span class="line">[H,theta,rho] = hough(BW);</span><br><span class="line"><span class="built_in">figure</span>();imshow(imadjust(mat2gray(H)),[],<span class="string">&#x27;XData&#x27;</span>,theta,<span class="string">&#x27;YData&#x27;</span>,rho,...</span><br><span class="line">        <span class="string">&#x27;InitialMagnification&#x27;</span>,<span class="string">&#x27;fit&#x27;</span>);</span><br><span class="line">xlabel(<span class="string">&#x27;\theta&#x27;</span>);ylabel(<span class="string">&#x27;\rho&#x27;</span>);</span><br><span class="line">axis on, axis normal, <span class="built_in">hold</span> on; <span class="comment">% 调整图像比例，不然会很窄</span></span><br><span class="line">title(<span class="string">&#x27;霍夫空间映射图像&#x27;</span>);</span><br></pre></td></tr></table></figure><p>边缘检测后的二值图像在霍夫空间上的映射图像：</p><p><img src="https://img.issey.top/img/202210182358436.png" /></p><h3 id="寻找霍夫空间中的交点">寻找霍夫空间中的交点</h3><p>matlab中使用houghpeaks函数在霍夫空间寻找满足条件的交点。</p><p>函数用法:<ahref="https://ww2.mathworks.cn/help/images/ref/houghpeaks.html?s_tid=doc_ta">Identifypeaks in Hough transform - MATLAB houghpeaks - MathWorks 中国</a></p><p><img src="https://img.issey.top/img/202210190012810.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 在Hough矩阵中寻找前30个大于Hough矩阵中最大值0.3的交点（交点即峰值）</span></span><br><span class="line">P = houghpeaks(H,<span class="number">30</span>,<span class="string">&#x27;threshold&#x27;</span>,<span class="built_in">ceil</span>(<span class="number">0.3</span>*<span class="built_in">max</span>(H(:))));</span><br><span class="line">x = theta(P(:,<span class="number">2</span>));y = rho(P(:,<span class="number">1</span>));</span><br><span class="line"><span class="built_in">plot</span>(x,y,<span class="string">&#x27;s&#x27;</span>,<span class="string">&#x27;color&#x27;</span>,<span class="string">&#x27;red&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210190042920.png" /></p><h3 id="在笛卡尔坐标系绘制线段">在笛卡尔坐标系绘制线段</h3><p>matlab中使用houghlines函数将在霍夫空间寻找到的交点提取成笛卡尔坐标系的<strong>线段</strong>（注意不是直线！）</p><p>函数用法：<ahref="https://ww2.mathworks.cn/help/images/ref/houghlines.html?s_tid=doc_ta">基于Hough 变换提取线段 - MATLAB houghlines - MathWorks 中国</a></p><p><img src="https://img.issey.top/img/202210190104135.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 在笛卡尔坐标系中找到这些直线</span></span><br><span class="line"><span class="comment">% 合并距离小于20的线段，丢弃所有长度小于2的线段</span></span><br><span class="line">lines=houghlines(BW,theta,rho,P,<span class="string">&#x27;FillGap&#x27;</span>,<span class="number">20</span>,<span class="string">&#x27;Minlength&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(BW);<span class="built_in">hold</span> on;</span><br><span class="line"></span><br><span class="line">max_len = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:<span class="built_in">length</span>(lines) <span class="comment">% 依次标出各条直线段</span></span><br><span class="line">   xy = [lines(k).point1; lines(k).point2];</span><br><span class="line">   <span class="built_in">plot</span>(xy(:,<span class="number">1</span>),xy(:,<span class="number">2</span>),<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>,<span class="string">&#x27;Color&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>);</span><br><span class="line"> </span><br><span class="line">   <span class="comment">% 绘制线段的起点和终点</span></span><br><span class="line">   <span class="built_in">plot</span>(xy(<span class="number">1</span>,<span class="number">1</span>),xy(<span class="number">1</span>,<span class="number">2</span>),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>,<span class="string">&#x27;Color&#x27;</span>,<span class="string">&#x27;yellow&#x27;</span>);</span><br><span class="line">   <span class="built_in">plot</span>(xy(<span class="number">2</span>,<span class="number">1</span>),xy(<span class="number">2</span>,<span class="number">2</span>),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>,<span class="string">&#x27;Color&#x27;</span>,<span class="string">&#x27;red&#x27;</span>);</span><br><span class="line"> </span><br><span class="line">   <span class="comment">% 确定最长线段的端点</span></span><br><span class="line">   len = norm(lines(k).point1 - lines(k).point2);<span class="comment">% 最长线段的长度</span></span><br><span class="line">   <span class="keyword">if</span> ( len &gt; max_len)</span><br><span class="line">      max_len = len;</span><br><span class="line">      xy_long = xy;</span><br><span class="line">   <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210190106435.png" /></p><h4 id="关于houghlines的补充说明">关于houghlines的补充说明</h4><p>我最开始没搞清啊，就觉得很奇怪：找到的线明显比交点多啊，这是为什么？</p><p>然后仔细研究才发现它是把直线拆成了很多根线段，现在我们只找一个交点就很容易看清楚了：</p><p><img src="https://img.issey.top/img/202210190113158.png" /></p><p><img src="https://img.issey.top/img/202210190113388.png" /></p><p>可能图有点小，但是很明显可以看到它把一条直线拆成了两个线段。</p><h2 id="完整代码">完整代码</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&#x27;example3.jpg&#x27;</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&#x27;原图&#x27;</span>);</span><br><span class="line">I = rgb2gray(I); <span class="comment">% 灰度化</span></span><br><span class="line">BW = edge(I,<span class="string">&#x27;Prewitt&#x27;</span>); <span class="comment">% 边缘检测</span></span><br><span class="line"><span class="built_in">figure</span>();imshow(BW);title(<span class="string">&#x27;边缘检测&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 霍夫变换</span></span><br><span class="line">[H,theta,rho] = hough(BW);</span><br><span class="line"><span class="built_in">figure</span>();imshow(imadjust(mat2gray(H)),[],<span class="string">&#x27;XData&#x27;</span>,theta,<span class="string">&#x27;YData&#x27;</span>,rho,...</span><br><span class="line">        <span class="string">&#x27;InitialMagnification&#x27;</span>,<span class="string">&#x27;fit&#x27;</span>);</span><br><span class="line">xlabel(<span class="string">&#x27;\theta&#x27;</span>);ylabel(<span class="string">&#x27;\rho&#x27;</span>);</span><br><span class="line">axis on, axis normal, <span class="built_in">hold</span> on; <span class="comment">% 调整图像比例，不然会很窄</span></span><br><span class="line">title(<span class="string">&#x27;霍夫空间映射图像&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 在Hough矩阵中寻找前30个大于Hough矩阵中最大值0.5峰值</span></span><br><span class="line">P = houghpeaks(H,<span class="number">30</span>,<span class="string">&#x27;threshold&#x27;</span>,<span class="built_in">ceil</span>(<span class="number">0.3</span>*<span class="built_in">max</span>(H(:))));</span><br><span class="line">x = theta(P(:,<span class="number">2</span>));y = rho(P(:,<span class="number">1</span>));</span><br><span class="line"><span class="built_in">plot</span>(x,y,<span class="string">&#x27;s&#x27;</span>,<span class="string">&#x27;color&#x27;</span>,<span class="string">&#x27;red&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 在笛卡尔坐标系中找到这些直线</span></span><br><span class="line"><span class="comment">% 合并距离小于20的线段，丢弃所有长度小于2的线段</span></span><br><span class="line">lines=houghlines(BW,theta,rho,P,<span class="string">&#x27;FillGap&#x27;</span>,<span class="number">20</span>,<span class="string">&#x27;Minlength&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(BW);<span class="built_in">hold</span> on;</span><br><span class="line"></span><br><span class="line">max_len = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:<span class="built_in">length</span>(lines) <span class="comment">% 依次标出各条直线段</span></span><br><span class="line">   xy = [lines(k).point1; lines(k).point2];</span><br><span class="line">   <span class="built_in">plot</span>(xy(:,<span class="number">1</span>),xy(:,<span class="number">2</span>),<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>,<span class="string">&#x27;Color&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>);</span><br><span class="line"> </span><br><span class="line">   <span class="comment">% 绘制线段的起点和终点</span></span><br><span class="line">   <span class="built_in">plot</span>(xy(<span class="number">1</span>,<span class="number">1</span>),xy(<span class="number">1</span>,<span class="number">2</span>),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>,<span class="string">&#x27;Color&#x27;</span>,<span class="string">&#x27;yellow&#x27;</span>);</span><br><span class="line">   <span class="built_in">plot</span>(xy(<span class="number">2</span>,<span class="number">1</span>),xy(<span class="number">2</span>,<span class="number">2</span>),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>,<span class="string">&#x27;Color&#x27;</span>,<span class="string">&#x27;red&#x27;</span>);</span><br><span class="line"> </span><br><span class="line">   <span class="comment">% 确定最长线段的端点</span></span><br><span class="line">   len = norm(lines(k).point1 - lines(k).point2);<span class="comment">% 最长线段的长度</span></span><br><span class="line">   <span class="keyword">if</span> ( len &gt; max_len)</span><br><span class="line">      max_len = len;</span><br><span class="line">      xy_long = xy;</span><br><span class="line">   <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>来个成果集合图：</p><p><img src="https://img.issey.top/img/202210190125295.png" /></p><hr /><p>到这里霍夫变换直线检测就写完了。但是霍夫变换的应用明显不止这么点，我原本想把圆环检测也写了，奈何时间确实不够，感兴趣的话就只能自行拓展啦。</p><p>写完本文已经是晚上1点了，明早还有早课，准备早课补眠喽~</p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【matlab图像处理笔记3】【图像变换】（二）图像的形态学变换</title>
      <link href="/article/2358738631df/"/>
      <url>/article/2358738631df/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>本系列其他文章</strong></p><ol type="1"><li><ahref="https://www.issey.top/article/34b6e799e503/">【matlab图像处理笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法| issey的博客</a></li></ol><p><strong>参考教程</strong></p><ul><li><ahref="https://blog.csdn.net/qq_40732350/article/details/116718329?ops_request_misc=%7B%22request%5Fid%22%3A%22166592310316782395377737%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=166592310316782395377737&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-116718329-null-null.142%5Ev56%5Econtrol,201%5Ev3%5Econtrol_2&amp;utm_term=图像形态学处理&amp;spm=1018.2226.3001.4187">图像处理(5): 形态学处理_QtHalcon的博客-CSDN博客_形态学处理</a></li><li><ahref="https://zhuanlan.zhihu.com/p/374643626">图像算法原理与实践——图像形态学处理- 知乎 (zhihu.com)</a></li></ul><h1 id="前言">前言</h1><p>本篇文章将介绍图像的形态学变换。</p><h1 id="形态学变换">形态学变换</h1><h2 id="简介">简介</h2><p>形态学是图像处理中应用最为广泛的技术之一，主要用于从图像中提取对表达和描绘区域形状有意义的图像分量，使后续的识别工作能够抓住目标对象最为本质的形状特征，如边界和连通区域等。同时像细化、像素化和修剪毛刺等技术也常应用于图像的预处理和后处理中，成为图像增强技术的有力补充。</p><p>常见图像形态学运算：<strong>腐蚀、膨胀、开运算、闭运算</strong>、骨架抽取、极线腐蚀、击中击不中变换、Top-hat变换、颗粒分析、流域变换、形态学梯度等。</p><p>形态学的基本思想是利用一种特殊的<strong>结构元</strong>来测量或提取输入图像中相应的形状或特征，以便进一步进行图像分析和目标识别。</p><blockquote><p>本文的形态学内容仅讨论腐蚀、膨胀、开运算、闭运算。</p></blockquote><h2 id="数学基础">数学基础</h2><p>形态学的数学基础和所用语言是<strong>集合论</strong>。</p><h3 id="结构元素">结构元素</h3><p>结构元素（StructuringElements，<strong>SE</strong>）是类似于“滤波核”的元素，或者说类似于一个“小窗”，在原图上进行“滑动”，在腐蚀与膨胀操作中都需要用到。</p><p>结构元素可以指定其形状和大小。<strong>结构元素一般由0和1的二值像素组成</strong>。结构元素的原点<strong>(锚点)</strong>相当于“小窗”的中心，其尺寸由具体的腐蚀或膨胀算子指定，结构元素的尺寸也决定着腐蚀或者膨胀的程度。结构元素越大,被腐蚀消失或者被膨胀增加的区域也会越大。</p><h3 id="在matlab中创建结构元">在matlab中创建结构元</h3><p><strong>基本语法</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SE = strel(shape, parameters)</span><br></pre></td></tr></table></figure><p>关于strel的相关参数说明请自行查阅官方文档：<ahref="https://ww2.mathworks.cn/help/releases/R2018b/images/ref/strel.html">Morphologicalstructuring element - MATLAB - MathWorks 中国</a></p><p><strong>示例</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">SE = strel(<span class="string">&#x27;diamond&#x27;</span>,<span class="number">100</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(SE.Neighborhood);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210162156595.png" /></p><h2 id="腐蚀与膨胀">腐蚀与膨胀</h2><p>腐蚀和膨胀是两种<strong>最基本也是最重要的形态学运算</strong>，它们是很多高级形态学处理的基础，很多其他的形态学算法都是由这两种基本运算复合而成。</p><p>注：以下图示部分来自于<ahref="https://zhuanlan.zhihu.com/p/374643626">图像算法原理与实践——图像形态学处理- 知乎 (zhihu.com)</a></p><h3 id="腐蚀">腐蚀</h3><p>将<strong>结构元</strong>s在图像f上滑动，把结构元<strong>锚点位置对应图像像素点位置，</strong>目标像素点的灰度值设置为<strong>结构元值为1的区域</strong>对应图像区域像素的<strong>最小值</strong>。腐蚀可用于<strong>消除边缘和杂点</strong>。</p><p>腐蚀运算用公式符号记为：<span class="math inline">\(f\ominuss\)</span></p><p>简单来说就是求<strong>局部最小值</strong>。直观视觉就是图像中物体尺寸会瘦小，看起来好像被腐蚀了。</p><p><img src="https://img.issey.top/img/202210162210028.webp" /></p><center>采用十字链结构元 腐蚀处理后的图像效果</center><p><img src="https://img.issey.top/img/202210162213612.webp" /></p><center>采用十字链结构元 腐蚀处理后的图像数据</center><p><strong>示例</strong></p><p><img src="https://img.issey.top/img/202210162250875.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line">I = rgb2gray(I);</span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">SE = strel(<span class="string">&#x27;diamond&#x27;</span>,<span class="number">10</span>);</span><br><span class="line">X = imerode(I,SE);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X);title(<span class="string">&quot;imerode&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210162254467.png" /></p><p>图像也可以是彩色。</p><h3 id="膨胀">膨胀</h3><p>将<strong>结构元</strong>s在图像f上滑动，把结构元<strong>锚点位置对应图像像素点位置，</strong>目标像素点的灰度值设置为<strong>结构元值为1的区域</strong>对应图像区域像素的<strong>最大值</strong>。经过膨胀操作，图像区域的边缘可能会变得平滑，区域的像素将会增加，不相连的部分可能会连接起来。</p><p>膨胀运算用公式符号记为:<span class="math inline">\(f\opluss\)</span></p><p><img src="https://img.issey.top/img/202210162300105.png" /></p><center>采用十字链结构元 膨胀处理后的图像效果</center><p><img src="https://img.issey.top/img/202210162301156.webp" /></p><center>采用十字链结构元 膨胀处理后的图像数据</center><p><strong>示例</strong></p><p><img src="https://img.issey.top/img/202210162310451.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line">I = rgb2gray(I);</span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">SE = strel(<span class="string">&#x27;diamond&#x27;</span>,<span class="number">10</span>);</span><br><span class="line">X = imdilate(I,SE);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X);title(<span class="string">&quot;imdilate&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210162311275.png" /></p><h2 id="开运算与并运算">开运算与并运算</h2><h3 id="开运算">开运算</h3><p>对图像 f 用同一结构元 s <strong>先腐蚀再膨胀</strong>称之为开运算。记为：<span class="math inline">\((f\ominuss)\oplus s\)</span></p><p><strong>基本功能</strong></p><p>消除小物体；在纤细处分离物体；平滑较大的边界并不明显改变其面积。从视觉上看仿佛将原本连接的物体“分开”了一样。</p><p><img src="https://img.issey.top/img/202210162319441.webp" /></p><center>采用十字链结构元 开运算处理后的图像效果</center><p><strong>示例</strong></p><p><img src="https://img.issey.top/img/202210162322088.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line"><span class="comment">%I = rgb2gray(I);</span></span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">SE = strel(<span class="string">&#x27;diamond&#x27;</span>,<span class="number">10</span>);</span><br><span class="line">X = imopen(I,SE);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X);title(<span class="string">&quot;imopen&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210162323667.png" /></p><h3 id="并运算">并运算</h3><p>图像 f 用同一结构元 s <strong>先膨胀 再腐蚀</strong>称之为闭运算。记为：<span class="math inline">\((f\oplus s)\ominuss\)</span></p><p><strong>基本功能</strong></p><p>排除小型黑洞，从视觉上看仿佛将原本分开的部分“闭合”了一样。</p><p><img src="https://img.issey.top/img/202210162327869.jpeg" /></p><center>采用十字链结构元 闭运算处理后的图像效果</center><p><strong>示例</strong></p><p><img src="https://img.issey.top/img/202210162329619.png" /></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line"><span class="comment">%I = rgb2gray(I);</span></span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">SE = strel(<span class="string">&#x27;diamond&#x27;</span>,<span class="number">10</span>);</span><br><span class="line">X = imclose(I,SE);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X);title(<span class="string">&quot;imclose&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210162330557.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【matlab图像处理笔记2】【图像变换】（一）图像的算术运算与几何变换、图像插值算法</title>
      <link href="/article/34b6e799e503/"/>
      <url>/article/34b6e799e503/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>本篇文章将介绍和应用图像的算术运算和几何变换,并对插值算法进行说明。比较简单的我就不举例了。</p><h1 id="图像的算术运算">图像的算术运算</h1><h2 id="图像相加">图像相加</h2><p>图像的加法运算是将一幅图像的内容叠加在另一幅图像上，或者给图像的每一个像素加一个常数来改变图像的亮度。主要应用是改变图像亮度和图像叠加。</p><p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = imadd(X,Y)</span><br></pre></td></tr></table></figure><h2 id="图像差分">图像差分</h2><p>主要作用是检测图像的变化和检测运动物体。</p><p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = imsubtract(X,Y)</span><br></pre></td></tr></table></figure><h2 id="图像乘法">图像乘法</h2><p>主要作用是掩膜和缩放。</p><p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = immultiply(X,Y)</span><br></pre></td></tr></table></figure><p>什么是掩膜：</p><p><img src="https://img.issey.top/img/202210152019297.png" /></p><p>可以对图像上一些区域起屏蔽作用。</p><h2 id="图像除法">图像除法</h2><p>主要作用是矫正成像设备的非线性影响。</p><p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = imdivide(X,Y)</span><br></pre></td></tr></table></figure><p><span class="math inline">\(Z = \frac{X}{Y}\)</span></p><h2 id="图像的线性组合">图像的线性组合</h2><p>函数</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z = imlincomb(A,X,B,Y,C) <span class="comment">% Z = A*X+B*Y+C</span></span><br><span class="line">Z = imlincomb(A,X,C)  <span class="comment">% Z = A*X+C</span></span><br><span class="line">Z = imlincomb(A,X,B,Y) <span class="comment">% Z = A*X+B*Y</span></span><br></pre></td></tr></table></figure><h1 id="图像的几何变换">图像的几何变换</h1><h2 id="图像平移">图像平移</h2><p>假设图中一点为<span class="math inline">\(f(x_0,y_0)\)</span>,对其水平平移tx个单位，垂直平移ty个单位，那么用矩阵表示应该为： <spanclass="math display">\[\begin{bmatrix}    x_1\\    y_1 \\    1\end{bmatrix} =\begin{bmatrix}    1&amp;0&amp;tx\\    0&amp;1&amp;ty \\    0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}    x_0\\    y_0 \\    1\end{bmatrix}\]</span>matlab中并没有提供平移图像的函数，但是可以通过<strong>膨胀函数</strong>平移图像。</p><p><strong>应用示例</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);title(<span class="string">&quot;original 1&quot;</span>);</span><br><span class="line"></span><br><span class="line">se = translate(strel(<span class="number">1</span>),[<span class="number">50</span>,<span class="number">100</span>]); <span class="comment">% 将一个平面结构元素向下移动50，向右边移动100</span></span><br><span class="line">X = imdilate(I,se); <span class="comment">%利用膨胀平移图像</span></span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X);title(<span class="string">&quot;now img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210152106779.png" /></p><h2 id="图片镜像">图片镜像</h2><p>设图像矩阵为<span class="math inline">\((M,N)\)</span> 。</p><p>图像镜像分为垂直镜像和水平镜像。</p><p>垂直镜像： <span class="math display">\[\begin{split}&amp; x_1 = M-x_0 \\&amp; y_1 = y_0\end{split}\]</span> 水平镜像: <span class="math display">\[\begin{split}&amp; x_1 = x_0 \\&amp; y_1 = N-y_0\end{split}\]</span> <strong>应用示例</strong></p><p>函数</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = flip(I,dim = _)</span><br><span class="line"><span class="comment">% dim = 1 水平镜像(翻转列)</span></span><br><span class="line"><span class="comment">% dim = 2 垂直镜像（翻转行）</span></span><br><span class="line"><span class="comment">% dim = 3 翻转第三维，可能是颜色？</span></span><br></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">X_1 = flip(I,<span class="number">1</span>); <span class="comment">% 翻转行</span></span><br><span class="line">X_2 = flip(I,<span class="number">2</span>); <span class="comment">% 翻转列</span></span><br><span class="line">X_3 = flip(I,<span class="number">3</span>); <span class="comment">% 翻转第三维</span></span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X_1);title(<span class="string">&quot;水平镜像&quot;</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>),imshow(X_2);title(<span class="string">&quot;垂直镜像&quot;</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),imshow(X_3);title(<span class="string">&quot;?&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210152138643.png" /></p><h2 id="图片转置">图片转置</h2><p>设<spanclass="math inline">\((x_0,y_0)\)</span>是原图上的点，则转置为： <spanclass="math display">\[\begin{split}&amp; x_1 = y_0 \\&amp; y_1 = x_0\end{split}\]</span> 表示为矩阵形式： <span class="math display">\[\begin{bmatrix}x_1&amp;y_1&amp;1\end{bmatrix}=\begin{bmatrix}0&amp;1&amp;0\\1&amp;0&amp;0\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}y_0&amp;x_0&amp;1\end{bmatrix}= \begin{bmatrix}x_0&amp;y_0&amp;1\end{bmatrix}\]</span></p><blockquote><p>注意：转置后图像的高度和宽度也会发生变化。</p></blockquote><p><strong>示例</strong></p><p>matlab中需要构建转换矩阵，然后用imwarp变换图像。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">T=affine2d([<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>;<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]);<span class="comment">%构造空间变换结构T.这里为转置变换矩阵</span></span><br><span class="line">X=imwarp(I,T);  <span class="comment">% 根据位移场变换图像。</span></span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X);title(<span class="string">&quot;now&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210152156681.png" /></p><h2 id="图像旋转">图像旋转</h2><p>设点<span class="math inline">\((x_0,y_0)\)</span>经过旋转<spanclass="math inline">\(\theta\)</span> 角度后，坐标变为<spanclass="math inline">\((x_1,y_1)\)</span>。</p><p>旋转前： <span class="math display">\[\left\{\begin{aligned}x_0 = rcos\theta\\y_0 = rsin\theta\\\end{aligned}\right.\]</span> 旋转后： <span class="math display">\[\left\{\begin{aligned}x_1 = rcos(\alpha-\theta) = rcos\alpha cos\theta + rsin\alpha sin\theta= x_0cos\theta+y_0sin\theta\\y_1 = rsin(\alpha-\theta) = rsin\alpha cos\theta - rcos\alpha sin\theta= -x_0sin\theta+y_0cos\theta\\\end{aligned}\right.\]</span> 矩阵形式： <span class="math display">\[\begin{bmatrix}x_1\\y_1\\1\end{bmatrix}=\begin{bmatrix}cos\theta&amp;sin\theta&amp;0\\-sin\theta&amp;cos\theta&amp;0\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}x_0\\y_0\\1\end{bmatrix}\]</span> 对矩阵求逆可得逆变换： <span class="math display">\[\begin{bmatrix}x_0\\y_0\\1\end{bmatrix}=\begin{bmatrix}cos\theta&amp;-sin\theta&amp;0\\sin\theta&amp;cos\theta&amp;0\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}x_1\\y_1\\1\end{bmatrix}\]</span> <strong>示例</strong></p><p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = imrotate(A,<span class="built_in">angle</span>) </span><br><span class="line">X = imrotate(A,<span class="built_in">angle</span>,method)</span><br><span class="line">X = imrotate(A,<span class="built_in">angle</span>,method,bbox)</span><br></pre></td></tr></table></figure><ul><li>angle：角度，大于0顺时针旋转，小于0逆时针旋转</li><li>method：插值方法，method取值为：'nearest'(默认)最近邻插值、'bilinear'双线性插值、'bicubic'双三次插值（关于插值算法将在下面介绍）</li><li>bbox：返回图像的大小，取值为'crop'或'loose'。'crop'输出大小和输入图像大小想等，对旋转后的图像进行裁剪；'loose'(默认)表示使输出图像足够大，包含完整的旋转图像.</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">X_1 = imrotate(I,<span class="number">45</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X_1);title(<span class="string">&quot;angle 45&quot;</span>);</span><br><span class="line">X_2 = imrotate(I,<span class="number">45</span>,<span class="string">&#x27;bicubic&#x27;</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>),imshow(X_2);title(<span class="string">&quot;angle 45 bicubic&quot;</span>);</span><br><span class="line">X_3 = imrotate(I,<span class="number">45</span>,<span class="string">&#x27;bicubic&#x27;</span>,<span class="string">&#x27;crop&#x27;</span>);</span><br><span class="line">subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),imshow(X_3);title(<span class="string">&quot;angle 45 bicubic crop&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210161425587.png" /></p><h2 id="图像缩放">图像缩放</h2><p>将放大或者缩小后的图像将其的坐标（长宽）拉伸或者压缩到和原图一样大时，其像素坐标点对应在原图上的位置就是其映射位置。假设图像x轴方向缩放比为<spanclass="math inline">\(f_x\)</span>，y轴方向缩放比为<spanclass="math inline">\(f_y\)</span> ,则原图中的点<spanclass="math inline">\((x_0,y_0)\)</span>缩放后对应的新位置为： <spanclass="math display">\[\begin{bmatrix}x_1\\y_1\\1\end{bmatrix}=\begin{bmatrix}f_x&amp;0&amp;0\\0&amp;f_y&amp;0\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}x_0\\y_0\\1\end{bmatrix}\]</span></p><p>注：以下图示来自<ahref="https://blog.csdn.net/m0_43609475/article/details/111601499?ops_request_misc=%7B%22request%5Fid%22%3A%22166590175216800180653440%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=166590175216800180653440&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-111601499-null-null.142%5Ev56%5Econtrol,201%5Ev3%5Econtrol_2&amp;utm_term=图像缩放&amp;spm=1018.2226.3001.4187">（三）图像的放大和缩小_淡定的炮仗的博客-CSDN博客_图像缩放原理</a></p><p><img src="https://img.issey.top/img/202210161439895.png" /></p><p>仔细看一下图就懂了。但是对于图像放大而言，这只是第一步。</p><p>观察图像放大的图示，会发现中间空了很多没有值的像素点位。上述图像来源的博客中用python具体实现了一下直接放大的代码，详情请见博客。该博客在最后提到，“<strong>放大的图像由于是原图像素直接搬移到放大后的画布上，导致放大后的画布上的一些像素位置没有值（值为0）</strong>”。为了解决该问题，需要用到<strong>图像插值算法</strong>进行补全。关于图像插值算法将在下一节说明。</p><p><strong>示例</strong></p><p>函数</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = imresize(I,m) <span class="comment">% I可以是灰度、RGB、二值图像。m为缩放尺寸，m大于0小于1时为缩小；m大于1时为放大。</span></span><br><span class="line">X = imresize(I,[mrows,ncols]) <span class="comment">% [mrows ncols]为放大后的行和列。当mrows或ncols取值为NaN，函数会根据I的纵横比，结合另一个已知值算出mrows或ncols的值。mrows和ncols不可同时为NaN.</span></span><br><span class="line">[X,newmap] = imresize(I,map,m) <span class="comment">% 对索引图像进行缩放</span></span><br><span class="line">[...] = imresize(...,method) <span class="comment">% ...表示可为之前说过的任意一种方式，method表示选用的插值算法。method值可选择插值方法的类型：&#x27;nearest&#x27;(默认)最近邻插值、&#x27;bilinear&#x27;双线性插值、&#x27;bicubic&#x27;双三次插值；或者选择插值的核函数：&#x27;box&#x27;Box型核函数、&#x27;triangle&#x27; 三角型核函数(bilinear相同)、&#x27;Cubic&#x27;立方体型核函数(bicubic相同)等。</span></span><br><span class="line">[...] = imresize(...,parameter,value,...) <span class="comment">% 通用形式，这里不对该方法进行详细说明。</span></span><br></pre></td></tr></table></figure><p>关于最后那种方法，详细取值表格请见连接：<ahref="https://blog.csdn.net/qq_44111805/article/details/126537001?ops_request_misc=%7B%22request%5Fid%22%3A%22166583707816782391830003%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=166583707816782391830003&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-126537001-null-null.142%5Ev56%5Econtrol,201%5Ev3%5Econtrol_2&amp;utm_term=图像几何变换&amp;spm=1018.2226.3001.4187">图像处理之图像的几何变换_HardCoder的博客-CSDN博客_图像几何变换</a></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">close all;clc;clear;</span><br><span class="line">I = imread(<span class="string">&#x27;example2.jpg&#x27;</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(I);title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">X_1 = imresize(I,[NaN,<span class="number">100</span>],<span class="string">&#x27;box&#x27;</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X_1);title(<span class="string">&quot;[mrows ncols],box&quot;</span>);</span><br><span class="line">X_2 = imresize(I,<span class="number">0.5</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X_2);title(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">[X,map] = rgb2ind(I,<span class="number">16</span>);</span><br><span class="line">[X_3,newmap] = imresize(X,map,<span class="number">1.3</span>);</span><br><span class="line"><span class="built_in">figure</span>();imshow(X_3,newmap);title(<span class="string">&quot;index img&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210161519905.png" /></p><h1 id="图像插值算法">图像插值算法</h1><p>参考文章：<ahref="https://zhuanlan.zhihu.com/p/110754637">一篇文章为你讲透双线性插值- 知乎 (zhihu.com)</a> 不过要注意的是，参考文章中有一些错误。</p><p>简单来说，插值指利用已知的点来“猜”未知的点，图像领域插值常用在修改图像尺寸的过程，由旧的图像矩阵中的点计算新图像矩阵中的点并插入，不同的计算过程就是不同的插值算法。</p><p>常用的插值算法有三种：</p><ol type="1"><li>最近邻(Nearest Interpolation)：计算速度最快，效果最差。</li><li>双线性插值(BilinearInterpolation)：双线性插值是用原图像中4(2*2)个点计算新图像中1个点，效果略逊于双三次插值，速度比双三次插值快，较为平衡，在很多框架中属于默认算法。</li><li>双三次插值(Bicubicinterpolation)：双三次插值是用原图像中16(4*4)个点计算新图像中1个点，效果比较好，但是计算代价过大。</li></ol><h2 id="最近邻插值算法">最近邻插值算法</h2><p>又称邻接插值算法。选取距离插入的像素点最近的一个像素点，用它的像素值代替插入的像素点。</p><p>公式： <span class="math display">\[\begin{split}&amp; src_x = des_x\times(src_w/des_w) \\&amp; src_y = des_y\times(src_h/des_h)\end{split}\]</span> 此公式是四舍五入的规则。其中：</p><ul><li><spanclass="math inline">\(src_x\)</span>：原图像中像素点的x坐标</li><li><spanclass="math inline">\(src_y\)</span>：原图像中像素点的y坐标</li><li><spanclass="math inline">\(des_x\)</span>：变换后图像的像素点的x坐标</li><li><spanclass="math inline">\(dex_y\)</span>：变换后图像的像素点的y坐标</li><li><span class="math inline">\(src_w\)</span>：原图像宽度(width)</li><li><span class="math inline">\(src_h\)</span>：原图像高度(height)</li><li><span class="math inline">\(des_w\)</span>：变换后图像的宽度</li><li><span class="math inline">\(des_h\)</span>：变换后图像的高度</li></ul><blockquote><p>有的小伙伴可能就要问了，为什么src在等式左边啊？</p><p>实际上这个公式是先有了变换后的图像，然后这个图像中有些像素值缺失的点位。现在要计算这些点位在原图像中离哪个像素点位最近，然后用其替换，所以src在左边。可以多看看参看文章中计算示例。</p></blockquote><p>最近邻法不需要计算只需要寻找原图中对应的点，所以最近邻法速度最快，但是<strong>会破坏原图像中像素的渐变关系，原图像中的像素点的值是渐变的，但是在新图像中局部破坏了这种渐变关系</strong>。</p><h2 id="双线性插值算法">双线性插值算法</h2><h3 id="单线性插值">单线性插值</h3><p>（以下图示来自之前提到的参考文章）</p><p>已知中P1点和P2点，坐标分别为<spanclass="math inline">\((x1,y1)\)</span>、<spanclass="math inline">\((x2,y2)\)</span>，要计算 $[x1,x2] $区间内某一位置x 在直线上的y值。</p><p><img src="https://img.issey.top/img/202210161914537.jpeg" /></p><p>看图就知道了，实际上单线型插值就是建立（线性）函数，然后找x对应的<spanclass="math inline">\(f(x)\)</span>即可。</p><p>根据两点求一条直线公式： <span class="math display">\[\frac{y-y_1}{x-x_1} = \frac{y_2-y_1}{x_2-x_1}\]</span> 整理： <span class="math display">\[y = \frac{x_2-x}{x_2-x_1}y_1+\frac{x-x_1}{x_2-x_1}y_2\]</span>上述是对于一维（图像）而言，x即像素点位，y为像素值。为了便于后续理解，将y改写为<spanclass="math inline">\(f(x)\)</span> 。右式<spanclass="math inline">\(f(x_i)\)</span>前的系数改称为权重。 <spanclass="math display">\[f(x) = \frac{x_2-x}{x_2-x_1}f(x_1)+\frac{x-x_1}{x_2-x_1}f(x_2)\]</span> 现在将一维图像拓展为二维图像。</p><h3 id="双线性插值">双线性插值</h3><p>已知四个点<spanclass="math inline">\(Q_{11}(x_1,y_1),Q_{12}(x_1,y_2),Q_{21}(x_2,y_1),Q_{22}(x_2,y_2)\)</span>。根据该求点<spanclass="math inline">\(P(x,y)\)</span> 的像素值。</p><p><img src="https://img.issey.top/img/202210161929998.jpeg" /></p><p>双线性插值是分别在两个方向计算了共3次单线性插值：在x方向求2次单线性插值，获得<spanclass="math inline">\(R1(x, y_1)\)</span>、<spanclass="math inline">\(R2(x,y_2)\)</span>两个临时点，再在y方向计算1次单线性插值得出<spanclass="math inline">\(P(x, y)\)</span>。</p><p>第一步： <span class="math display">\[\begin{split}&amp; f(R_1) =\frac{x_2-x}{x_2-x_1}f(Q_{11})+\frac{x-x_1}{x_2-x_1}f(Q_{21}) \\&amp; f(R_2) =\frac{x_2-x}{x_2-x_1}f(Q_{12})+\frac{x-x_1}{x_2-x_1}f(Q_{22})\end{split}\]</span></p><blockquote><p>为什么权值计算没有涉及y轴？</p><p>因为y轴没变，所以权值仅取决于x轴。在接下来的第二步中也是同样的道理，x轴没变，权值仅取决于y轴。</p></blockquote><p>第二步： <span class="math display">\[f(P) = \frac{y_2-y}{y_2-y_1}f(R_1)+\frac{y-y_1}{y_2-y_1}f(R_2)\]</span>先暂时不联立第一步和第二步，我们先想想已知四个点的关系。我们进行插值时，找的四个点应该是靠在一起的，所以有：<span class="math display">\[\begin{split}&amp; x_2-x_1 = 1 \\&amp; y_2-y_1 = 1\end{split}\]</span> <strong>化简</strong></p><p>第一步： <span class="math display">\[\begin{split}&amp; f(R_1) = (x_2-x)f(Q_{11})+{(x-x_1)}f(Q_{21}) \\&amp; f(R_2) = {(x_2-x)}f(Q_{12})+{(x-x_1)}f(Q_{22})\end{split}\]</span> 第二步： <span class="math display">\[f(P) = (y_2-y)f(R_1)+(y-y_1)f(R_2)\]</span> 联立第一步和第二步： <span class="math display">\[\begin{split}f(P) &amp; = (y_2-y)f(R_1)+(y-y_1)f(R_2) \\&amp; =(y_2-y)[(x_2-x)f(x_1,y_1)+(x-x_1)f(x_2,y_1)]+(y-y_1)[{(x_2-x)}f(x_1,y_2)+{(x-x_1)}f(x_2,y_2)]\\&amp; =(y_2-y)(x_2-x)f(x_1,y_1)+(y_2-y)(x-x_1)f(x_2,y_1)+(y-y_1)(x_2-x)f(x_1,y_2)+(y-y_1)(x-x_1)f(x_2,y_2)\\&amp; =(y_2-y)(x_2-x)f(Q_{11})+(y_2-y)(x-x_1)f(Q_{21})+(y-y_1)(x_2-x)f(Q_{12})+(y-y_1)(x-x_1)f(Q_{22})\end{split}\]</span> <strong>总结</strong></p><p>双线性插值算法是有缺陷的，比如边界像素点还是存在有的像素只是进行了单线性插值，并不能保证每一个像素都是双线性插值。不过这里暂时不讨论。</p><h2 id="双三次插值算法">双三次插值算法</h2><p>又称三次卷积插值。它更复杂，不仅考虑了4个邻近点，还考虑了灰度值的变换率。双三次插值算法可以克服最近邻和双线性插值算法的缺陷，计算精度高。但由此也会导致计算量较大。</p><p>这里暂时不对双三次插值算法进行详细讨论。</p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【从FT到DFT和FFT】（三）从离散傅里叶变换到快速傅里叶变换</title>
      <link href="/article/962730553a9a/"/>
      <url>/article/962730553a9a/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>前置阅读</strong></p><ol type="1"><li><ahref="https://www.issey.top/article/bdbeeb7f1cb2/">【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导| issey的博客</a></li><li><ahref="https://www.issey.top/article/5921f880c513/">【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换| issey的博客</a></li></ol><p><strong>推荐阅读</strong></p><ul><li><a href="http://wjknowledge.top/2022/08/22/from-fft-to-ntt/">FromFFT to NTT | fat fatzard's Blog</a></li><li><ahref="https://blog.csdn.net/u011861755/article/details/106832974?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=fft公式&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-9-106832974.142%5Ev56%5Econtrol,201%5Ev3%5Econtrol_2&amp;spm=1018.2226.3001.4187">【数字信号处理】【傅里叶分析】【FFT】快速傅里叶变换的完整公式推导_寒霜雨刃的博客-CSDN博客</a></li><li><ahref="https://blog.csdn.net/enjoy_pascal/article/details/81478582?ops_request_misc=%7B%22request%5Fid%22%3A%22166576319316782417091766%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&amp;request_id=166576319316782417091766&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-9-81478582-null-null.142%5Ev56%5Econtrol,201%5Ev3%5Econtrol_2&amp;utm_term=fft公式&amp;spm=1018.2226.3001.4187">十分简明易懂的FFT（快速傅里叶变换）_路人黑的纸巾的博客-CSDN博客_fft</a></li></ul><h1 id="前言">前言</h1><p>早在打ACM时对FFT就有所耳闻，学长一再叮嘱一定要每个队员都把FFT尽早学会。当时只知道FFT可以拿来加速多项式乘法，可以将时间复杂度从<spanclass="math inline">\(O(n^2)\)</span>加速至<spanclass="math inline">\(O(nlogn)\)</span>。奈何我是数论fw，一直迟迟没有学习。直到图像处理，才知道FFT其实是离散傅里叶变换（DFT）的加速版，多项式乘法不过是它的应用之一。</p><p>我查到的推导FFT的过程大多都是从多项式乘法入手，在这里先贴上我某数论朋友的FFT详细推导（推荐阅读第一篇文章），他在文章中同时用代码详细实现了FFT，并且进一步推导了快速数论变换（NTT），如果对公式推导和代码实现感兴趣的朋友，强力推荐。</p><h1id="从离散傅里叶变换到快速傅里叶变换">从离散傅里叶变换到快速傅里叶变换</h1><p>上篇提到的离散傅里叶变换(DFT)公式以及逆变换公式： <spanclass="math display">\[\begin{split}&amp; X[k] = \sum_{m=0}^{M-1}x[m]e^{-i\frac{2\pi}{M}mk} \\&amp; x[k] = \frac{1}{M}\sum_{m=0}^{M-1}X[m]e^{i\frac{2\pi}{M}mk}\end{split}\]</span> 我们先讨论正变换公式： <span class="math display">\[X[k] = \sum_{m=0}^{M-1}x[m]e^{-i\frac{2\pi}{M}mk}\]</span>现在考虑来对一个长度为N的离散非周期序列做离散傅里叶变换，时间复杂度很容易看出是<spanclass="math inline">\(O(n^2)\)</span>。接下来我们将使用FFT把时间复杂度减小到<spanclass="math inline">\(O(nlogn)\)</span>。</p><h2 id="单位根">单位根</h2><p>复数<span class="math inline">\(w\)</span>满足<spanclass="math inline">\(w^n=1\)</span>称作w是n次单位根。<del>如果你看过傅里叶变换原理，应该可以知道复数相乘其实是旋转。</del>接下来例举8次单位根和4次单位根图像：</p><p>注：这两幅图来自博客：<ahref="https://blog.csdn.net/WADuan2/article/details/79529900?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=fft算法&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-79529900.nonecase&amp;spm=1018.2226.3001.4187">FFT算法讲解——麻麻我终于会FFT了！_Duan2baka的博客-CSDN博客_fft算法</a></p><p><img src="https://img.issey.top/img/202210151636838.png" /></p><p><img src="https://img.issey.top/img/202210151637783.png" /></p><p>能够轻易看出， <span class="math display">\[\begin{split}&amp; w_{2n}^{2m} = w_n^m \\&amp; w_n^m = -w_n^{m+\frac{n}{2}}\end{split}\]</span></p><h2 id="对dft进行分治得到fft">对DFT进行分治得到FFT</h2><p>设多项式<span class="math inline">\(A(x)\)</span>: <spanclass="math display">\[A(x) = \sum_{i=0}^{n-1}a_ix^i =a_0+a_1x+a_2x^2+...+a_{n-1}x^{n-1}\]</span></p><blockquote><p>仔细观察DFT，其实<span class="math inline">\(A(x)\)</span>就是DFT的简化版。</p></blockquote><p>对<span class="math inline">\(A(x)\)</span>根据奇偶性劈成两半： <spanclass="math display">\[\begin{split}A(x) &amp; =(a_0+a_2x^2+...+a_{n-2}x^{n-2})+(a_1x+a_3x^3+...+a_{n-1}x^{n-1}) \\&amp; = (a_0+a_2x^2+...+a_{n-2}x^{n-2})+x(a_1+a_3x^2+...+a_{n-1}x^{n-2})\end{split}\]</span> 现在，设: <span class="math display">\[\begin{split}&amp; A_1(x) =a_0+a_2x+a_4x^2+...+a_{n-2}x^{\frac{n}{2}-1} \\&amp; A_2(x) =a_1+a_3x+a_5x^2+...+a_{n-1}x^{\frac{n}{2}-1} \\\end{split}\]</span> 于是： <span class="math display">\[A(x) = A_1(x^2)+xA_2(x^2)\]</span></p><h3 id="计算前半截">计算前半截</h3><p>设<span class="math inline">\(k&lt;\frac{n}{2}\)</span>，将<spanclass="math inline">\(x\)</span>换为单位根<spanclass="math inline">\(w_n^k\)</span>: <span class="math display">\[\begin{split}A(w_n^k) &amp; = A_1(w_n^{2k})+w_n^kA_2(w_n^{2k}) \\&amp; = A_1(w_{\frac{n}{2}}^{k})+w_n^kA_2(w_{\frac{n}{2}}^{k})\end{split}\]</span></p><h3 id="计算后半截">计算后半截</h3><p>将<span class="math inline">\(w_n^{k+\frac{n}{2}}\)</span> 代入<spanclass="math inline">\(A(w_n^{k+\frac{n}{2}})\)</span>: <spanclass="math display">\[\begin{split}A(w_n^{k+\frac{n}{2}}) &amp; =A_1(w_n^{2k+n})+w_n^{k+\frac{n}{2}}A_2(w_n^{2k+n}) \\&amp; = A_1(w_{n}^{2k}w_n^n)-w_n^kA_2(w_{n}^{2k}w_n^n) \\&amp; = A_1(w_n^{2k})-w_n^kA_2(w_n^{2k}) \\&amp; = A_1(w_{\frac{n}{2}}^{k})-w_n^kA_2(w_{\frac{n}{2}}^{k})\end{split}\]</span></p><hr /><p>我们发现前半截和后半截要计算的局部都一样，不过符号不一样。所以只需要计算<spanclass="math inline">\(A_1(w_{\frac{n}{2}}^{k})\)</span> 、<spanclass="math inline">\(A_2(w_{\frac{n}{2}}^{k})\)</span>,就可以计算出前半段和后半段的值。所以我们可以利用分治：每次计算只扫描前一半的序列，即可得后一半序列结果。时间复杂度自然缩短为<spanclass="math inline">\(O(nlog n)\)</span>。对DFT进行分治的算法就叫FFT。</p><h2 id="快速傅里叶逆变换ifft">快速傅里叶逆变换（IFFT）</h2><p>结论：<strong>一个多项式在分治的过程中乘上单位根的共轭复数，分治完的每一项除以n即为原多项式的每一项系数。</strong></p><p>看不懂？这里解释一下这句话在说什么。</p><p>考虑这样一个问题：现在要将离散频域变为离散时域，即离散傅里叶逆变换（IDFT），现在需要利用分治加速（IFFT）。</p><p>前面已经提到了IDFT的公式： <span class="math display">\[x[k] = \frac{1}{M}\sum_{m=0}^{M-1}X[m]e^{i\frac{2\pi}{M}mk}\]</span> 使用同样的思路，对<spanclass="math inline">\(\sum_{m=0}^{M-1}X[m]e^{i\frac{2\pi}{M}mk}\)</span>进行分治即可。</p><p>不过在分治过程中，观察DFT和IDFT指数部分的不同：</p><p>DFT：<span class="math inline">\(e^{-i\frac{2\pi}{M}mk}\)</span></p><p>IDFT:<span class="math inline">\(e^{i\frac{2\pi}{M}mk}\)</span></p><p>发现没，这两个虚部是相反数。所以对于FFT的分治步骤，需要对单位根乘共轭复数。</p><p>然后直接看公式就知道，分治结束了还要除个M。</p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正交变换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【从FT到DFT和FFT】（二）从傅里叶变换到离散傅里叶变换</title>
      <link href="/article/5921f880c513/"/>
      <url>/article/5921f880c513/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p><strong>前置阅读</strong></p><ol type="1"><li><ahref="https://www.issey.top/article/bdbeeb7f1cb2/">【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导| issey的博客</a></li></ol><p><strong>参考教程</strong></p><ul><li><ahref="https://binaryai.blog.csdn.net/article/details/122244167">傅里叶变换落地：离散傅里叶变换（DFT）<em>二进制人工智能的博客-CSDN博客</em></a></li><li><ahref="https://blog.csdn.net/qq_43409114/article/details/104870977">傅里叶变换、离散傅里叶变换(DFT)、快速傅里叶变换(FFT)详解_ReWz的博客-CSDN博客_</a></li><li><ahref="https://www.cnblogs.com/ymzm204/p/12728706.html">二维离散傅里叶变换计算过程详析- EAo - 博客园</a></li></ul><h1 id="前言">前言</h1><p>本篇不会像上一篇那样推导的详细，原因其一是最近时间不够，在这样推下去就要落学习的进度了；其二是DFT涉及数字信号方面的知识，前置知识本人了解的较少。如果之后有机会，可能会倒回来推公式。只能说是心有余而力不足了。</p><p>本篇主要为从FT到DFT的演化的公式总结，另外引出二维离散傅里叶公式。</p><hr /><p><strong>为什么需要离散傅里叶</strong></p><p>在计算机中处理数据都是离散的，且无法直接进行连续积分运算，所以需要对傅里叶变换进行离散化。</p><h1id="从连续傅里叶级数fs到离散傅里叶级数dfs">从连续傅里叶级数（FS）到离散傅里叶级数(DFS)</h1><p>回顾上篇我们推导得出的傅里叶级数公式;</p><p>对于周期为T的函数：<span class="math inline">\(f(t) =f(t+T)\)</span></p><p>它的傅里叶级数为： <span class="math display">\[\begin{split}&amp; F(t) = \frac{1}{T}\int_0^Tf(t)e^{-inwt}dt \\&amp; f(t) = \sum_{-\infty}^{\infty}F(t)e^{inwt}\end{split}\]</span> 现在将<span class="math inline">\(f(t)\)</span>改为离散的周期序列，</p><p>对于周期为N的序列<span class="math inline">\(x[k] = x[k+N]\)</span>，将连续傅里叶级数改为离散傅里叶级数： <span class="math display">\[\begin{split}&amp; X[k] = \sum_{n=0}^{N-1}x[n]e^{-i\frac{2\pi}{N}nk} \\&amp; x[k] = \frac{1}{N}\sum_{n=0}^{N-1}X[n]e^{i\frac{2\pi}{N}nk}\end{split}\]</span></p><blockquote><p>推导暂时略过。</p></blockquote><h1id="从离散傅里叶级数dfs到离散傅里叶变换dft">从离散傅里叶级数(DFS)到离散傅里叶变换（DFT）</h1><p>实际上DFT和DFS的公式是一样的，但是N的含义不同。可以这样理解：</p><p>对于周期离散序列，N就是它的周期，我们使用离散傅里叶级数来变换它。而对于非周期离散序列，我们首先回顾在上篇中推导傅里叶变换时的思路：<code>一个非周期函数可以理解为一个周期为无穷的函数。</code>非周期函数的x可以无限延长，所以我们将它的周期设置为无穷大。</p><p>而计算机中处理的序列始终是<strong>有限序列</strong>，用同样的思想，我们可以将<code>非周期离散序列的周期理解为它本身的长度</code>。如果一个非周期有限序列的长度为N，那我们可以理解为它的周期为N。</p><p>于是对于有限非周期离散序列，离散傅里叶公式可等同于周期为它本身的离散傅里叶级数。</p><p>为了便于区分，令有限非周期离散序列的长度为M，那么DFT的公式为： <spanclass="math display">\[\begin{split}&amp; X[k] = \sum_{m=0}^{M-1}x[m]e^{-i\frac{2\pi}{M}mk} \\&amp; x[k] = \frac{1}{M}\sum_{m=0}^{M-1}X[m]e^{i\frac{2\pi}{M}mk}\end{split}\]</span> 其实也是有证明过程的，不过这里还是先略过了。</p><h1 id="二维离散傅里叶变换">二维离散傅里叶变换</h1><blockquote><p>详细推导过程请见参考连接。</p></blockquote><p>二维离散傅里叶变换主要用于图像处理。</p><p>令<span class="math inline">\(f(x,y)\)</span>代表一副大小为<spanclass="math inline">\(M\times N\)</span>的数字图像，其中：<spanclass="math inline">\(x = 0,1,2,...,M-1;y = 0,1,2,...,N-1\)</span>。</p><p>二维离散傅里叶变换公式如下： <span class="math display">\[\begin{split}&amp; F(u,v) =\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)e^{-i2\pi(\frac{ux}{M}+\frac{vy}{N})}\\&amp; f(u,v) =\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}F(x,y)e^{i2\pi(\frac{ux}{M}+\frac{vy}{N})}\end{split}\]</span> 注：第一个式子为正变换，第二个式子为反变换。</p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正交变换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【从FT到DFT和FFT】（一）从傅里叶级数到傅里叶变换的详细公式推导</title>
      <link href="/article/bdbeeb7f1cb2/"/>
      <url>/article/bdbeeb7f1cb2/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p>参考教程：</p><ol type="1"><li><ahref="https://www.bilibili.com/video/BV1Et411R78v/?spm_id_from=333.999.0.0&amp;vd_source=747540861ba5c41c17852ccf069029f5">纯干货数学推导_傅里叶级数与傅里叶变换_Part1_三角函数的正交性_</a></li><li><ahref="https://www.bilibili.com/video/BV17t411d7hm/?spm_id_from=333.788.b_636f6d6d656e74.29&amp;vd_source=747540861ba5c41c17852ccf069029f5">纯干货数学推导_傅里叶级数与傅里叶变换_Part2_周期为2Pi的函数展开_</a></li><li><ahref="https://www.bilibili.com/video/BV1wb411K7Kp">纯干货数学推导_傅里叶级数与傅里叶变换_Part3_周期为2L的函数展开</a></li><li><ahref="https://www.bilibili.com/video/BV13b411P7mU">纯干货数学推导_傅里叶级数与傅里叶变换_Part4_傅里叶级数的复数形式</a></li><li><ahref="https://www.bilibili.com/video/BV1jt411U7Bp">纯干货数学推导_傅里叶级数与傅里叶变换_Part5_从傅里叶级数推导傅里叶变换</a></li><li><ahref="https://www.bilibili.com/video/BV1St41117fH">纯干货数学推导_傅里叶级数与傅里叶变换_Part6_总结与闲话（完）</a></li></ol><h1 id="前言">前言</h1><p>本系列为傅里叶系列公式推导学习笔记，本文的参考教程如上所示。</p><p>本文将推导从三角函数正交性到傅里叶级数再到傅里叶变换整个过程，如有错误欢迎指出。</p><p>本文将不会介绍傅里叶变换的具体含义和应用，只聚焦于公式推导。在开始之前，请确保已经了解傅里叶变换是什么，有什么用。相关文章可以参考：<ahref="https://zhuanlan.zhihu.com/p/19763358">傅里叶分析之掐死教程（完整版）</a></p><h1 id="三角函数的正交性">三角函数的正交性</h1><p>这部分将介绍整个傅里叶变换的数学基础——三角函数的正交性。</p><h2 id="简单解释一下正交">简单解释一下正交</h2><p>对于两个向量，正交意味着相互垂直，二者相乘为0。 <spanclass="math display">\[\vec a\cdot\vec b = a_1b_1+a_2b_2+...+a_nb_n = 0\]</span></p><p>拓展到连续函数后，当两个函数积分等于0的时候，我们说这两个函数正交。</p><p><span class="math inline">\(a = f(x)\)</span></p><p><span class="math inline">\(b=g(x)\)</span></p><p>若<span class="math inline">\(f(x)\)</span>与<spanclass="math inline">\(g(x)\)</span> 正交，有： <spanclass="math display">\[a\cdot b=\int_{x_0}^{x_1}f(x)g(x)dx = 0\]</span></p><h2 id="三角函数正交性定义">三角函数正交性定义</h2><p><strong>三角函数系：</strong></p><p><spanclass="math inline">\(\{sin(0x),cos(0x),sin(x),cos(x),sin(2x),cos(2x),...,sin(nx)\}\)</span></p><p>上述表示也可简化为</p><p><spanclass="math inline">\(\{1,sin(x),cos(x),sin(2x),cos(2x),...,sin(nx)\}\)</span></p><p><strong>对于上述集合，有正交：</strong></p><p><span class="math inline">\(\int_{-\pi}^\pi sin(nx)cos(mx)dx =0\)</span> ,当<span class="math inline">\(n=m\)</span>和<spanclass="math inline">\(n\neq m\)</span> 时均成立。</p><p><span class="math inline">\(\int_{-\pi}^\pi cos(nx)cos(mx)dx =0,n\neq m\)</span>。</p><h2 id="证明">证明</h2><p>已知： <span class="math display">\[\begin{split}&amp;sin(A+B) = sinAcosB+cosAsinB\\&amp;sin(A-B) = sinAcosB-cosAsinB\\&amp;cos(A+B) = cosAcosB-sinAsinB\\&amp;cos(A-B) = cosAcosB+sinAsinB\end{split}\]</span></p><p><span class="math display">\[\begin{split}\int_{-\pi}^\pi sin(nx)cos(mx)dx&amp; = \int_{-\pi}^{\pi}\frac{1}{2}[sin(n-m)x+sin(n+m)x]dx \\&amp; = \frac{1}{2}\int_{-\pi}^{\pi}[sin(n-m)x+sin(n+m)x]dx \\&amp; =\frac{1}{2}[-\frac{1}{n-m}cos(n-m)x|_{-\pi}^{\pi}-\frac{1}{n+m}cos(n+m)x|_{-\pi}^{\pi}]\\&amp; = 0+0 \\&amp; = 0\end{split}\]</span></p><p>当<span class="math inline">\(m\neq n\)</span>时： <spanclass="math display">\[\begin{split}\int_{-\pi}^\pi cos(nx)cos(mx)dx&amp; = \int_{-\pi}^\pi\frac{1}{2}[cos(n-m)x+cos(n+m)x]dx \\&amp; = \int_{-\pi}^\pi\frac{1}{2}[cos(n-m)xdx+cos(n+m)xdx] \\&amp; =\frac{1}{2}[\frac{1}{n-m}sin(n-m)x|_{-\pi}^{\pi}+\frac{1}{n+m}sin(n+m)x|_{-\pi}^{\pi}]\\&amp; = 0+0 \\&amp; = 0\end{split}\]</span> 当<span class="math inline">\(m=n\neq0\)</span>时： <spanclass="math display">\[\begin{split}\int_{-\pi}^\pi cos(nx)cos(mx)dx&amp; =  \int_{-\pi}^\pi\frac{1}{2}(1+cos2mx)dx \\&amp; = \frac{1}{2}(\int_{-\pi}^\pi1dx+0) \\&amp; = \frac{1}{2}x|_{-\pi}^{\pi} \\&amp; = \pi\end{split}\]</span> 当<span class="math inline">\(m=n=0\)</span>时： <spanclass="math display">\[\begin{split}\int_{-\pi}^\pi cos(nx)cos(mx)dx&amp; = \int_{-\pi}^\pi1dx \\&amp; = 2\pi\end{split}\]</span></p><h1 id="展开周期为2pi的傅里叶级数">展开周期为2pi的傅里叶级数</h1><p>看过傅里叶变换原理后，可以知道，任意一个周期为<spanclass="math inline">\(2\pi\)</span>的波形都可以看作是许多个波形的叠加。</p><p>即对于函数<span class="math inline">\(f(x) = f(x+2\pi)\)</span>，有： <span class="math display">\[\begin{split}f(x) = \sum_{n=0}^{\infty}a_ncosnx+\sum_{n=0}^{\infty}b_nsinnx\end{split}\]</span> 上式也可写为： <span class="math display">\[\begin{split}f(x) = \frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncosnx+b_nsinnx)\end{split}\]</span> 两式的区别在于单独提出了<spanclass="math inline">\(a_0\)</span>，接下来会解释为什么要这样提取，同时展开<spanclass="math inline">\(a_0,a_n,b_n\)</span>。</p><h2 id="公式变换与展开">公式变换与展开</h2><strong>展开<span class="math inline">\(a_0\)</span></strong> <spanclass="math display">\[\begin{split}f(x)&amp; = \sum_{n=0}^{\infty}a_ncosnx+\sum_{n=0}^{\infty}b_nsinnx \\&amp; =a_0cos0x+\sum_{n=1}^{\infty}a_ncosnx+b_0sin0x+\sum_{n=1}^{\infty}b_nsinnx\\&amp; = a_0+\sum_{n=1}^{\infty}a_ncosnx+\sum_{n=1}^{\infty}b_nsinnx \\\end{split}\]</span> 两边同时求积分： $$<span class="math display">\[\begin{split}\int_{-\pi}^{\pi}f(x)dx&amp; =\int_{-\pi}^{\pi}a_0dx+\int_{-\pi}^{\pi}\sum_{n=1}^{\infty}a_ncosnxdx+\int_{-\pi}^{\pi}\sum_{n=1}^{\infty}b_nsinnxdx\\&amp; = \int_{-\pi}^{\pi}a_0dx\end{split}\]</span><p>$$</p><blockquote><p>简单说一下后两项为什么都为0：</p><p>先从求和里单独拿出一项，将<spanclass="math inline">\(a_n\)</span>提到求积前面，对后面的积分为0。于是所有项求和为0.</p></blockquote><p><span class="math display">\[\begin{split}&amp; \int_{-\pi}^{\pi}f(x)dx = \int_{-\pi}^{\pi}a_0dx = 2\pi a_0 \\&amp; a_0 = \frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)dx\end{split}\]</span></p><p>为了使得“更好看一点”，就把<spanclass="math inline">\(a_0\)</span>再提了一个<spanclass="math inline">\(\frac{1}{2}\)</span>出来，于是才有了： <spanclass="math display">\[\begin{split}f(x) = \frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncosnx+b_nsinnx)\end{split}\]</span> 此时<span class="math inline">\(a_0 =\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)dx\)</span></p><p><strong>展开<span class="math inline">\(a_n\)</span></strong></p><p>两边同时乘<span class="math inline">\(cosmx\)</span>，再求积分 <spanclass="math display">\[\begin{split}&amp; f(x) =\frac{a_0}{2}+\sum_{n=1}^{\infty}a_ncosnx+\sum_{n=1}^{\infty}b_nsinnx \\&amp; f(x)cosmx =\frac{a_0}{2}cosmx+\sum_{n=1}^{\infty}a_ncosnx~cosmx+\sum_{n=1}^{\infty}b_nsinnx~cosmx\\&amp; \int_{-\pi}^{\pi}f(x)cosmx~dx =\int_{-\pi}^{\pi}\frac{a_0}{2}cosmx~dx+\int_{-\pi}^{\pi}\sum_{n=1}^{\infty}a_ncosnx~cosmx~dx+\int_{-\pi}^{\pi}\sum_{n=1}^{\infty}b_nsinnx~cosmx~dx\\&amp; \int_{-\pi}^{\pi}f(x)cosmx~dx=0+\int_{-\pi}^{\pi}\sum_{n=1}^{\infty}a_ncosnx~cosmx~dx+0 \\&amp; \int_{-\pi}^{\pi}f(x)cosmx~dx =\int_{-\pi}^{\pi}\sum_{n=1}^{\infty}a_ncosnx~cosmx~dx \\\end{split}\]</span> 根据三角函数的正交性，有<spanclass="math inline">\(\int_{-\pi}^\pi cos(nx)cos(mx)dx = 0,n\neqm\)</span>。</p><p>因此只有当<span class="math inline">\(n=m\)</span>时，右边才可以被保留下来，其他情况均为0。</p><p>于是： <span class="math display">\[\begin{split}&amp; \int_{-\pi}^{\pi}f(x)cosmx~dx =\int_{-\pi}^{\pi}a_ncosnx~cosmx~dx,n=m\\&amp; \int_{-\pi}^{\pi}f(x)cosnx~dx = \int_{-\pi}^{\pi}a_ncos^2nx~dx \\&amp; \int_{-\pi}^{\pi}f(x)cosnx~dx = a_n\int_{-\pi}^{\pi}cos^2nx~dx \\&amp; \int_{-\pi}^{\pi}f(x)cosnx~dx = a_n\pi \\&amp; a_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)cosnx~dx\end{split}\]</span> <strong>展开<spanclass="math inline">\(b_n\)</span></strong></p><p>使用同样的方法，不同点只是两边同时乘<spanclass="math inline">\(sinmx\)</span>。 <span class="math display">\[b_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)sinnx~dx\]</span> 于是我们可以得到一个周期为<spanclass="math inline">\(2\pi\)</span>的函数的傅里叶级数展开：</p><h2 id="最终结果">最终结果</h2><p>对于函数<span class="math inline">\(f(x) = f(x+2\pi)\)</span> ，有：<span class="math display">\[\begin{split}f(x) = \frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncosnx+b_nsinnx)\end{split}\]</span> 其中， <span class="math display">\[\begin{split}&amp; a_0 = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)dx \\&amp; a_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)cosnx~dx \\&amp; b_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)sinnx~dx \\\end{split}\]</span> 那么，如果将周期为<span class="math inline">\(2\pi\)</span>推广为周期为<spanclass="math inline">\(2L\)</span>的函数，会有什么变化呢？</p><h1id="推广为周期为2l的函数适用的傅里叶级数">推广为周期为2L的函数适用的傅里叶级数</h1><p>现在有周期为2L的函数：<span class="math inline">\(f(t) =f(t+2L)\)</span>。</p><p>如果要想使用刚才我们推出的周期为<spanclass="math inline">\(2\pi\)</span> 的傅里叶级数，可以使用换元。</p><h2 id="换元处理">换元处理</h2><p>令 <span class="math display">\[\begin{split}&amp; x = \frac{\pi}{L}t \\&amp; t = \frac{L}{\pi}x\end{split}\]</span> 那么<span class="math inline">\(f(t) =f(\frac{L}{\pi}x)\)</span>，令<spanclass="math inline">\(f(\frac{L}{\pi}x) =g(x)\)</span>。并且我们可以算出，当<spanclass="math inline">\(t=2L\)</span>时，<span class="math inline">\(x =2\pi\)</span> 。</p><p>于是对于<span class="math inline">\(g(x)\)</span> ，它的周期为<spanclass="math inline">\(2\pi\)</span> ，适用刚才的傅里叶级数展开： <spanclass="math display">\[g(x) = \frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncosnx+b_nsinnx)\]</span> 其中， <span class="math display">\[\begin{split}&amp; a_0 = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)dx \\&amp;a_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)cosnx~dx \\&amp; b_n =\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)sinnx~dx \\\end{split}\]</span> 代入<span class="math inline">\(x = \frac{\pi}{L}t\)</span>:<span class="math display">\[\begin{split}&amp; cosnx = cos\frac{n\pi}{L}t \\&amp; sinnx = sin\frac{n\pi}{L}t \\&amp; g(x) = f(t) \\&amp; \int_{-\pi}^{\pi}dx = \int_{-L}^{L}d\frac{\pi}{L}t \\&amp; \frac{1}{\pi}\int_{-\pi}^{\pi}dx = \frac{1}{L}\int_{-L}^Ldt\end{split}\]</span></p><h2 id="最终结果-1">最终结果</h2><p>于是对于周期为2L的函数的傅里叶级数展开： <spanclass="math display">\[f(t) =\frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncos\frac{n\pi}{L}t+b_nsin\frac{n\pi}{L}t)\]</span> 其中： <span class="math display">\[\begin{split}&amp; a_0 = \frac{1}{L}\int_{-L}^Lf(t)dt \\&amp; a_n = \frac{1}{L}\int_{-L}^Lf(t)cos\frac{n\pi}{L}t~dt \\&amp; b_n = \frac{1}{L}\int_{-L}^Lf(t)sin\frac{n\pi}{L}t~dt \\\end{split}\]</span></p><h2 id="美化公式">美化公式</h2><p>一般在工程中，时间t从0开始，周期<span class="math inline">\(T =2L\)</span>,我们再令<span class="math inline">\(\omega =\frac{2\pi}{T}\)</span> 。</p><p>因为 <span class="math display">\[\int_{-L}^Ldt = \int_0^{2L}dt = \int_0^Tdt\]</span> 于是对于<span class="math inline">\(f(t) = f(t+T)\)</span>,<span class="math display">\[f(t) = \frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncoswt+b_nsinwt),w =\frac{2\pi}{T}\]</span> 其中， <span class="math display">\[\begin{split}&amp; a_0 = \frac{1}{L}\int_{-L}^Lf(t)dt \\&amp; a_n = \frac{1}{L}\int_{-L}^Lf(t)coswt~dt \\&amp; b_n = \frac{1}{L}\int_{-L}^Lf(t)sinwt~dt \\\end{split}\]</span>联系物理上的频率，傅里叶级数一下就变得优美起来。不过还不够，我们需要将它推广到更普遍的情况——没有周期的函数。不过在此之前，我们需要做一件非常重要的事：<strong>将傅里叶级数表示为复数形式</strong>。</p><h1 id="将傅里叶级数表示为复数形式">将傅里叶级数表示为复数形式</h1><p>这部分非常非常重要，将是我们下一步推导傅里叶变换的基础。我们将会把上一小节中求到的<spanclass="math inline">\(f(t)\)</span>变换为复数形式。在变换时会用到<strong>欧拉公式</strong>，这里先简单介绍一下欧拉公式。</p><h2 id="欧拉公式">欧拉公式</h2><p>关于欧拉公式的详细推导请参考连接：<ahref="https://www.bilibili.com/video/av32271249/?vd_source=747540861ba5c41c17852ccf069029f5">【工程数学基础】6_SinX=2?复变函数 欧拉公式_</a></p><p>欧拉公式：<span class="math inline">\(e^{i\theta} =cos\theta+isin\theta\)</span></p><p>由欧拉公式可进一步推出： <span class="math display">\[\begin{split}&amp; cos\theta = \frac{1}{2}(e^{i\theta}+e^{-i\theta}) \\&amp; sin\theta = - \frac{1}{2}i(e^{i\theta}-e^{-i\theta})\end{split}\]</span></p><h2 id="变换公式">变换公式</h2><p>将欧拉公式推导公式代入<span class="math inline">\(f(t)\)</span>中：<span class="math display">\[\begin{split}f(t)&amp; = \frac{a_0}{2}+\sum_{n=1}^{\infty}(a_ncoswt+b_nsinwt) \\&amp; =\frac{a_0}{2}+\sum_{n=1}^{\infty}(\frac{a_n-ib_n}{2}e^{inwt}+\frac{a_n+ib_n}{2}e^{-inwt})\\&amp; = \frac{a_0}{2}+\sum_{n=1}^{\infty}\frac{a_n-ib_n}{2}e^{inwt}+\sum_{n=1}^{\infty}\frac{a_n+ib_n}{2}e^{-inwt}\end{split}\]</span> <strong>先观察最后一项</strong></p><p>将n换元为-n： <span class="math display">\[\begin{split}\sum_{n=1}^{\infty}\frac{a_n+ib_n}{2}e^{-inwt} =\sum_{n=-\infty}^{-1}\frac{a_{-n}+ib_{-n}}{2}e^{inwt}\end{split}\]</span> <strong>观察第一项</strong> <span class="math display">\[\frac{a_0}{2} = \sum_{n=0}^0\frac{a_0}{2}e^{inwt}\]</span> <strong>于是<spanclass="math inline">\(f(t)\)</span>进一步变为</strong> <spanclass="math display">\[\begin{split}f(t)&amp; = \frac{a_0}{2}+\sum_{n=1}^{\infty}\frac{a_n-ib_n}{2}e^{inwt}+\sum_{n=1}^{\infty}\frac{a_n+ib_n}{2}e^{-inwt}\\&amp; = \sum_{n=0}^0\frac{a_0}{2}e^{inwt}+\sum_{n=1}^{\infty}\frac{a_n-ib_n}{2}e^{inwt}+\sum_{n=-\infty}^{-1}\frac{a_{-n}+ib_{-n}}{2}e^{inwt}\\&amp; = \sum_{-\infty}^{\infty}C_ne^{inwt}\end{split}\]</span> 其中， <span class="math display">\[C_n = \left\{\begin{aligned}&amp;\frac{a_0}{2},n=0\\&amp;\frac{a_n-ib_n}{2},n=1,2,3,4,...\\&amp;\frac{a_{-n}+ib_{-n}}{2},n = -1,-2,-3,-4,...\end{aligned}\right.\]</span></p><h3 id="计算c_n">计算C_n</h3><p>将以下公式代入<span class="math inline">\(C_n\)</span> 。 <spanclass="math display">\[\begin{split}&amp; a_0 = \frac{1}{L}\int_{-L}^Lf(t)dt \\&amp; a_n = \frac{1}{L}\int_{-L}^Lf(t)coswt~dt \\&amp; b_n = \frac{1}{L}\int_{-L}^Lf(t)sinwt~dt \\\end{split}\]</span> <strong>当<spanclass="math inline">\(n=0\)</span>时：</strong> <spanclass="math display">\[\begin{split}C_0 &amp; = \frac{a_0}{2} = \frac{1}{2}\frac{2}{T}\int_{0}^{T}f(t)dt \\&amp; = \frac{1}{T}\int_{0}^{T}f(t)dt \\&amp; = \frac{1}{T}\int_{0}^{T}e^0f(t)dt\end{split}\]</span> <strong>当<spanclass="math inline">\(n&gt;0\)</span>时:</strong> <spanclass="math display">\[\begin{split}C_{n=1,2,3,...}&amp; =\frac{1}{2}(\frac{2}{T}\int_0^Tf(t)cosnwt~dt-i\frac{2}{T}\int_0^Tf(t)sinnwt~dt)\\&amp; = \frac{1}{T}\int_0^Tf(t)(cosnwt-isinnwt)dt \\&amp; = \frac{1}{T}\int_0^Tf(t)[cos(-nwt)+isin(-nwt)]dt\\&amp; = \frac{1}{T}\int_0^Tf(t)e^{-inwt}dt\end{split}\]</span></p><blockquote><p>注：上述第三步到第四步的变换用的欧拉公式。</p></blockquote><p><strong>当<span class="math inline">\(n&lt;0\)</span>时：</strong><span class="math display">\[\begin{split}C_{n=-1,-2,-3,...}&amp; =\frac{1}{2}[\frac{2}{T}\int_0^Tf(t)cos(-nwt)~dt+i\frac{2}{T}\int_0^Tf(t)sin(-nwt)~dt]\\&amp; = \frac{1}{T}\int_0^Tf(t)[cos(-nwt)+isin(-nwt)]dt\\&amp; = \frac{1}{T}\int_0^Tf(t)e^{-inwt}dt\end{split}\]</span> 于是，我们发现三种情况的<spanclass="math inline">\(C_n\)</span>都可以用一个式子表示，于是我们得出了傅里叶级数的复数形式。</p><h2 id="傅里叶级数的复数形式">傅里叶级数的复数形式</h2><p>现有周期为T的函数：<span class="math inline">\(f(t) =f(t+T)\)</span></p><p>它的傅里叶级数（复数形式）为： <span class="math display">\[\begin{split}&amp; f(t) = \sum_{-\infty}^{\infty}C_ne^{inwt} \\&amp; C_n = \frac{1}{T}\int_0^Tf(t)e^{-inwt}dt\end{split}\]</span>至此，傅里叶级数推导完毕。接下来，我们将开始寻找非周期函数的傅里叶变换。</p><h1 id="傅里叶变换">傅里叶变换</h1><p>终于，我们来到了最后一部分。</p><p>一个非周期函数可以理解为，一个周期为无穷的函数。</p><h2 id="从求和转为积分">从求和转为积分</h2><p>这部分原理涉及时域转频域的图像，最开始推荐的那篇原理文章已经详细解释过。这里就不细说了。将<spanclass="math inline">\(C_n\)</span> 投射到图像上，会发现当n越多，<spanclass="math inline">\(C_n\)</span> 就靠的越近。当n趋近无穷，<spanclass="math inline">\(C_n\)</span>会连成一块儿。于是求和可以变成积分。</p>如果这部分公式没看懂，建议去看参考教程的第五部分。 $$<span class="math display">\[\begin{split}&amp; f(t) =\sum_{-\infty}^{\infty}\frac{1}{T}\int_0^Tf(t)e^{-inwt}dte^{inwt} \\&amp; f(t) = \sum_{-\infty}^{\infty}\frac{\trianglew}{2\pi}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-inwt}dte^{inwt} \\&amp; f(t) =\frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(t)e^{-iwt}dt~e^{iwt}dw\end{split}\]</span><p>$$</p><h2 id="傅里叶变换公式">傅里叶变换公式</h2><p>我们令中间那部分为<spanclass="math inline">\(F(t)\)</span>,它就是傅里叶变换的公式。 <spanclass="math display">\[F(t) =\int_{-\infty}^{\infty}f(t)e^{-iwt}dt\]</span></p><h2 id="傅里叶逆变换公式">傅里叶逆变换公式</h2><p>然后原来的公式就变成了： <span class="math display">\[f(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}F(w)~e^{iwt}dw\]</span> 这个公式就是傅里叶逆变换公式。</p><hr /><p>至此，傅里叶变换和傅里叶逆变换公式已全部推导完毕。</p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正交变换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【matlab图像处理笔记1】matlab图像类型的分类与转换</title>
      <link href="/article/03d80dab2395/"/>
      <url>/article/03d80dab2395/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>本系列为使用matlab进行图像处理的工具类笔记，将很少涉及相关原理、公式与推导。重点在于函数应用上，此笔记的目的在于便于快速查阅和使用。</p><h1 id="matlab提供的图像类型">Matlab提供的图像类型</h1><p>matlab提供的图像类型主要包括以下四种：<strong>索引图像、灰度图像、RGB图像、二值图像</strong></p><h2 id="索引图像">索引图像</h2><p>索引图像包括一个数据矩阵X，一个颜色映射矩阵map。</p><blockquote><p>X：可以为uint8、uint16或double类型。</p><p>map：总是一个<span class="math inline">\(m\times 3\)</span>的double类型矩阵,且元素值位于<spanclass="math inline">\([0,1]\)</span>之间。每一列分别表示红（R）、绿（G）、蓝（B），每一行表示一个颜色。</p></blockquote><p>索引图像是从像素值到颜色映射表值的“直接映射”。像素颜色由数据矩阵X作为索引指向矩阵Map进行索引，例如，值1指向矩阵Map中的第一行，值2指向第二行，以此类推。</p><h2 id="灰度图像">灰度图像</h2><p>灰度图像一般由uint8、uint16或double类型的矩阵描述。通常0表示黑色，uint8中的255、uint16中的65535、double中的1表示白色。大于相应类型的值也为白色。</p><h2 id="rgb图像">RGB图像</h2><p>RGB图像又称真彩图像，它利用R、G、B三个分量来表示一个颜色，通过RGB三种基本颜色可以组合出任意颜色。对于一个<spanclass="math inline">\(M\times N\)</span>的图像，matlab将储存一个<spanclass="math inline">\(M\times N\times 3\)</span> 的数据矩阵。</p><p>RGB图像不是用windows的颜色映射表，像素的颜色由保存在像素位置上的红、绿、蓝的灰度值的组合来确定。图形文件格式把RGB图像存储为24位的图像，红、绿、蓝分别占8位，这样可以有1000多万种颜色。</p><h2 id="二值图像">二值图像</h2><p>与灰度图像类似，不过在数据矩阵中，灰度等级只有两种，即0或1。其中0代表黑色，1代表白色。</p><h1 id="图像类型的转换">图像类型的转换</h1><p>下面将介绍matlab中上述几种图像类型的转换函数。</p><blockquote><p>常用的类型转换函数：</p><ul><li>rgb2gray</li><li>gray2ind</li><li>rgb2ind</li><li>ind2gray</li><li>ind2rgb</li><li>im2bw(imbinarize)</li><li>grayslice</li></ul></blockquote><h2 id="rgb2gray">1.rgb2gray</h2><p><strong>作用：</strong>将RGB图像转化为灰度图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = rgb2gray(I)</span><br></pre></td></tr></table></figure><blockquote><ul><li>I：原RGB图像</li><li>X：转换后的灰度图像</li></ul></blockquote><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将RGB图像转换为灰度图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);</span><br><span class="line">title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">X = rgb2gray(I); </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X);</span><br><span class="line">title(<span class="string">&quot;gray img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091517629.png" /></p><h2 id="gray2ind">2.gray2ind</h2><p><strong>作用：</strong>将灰度图像或二值图像转换为索引图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[X,map] = gray2ind(I,n)</span><br></pre></td></tr></table></figure><blockquote><ul><li>I - 转换前的灰度或二值图像数据矩阵</li><li>n - 灰度级数</li><li>X - 转换后得到的索引数据矩阵</li><li>map - 转换后得到的颜色映射矩阵</li></ul></blockquote><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将灰度图像转化为4个灰度级的索引图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">I = rgb2gray(I); <span class="comment">% 将RGB图像转换为灰度图像</span></span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);</span><br><span class="line">title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">[X,map] = gray2ind(I,<span class="number">4</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X,map);</span><br><span class="line">title(<span class="string">&quot;gray(4) img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091524800.png" /></p><h2 id="rgb2ind">3.rgb2ind</h2><p><strong>作用：</strong>将RGB图像转换为索引图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[X,map] = rgb2ind(RGB,n)</span><br><span class="line">[X,map] = rgb2ind(RGB,tol)</span><br><span class="line">X = rgb2ind(RGB,inmap)</span><br><span class="line">___ = rgb2ind(___,dithering)</span><br></pre></td></tr></table></figure><p><strong>说明：</strong></p><blockquote><ul><li>[X,map] = rgb2ind(RGB,n) ：使用具有<code>n</code>种量化颜色的最小方差量化法并加入抖动，将 RGB图像转换为索引图像 <code>X</code>，关联颜色图为 <code>map</code>。</li><li>[X,map] = rgb2ind(RGB,tol)：使用均匀量化法并加入抖动，将 RGB图像转换为索引图像，容差为 <code>tol</code>。</li><li>X = rgb2ind(RGB,inmap) ：使用逆颜色图算法并加入抖动，将 RGB图像转换为索引图像，指定的颜色图为 <code>inmap</code>。</li><li>_ = rgb2ind(_,dithering)：启用或禁用抖动。</li></ul></blockquote><p>关于最小方差量化法、均匀量化法不在这里介绍。</p><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将RGB图像转化为包含4种颜色的索引图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);</span><br><span class="line">title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">[X,map] = rgb2ind(I,<span class="number">4</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X,map);</span><br><span class="line">title(<span class="string">&quot;rgb\_ind(4) img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091545394.png" /></p><h2 id="ind2gray">4.ind2gray</h2><p><strong>作用：</strong>将索引图像转换为灰度图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = ind2gray(I,map)</span><br></pre></td></tr></table></figure><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将索引图像转化为灰度图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">[I,map] = rgb2ind(I,<span class="number">4</span>);<span class="comment">% 将RGB图像转化为包含4种颜色的索引图像</span></span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I,map);</span><br><span class="line">title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">X = ind2gray(I,map);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X);</span><br><span class="line">title(<span class="string">&quot;gray img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091556346.png" /></p><h2 id="ind2rgb">5.ind2rgb</h2><p><strong>作用：</strong>将索引图像转换为RGB图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = ind2rgb(I,map)</span><br></pre></td></tr></table></figure><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将索引图像转化为RGB图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">[I,map] = rgb2ind(I,<span class="number">4</span>);<span class="comment">% 将RGB图像转化为包含4种颜色的索引图像</span></span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I,map);</span><br><span class="line">title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line"></span><br><span class="line">X = ind2rgb(I,map);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X);</span><br><span class="line">title(<span class="string">&quot;RGB img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091559482.png" /></p><h2 id="im2bwimbinarize">6.im2bw(imbinarize)</h2><p><strong>作用：</strong>通过阈值法将图像转化为二值图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BW = im2bw(I,level) <span class="comment">% I为灰度图像或RGB图像</span></span><br><span class="line">BW = im2bw(I,map,level) <span class="comment">% I为索引图像</span></span><br></pre></td></tr></table></figure><ul><li>level：阈值，取值<span class="math inline">\([0,1]\)</span>。</li></ul><blockquote><p>注：</p><p>在 <strong>matlab2018</strong> 中建议用 imbinarize来将图片转换为二值图，其参数必须为灰度图。</p><p>在 <strong>matlab2016 中</strong>，只有 im2bw函数，其参数可以是灰度图或 rgb 图。</p><p>在 <strong>matlab2016</strong> 中可以用 im2bw 直接将 rgb图像转换为二值图；</p><p>在 <strong>matlab2018</strong> 中若要使用imbinarize图片二值化，需要先将 rgb 图片转换为灰度图（用<strong>rgb2gray()</strong> 函数），再用 imbinarize 转换为二值图。</p></blockquote><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将RGB图像转化为二值图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);</span><br><span class="line">title(<span class="string">&quot;original RGB&quot;</span>);</span><br><span class="line">X_1 = im2bw(I,<span class="number">0.5</span>);</span><br><span class="line">subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X_1);</span><br><span class="line">title(<span class="string">&quot;RBG\_bw img&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 将灰度图像转化为二值图像</span></span><br><span class="line">I_2 = rgb2gray(I); <span class="comment">% 将RGB图像转化为灰度图像</span></span><br><span class="line">subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>),imshow(I_2);</span><br><span class="line">title(<span class="string">&quot;original gray&quot;</span>);</span><br><span class="line">X_2 = im2bw(I_2,<span class="number">0.5</span>);</span><br><span class="line">subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>),imshow(X_2);</span><br><span class="line">title(<span class="string">&quot;gray\_bw img&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 将索引图像转化为二值图像</span></span><br><span class="line">[I_3,map_3] = rgb2ind(I,<span class="number">4</span>);<span class="comment">% 将RGB图像转化为包含4种颜色的索引图像</span></span><br><span class="line">subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>),imshow(I_3,map_3);</span><br><span class="line">title(<span class="string">&quot;original index&quot;</span>);</span><br><span class="line">X_3 = im2bw(I_3,map_3,<span class="number">0.5</span>);</span><br><span class="line">subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">6</span>),imshow(X_3);</span><br><span class="line">title(<span class="string">&quot;index\_bw img&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091618713.png" /></p><h2 id="grayslice">7.grayslice</h2><p><strong>作用：</strong>通过多级阈值法将灰度图像转换为索引图像。</p><p><strong>用法：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = grayslice(I,n)</span><br><span class="line">X = grayslice(I,v)</span><br></pre></td></tr></table></figure><ul><li>n：将图像I的亮度均匀化为n个等级，默认64</li><li>v：按指定阈值向量v对图像I的亮度进行划分</li></ul><p><strong>示例：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 将灰度图像转化为8级索引图像</span></span><br><span class="line">clc;clear;close all;</span><br><span class="line">I = imread(<span class="string">&quot;example.jpg&quot;</span>);</span><br><span class="line">I = rgb2gray(I); <span class="comment">% 将RGB图像转化为灰度图像</span></span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),imshow(I);</span><br><span class="line">title(<span class="string">&quot;original&quot;</span>);</span><br><span class="line">X = grayslice(I,<span class="number">8</span>);</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),imshow(X,jet(<span class="number">8</span>)); <span class="comment">% jet:蓝头红尾饱和色</span></span><br><span class="line">title(<span class="string">&quot;index\_jet&quot;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202210091631803.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记12——【GMM 2】高斯混合模型实现与应用</title>
      <link href="/article/ca2465afd48c/"/>
      <url>/article/ca2465afd48c/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ol type="1"><li><ahref="https://www.issey.top/article/ee70792b2110/">机器学习笔记10——EM算法原理与详细推导| issey的博客</a></li><li><ahref="https://www.issey.top/article/4f4d2f7b3b7a/">机器学习笔记11——【GMM1】高斯混合模型原理与推导 | issey的博客</a></li></ol><h1 id="前言">前言</h1><p>前几天忙着开学的事，加上图像处理的学习，这篇文章稍微搁置了些时间。闲话不多说，直接进入正题。本篇对EM算法和高斯混合模型的推导过程不会再详细说明，相关部分请见前两篇文章。</p><h1 id="高斯混合模型的聚类步骤">高斯混合模型的聚类步骤</h1><p>回顾在上一篇文章结尾，GMM聚类步骤为：</p><p><strong>step1：</strong></p><p>定义高斯分布个数K，对每个高斯分布设置初始参数值<spanclass="math inline">\(\theta^{(0)}_k = \alpha_k,\mu_k,\Sigma_k\)</span>。<strong>一般第一步不会自己设置初始值，而是通过K-mean算法计算初始值。</strong></p><p><strong>step2 E-step：</strong></p><p>根据当前的参数<span class="math inline">\(\theta^{(t)}\)</span>,计算每一个隐变量的后验概率分布<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>。精确到每一个后验概率的计算，有</p><p><span class="math display">\[\gamma_t(z_k^{(i)}) =\frac{\alpha_k^{(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}\]</span></p><blockquote><p>这里解释一下：</p><ul><li><span class="math inline">\(\gamma_t(z_k^{(i)})\)</span>：在第t轮时，第i个样本的隐变量z的第k个概率。或者说，在第t轮时，第i个样本属于第k个高斯分布的后验概率。</li><li><spanclass="math inline">\(\alpha_k^{(t)}\)</span>：在第t轮时，<strong>已固定的</strong>第k个高斯分布的权值。</li><li><span class="math inline">\(N(x|\mu_k^{(t)},\Sigma_k^{(t)})\)</span>：在第t轮时，第k个<strong>已固定的</strong>多维高斯分布概率密度函数。</li><li>带上标(t)的都是<strong>已固定参数</strong>。</li></ul></blockquote><p><strong>step3 M-step：</strong></p><p>根据E-step计算出的隐变量后验概率分布，进一步计算新的<spanclass="math inline">\(\theta^{(t+1)}\)</span></p><p><span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p><span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><strong>step4:</strong></p><p>循环E-step和M-step直至收敛。</p><h1 id="代码实现">代码实现</h1><h2 id="前置说明">前置说明</h2><p>由于多维高斯分布概率密度函数对于个人而言不太好实现（其实是因为没学，可能后面会开一篇文章讲多维高斯分布概率密度函数的计算），所以这里就以服从一维高斯分布的数据作为手动代码实现的训练集。即训练集只有一个特征。</p><h2 id="数据生成">数据生成</h2><p>这里使用sklearn的make_blobs函数生成数据。</p><h3 id="make_blobs聚类数据生成器">make_blobs聚类数据生成器</h3><p>scikit中的make_blobs方法常被用来生成聚类算法的测试数据，make_blobs会根据用户指定的特征数量、中心点数量、范围等来生成几类数据。</p><blockquote><p>其中：</p><ul><li>n_samples是待生成的样本的总数</li><li>n_features是每个样本的特征数</li><li>centers表示聚类中心点个数，可以理解为种类数。当centers为列表时，为每个类别指定中心位置。</li><li>cluster_std设置每个类别的方差</li><li>random_state是随机种子</li></ul></blockquote><p>下面将通过两个例子来直观说明make_blobs函数。</p><p>绘制数据时会用到plt.hist函数，详见<ahref="https://blog.csdn.net/ilovegem/article/details/109702151">python--plt.hist函数的输入参数和返回值的解释</a>。</p><hr /><ol type="1"><li>生成1000个数据：特征只有一列，分为两簇，两簇的方差为0.5和1：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets._samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成样本集</span></span><br><span class="line"></span><br><span class="line">centers = <span class="number">2</span></span><br><span class="line">X,y = make_blobs(n_samples=<span class="number">1000</span>,n_features=<span class="number">1</span>,centers=centers,cluster_std=[<span class="number">0.5</span>,<span class="number">1</span>],random_state=<span class="number">100</span>)</span><br><span class="line">plt.hist(X,bins = <span class="number">100</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://img.issey.top/img/202210031337857.png"alt="image-20221003133732791" /><figcaption aria-hidden="true">image-20221003133732791</figcaption></figure><ol start="2" type="1"><li>生成1000个数据：特征只有一列，分为两簇，两簇中心点为2和5，方差为0.5和1：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets._samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成样本集</span></span><br><span class="line">centers = [[<span class="number">3</span>],[<span class="number">5</span>]]</span><br><span class="line">X,y = make_blobs(n_samples=<span class="number">1000</span>,n_features=<span class="number">1</span>,centers=centers,cluster_std=[<span class="number">0.5</span>,<span class="number">1</span>],random_state=<span class="number">100</span>)</span><br><span class="line">plt.hist(X,bins = <span class="number">100</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure><img src="https://img.issey.top/img/202210031424635.png"alt="image-20221003142413580" /><figcaption aria-hidden="true">image-20221003142413580</figcaption></figure><p>可以看到上图为两个一维高斯分布混在了一起。接下来将使用上图所示数据集作为我们手动实现GMM的训练集。</p><h2id="gmm算法手动实现仅对于一维高斯分布">GMM算法手动实现（仅对于一维高斯分布）</h2><h2 id="e-step">E-step</h2><p>对于一维高斯分布概率密度函数，求解： <span class="math display">\[\gamma_t(z_k^{(i)}) =\frac{\alpha_k^{(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}\]</span> 其中， <span class="math display">\[N(x|\mu,\Sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span>本来打算把一维高斯分布概率密度函数写成代码的形式，但是途中发现了scipy科学计算包，这里就顺带理一下scipy中的高斯分布使用方法：<ahref="https://blog.csdn.net/qq_36056219/article/details/112118602">【Python笔记】Scipy.stats.norm函数解析</a>。相关参数解释都在这篇文章里了，建议先看文章。当然如果觉得太麻烦了，就直接自己把上面的概率密度函数变成代码形式就好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">E_step</span>(<span class="params">data,theta</span>):</span><br><span class="line">    Rz = []</span><br><span class="line">    <span class="comment"># 计算分子</span></span><br><span class="line">    Rz1_up = theta[<span class="string">&#x27;w1&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu1&#x27;</span>],theta[<span class="string">&#x27;sigma1&#x27;</span>]).pdf(data)</span><br><span class="line">    Rz2_up = theta[<span class="string">&#x27;w2&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu2&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>]).pdf(data)</span><br><span class="line">    <span class="comment"># 分母</span></span><br><span class="line">    Rz_down = theta[<span class="string">&#x27;w1&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu1&#x27;</span>],theta[<span class="string">&#x27;sigma1&#x27;</span>]).pdf(data)+theta[<span class="string">&#x27;w2&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu2&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>]).pdf(data)</span><br><span class="line">    <span class="comment"># 第K个隐变量的后验概率</span></span><br><span class="line">    Rz1 = Rz1_up/Rz_down</span><br><span class="line">    Rz2 = Rz2_up/Rz_down</span><br><span class="line">    Rz.append(Rz1)</span><br><span class="line">    Rz.append(Rz2)</span><br><span class="line">    Rz = np.array(Rz)</span><br><span class="line">    <span class="keyword">return</span> Rz</span><br></pre></td></tr></table></figure><h2 id="m-step">M-step</h2><p>计算：</p><p><span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p><span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p>前两个没什么好说明的，最后一个需要注意：因为示例是一维的，所以<spanclass="math inline">\(x_n-\mu_k\)</span>是数字，倒置也是他自己，所以分子后面那一块变成了求平方。另外，<spanclass="math inline">\(\Sigma\)</span> 是<spanclass="math inline">\(\sigma^2\)</span>,求出来后需要开平方才是我们需要的<spanclass="math inline">\(\sigma\)</span>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">M_step</span>(<span class="params">data,theta,Rz,n</span>):</span><br><span class="line">    theta[<span class="string">&#x27;w1&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>])/n</span><br><span class="line">    theta[<span class="string">&#x27;w2&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>])/n</span><br><span class="line">    theta[<span class="string">&#x27;mu1&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>]*data)/np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>])</span><br><span class="line">    theta[<span class="string">&#x27;mu2&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>]*data)/np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>])</span><br><span class="line">    Sigma1 = np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>]*np.square(data-theta[<span class="string">&#x27;mu1&#x27;</span>]))/(np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>]))</span><br><span class="line">    Sigma2 = np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>]*np.square(data-theta[<span class="string">&#x27;mu2&#x27;</span>]))/(np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>]))</span><br><span class="line">    theta[<span class="string">&#x27;sigma1&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>] = np.sqrt(Sigma1),np.sqrt(Sigma2)</span><br></pre></td></tr></table></figure><h2 id="初始化未知参数以及迭代">初始化未知参数以及迭代</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_GMM</span>(<span class="params">data,times</span>):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    theta = &#123;&#125;</span><br><span class="line">    theta[<span class="string">&#x27;w1&#x27;</span>],theta[<span class="string">&#x27;w2&#x27;</span>] = <span class="number">0.5</span>,<span class="number">0.5</span></span><br><span class="line">    theta[<span class="string">&#x27;mu1&#x27;</span>],theta[<span class="string">&#x27;mu2&#x27;</span>] = <span class="number">0</span>,<span class="number">10</span></span><br><span class="line">    theta[<span class="string">&#x27;sigma1&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>] = <span class="number">0.8</span>,<span class="number">0.8</span></span><br><span class="line">    n = <span class="built_in">len</span>(data)</span><br><span class="line">    data = np.array(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(times):</span><br><span class="line">        Rz = E_step(data,theta)</span><br><span class="line">        M_step(data,theta,Rz,n)</span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(theta)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;=========&quot;</span>)</span><br></pre></td></tr></table></figure><p>然后我们可以运行看看效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成样本集</span></span><br><span class="line">    centers = [[<span class="number">3</span>], [<span class="number">5</span>]]</span><br><span class="line">    X_train, y_test = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">1</span>, centers=centers, cluster_std=[<span class="number">0.5</span>, <span class="number">1</span>], random_state=<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># plt.hist(X, bins=100)</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line">    run_GMM(data=X_train,times=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>可以看到，最后GMM收敛到了下面所示的值：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;w1&#x27;</span>: 0.467708978482864, <span class="string">&#x27;w2&#x27;</span>: 0.5322910215171359, <span class="string">&#x27;mu1&#x27;</span>: 2.9671070095836107, <span class="string">&#x27;mu2&#x27;</span>: 4.878609055231818, <span class="string">&#x27;sigma1&#x27;</span>: 0.4970731536722904, <span class="string">&#x27;sigma2&#x27;</span>: 1.0992431014967026&#125;</span><br><span class="line">=========</span><br></pre></td></tr></table></figure><p>对比我们生成数据时的各个参数，发现mu和sigma差别都不大。它成功的从高斯混合模型中分离了两个高斯模型。并且与我们最初设置这两个高斯模型的参数差别不大。</p><h2 id="绘制可视化动态图像">绘制可视化动态图像</h2><p>这部分是可选内容，觉得比较好玩的可以试试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">draw_GMM</span>(<span class="params">thetas</span>):</span><br><span class="line">    x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)  <span class="comment"># 设置画图范围</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># ax.hist(X_train, bins=100)</span></span><br><span class="line">    plt.ion()</span><br><span class="line">    <span class="keyword">for</span> theta <span class="keyword">in</span> thetas:</span><br><span class="line">        y_1 = stats.norm(theta[<span class="string">&#x27;mu1&#x27;</span>], theta[<span class="string">&#x27;sigma1&#x27;</span>]).pdf(x)</span><br><span class="line">        y_2 = stats.norm(theta[<span class="string">&#x27;mu2&#x27;</span>], theta[<span class="string">&#x27;sigma2&#x27;</span>]).pdf(x)</span><br><span class="line">        lines1 = ax.plot(x, y_1,c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">        lines2 = ax.plot(x, y_2,c=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">        plt.pause(<span class="number">0.3</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            ax.lines.remove(lines1[<span class="number">0</span>])</span><br><span class="line">            ax.lines.remove(lines2[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="完整代码">完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets._samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_GMM</span>(<span class="params">thetas</span>):</span><br><span class="line">    x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)  <span class="comment"># 设置画图范围</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># ax.hist(X_train, bins=100)</span></span><br><span class="line">    plt.ion()</span><br><span class="line">    <span class="keyword">for</span> theta <span class="keyword">in</span> thetas:</span><br><span class="line">        y_1 = stats.norm(theta[<span class="string">&#x27;mu1&#x27;</span>], theta[<span class="string">&#x27;sigma1&#x27;</span>]).pdf(x)</span><br><span class="line">        y_2 = stats.norm(theta[<span class="string">&#x27;mu2&#x27;</span>], theta[<span class="string">&#x27;sigma2&#x27;</span>]).pdf(x)</span><br><span class="line">        lines1 = ax.plot(x, y_1,c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">        lines2 = ax.plot(x, y_2,c=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">        plt.pause(<span class="number">0.3</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            ax.lines.remove(lines1[<span class="number">0</span>])</span><br><span class="line">            ax.lines.remove(lines2[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">E_step</span>(<span class="params">data,theta</span>):</span><br><span class="line">    Rz = []</span><br><span class="line">    <span class="comment"># 计算分子</span></span><br><span class="line">    Rz1_up = theta[<span class="string">&#x27;w1&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu1&#x27;</span>],theta[<span class="string">&#x27;sigma1&#x27;</span>]).pdf(data)</span><br><span class="line">    Rz2_up = theta[<span class="string">&#x27;w2&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu2&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>]).pdf(data)</span><br><span class="line">    <span class="comment"># 分母</span></span><br><span class="line">    Rz_down = theta[<span class="string">&#x27;w1&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu1&#x27;</span>],theta[<span class="string">&#x27;sigma1&#x27;</span>]).pdf(data)+theta[<span class="string">&#x27;w2&#x27;</span>]*stats.norm(theta[<span class="string">&#x27;mu2&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>]).pdf(data)</span><br><span class="line">    <span class="comment"># 第K个隐变量的后验概率</span></span><br><span class="line">    Rz1 = Rz1_up/Rz_down</span><br><span class="line">    Rz2 = Rz2_up/Rz_down</span><br><span class="line">    Rz.append(Rz1)</span><br><span class="line">    Rz.append(Rz2)</span><br><span class="line">    Rz = np.array(Rz)</span><br><span class="line">    <span class="keyword">return</span> Rz</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">M_step</span>(<span class="params">data,theta,Rz,n</span>):</span><br><span class="line">    theta[<span class="string">&#x27;w1&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>])/n</span><br><span class="line">    theta[<span class="string">&#x27;w2&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>])/n</span><br><span class="line">    theta[<span class="string">&#x27;mu1&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>]*data)/np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>])</span><br><span class="line">    theta[<span class="string">&#x27;mu2&#x27;</span>] = np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>]*data)/np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>])</span><br><span class="line">    Sigma1 = np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>]*np.square(data-theta[<span class="string">&#x27;mu1&#x27;</span>]))/(np.<span class="built_in">sum</span>(Rz[<span class="number">0</span>]))</span><br><span class="line">    Sigma2 = np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>]*np.square(data-theta[<span class="string">&#x27;mu2&#x27;</span>]))/(np.<span class="built_in">sum</span>(Rz[<span class="number">1</span>]))</span><br><span class="line">    theta[<span class="string">&#x27;sigma1&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>] = np.sqrt(Sigma1),np.sqrt(Sigma2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_GMM</span>(<span class="params">data,times</span>):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    theta = &#123;&#125;</span><br><span class="line">    draw_theta = []</span><br><span class="line">    theta[<span class="string">&#x27;w1&#x27;</span>],theta[<span class="string">&#x27;w2&#x27;</span>] = <span class="number">0.5</span>,<span class="number">0.5</span></span><br><span class="line">    theta[<span class="string">&#x27;mu1&#x27;</span>],theta[<span class="string">&#x27;mu2&#x27;</span>] = <span class="number">0</span>,<span class="number">10</span></span><br><span class="line">    theta[<span class="string">&#x27;sigma1&#x27;</span>],theta[<span class="string">&#x27;sigma2&#x27;</span>] = <span class="number">0.8</span>,<span class="number">0.8</span></span><br><span class="line">    n = <span class="built_in">len</span>(data)</span><br><span class="line">    data = np.array(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(times):</span><br><span class="line">        Rz = E_step(data,theta)</span><br><span class="line">        M_step(data,theta,Rz,n)</span><br><span class="line">        <span class="keyword">if</span> t&lt;=<span class="number">50</span>:</span><br><span class="line">            <span class="built_in">print</span>(theta)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;=========&quot;</span>)</span><br><span class="line">            draw_theta.append(theta.copy())</span><br><span class="line">    draw_GMM(draw_theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成样本集</span></span><br><span class="line">    centers = [[<span class="number">3</span>], [<span class="number">5</span>]]</span><br><span class="line">    X_train, y_test = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">1</span>, centers=centers, cluster_std=[<span class="number">0.5</span>, <span class="number">1</span>], random_state=<span class="number">100</span>)</span><br><span class="line">    plt.hist(X_train,<span class="number">100</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    run_GMM(data=X_train,times=<span class="number">650</span>)</span><br></pre></td></tr></table></figure><h2 id="可视化演示">可视化演示</h2><p>原始数据：</p><figure><img src="https://img.issey.top/img/202210031424635.png"alt="image-20221003142413580" /><figcaption aria-hidden="true">image-20221003142413580</figcaption></figure><p>训练结果：</p><figure><img src="https://img.issey.top/img/202210032155191.gif"alt="1664799029453" /><figcaption aria-hidden="true">1664799029453</figcaption></figure><h1 id="使用sklearn的gmm">使用sklearn的GMM</h1><blockquote><p>又到了令人愉悦的部分了。</p><p>这部分将使用sklearn的GMM来实现聚类，为了便于画图，这里特征只选两个。即二维高斯混合模型。另外顺带一提，如果你稍微查看了sklearn中GMM的源码，就会发现它在初始化时就是先用了一次Kmean。</p></blockquote><p>关于sklearn的GMM用法就不多说了，推荐一篇文章：<ahref="https://blog.csdn.net/jasonzhoujx/article/details/81947663">Scikit-Learn学习笔记——高斯混合模型(GMM)应用：分类、密度估计、生成模型_盐味橙汁的博客-CSDN博客_高斯混合模型分类</a></p><p>这篇文章还说明了GMM在密度估计、生成模型上的应用。</p><h2 id="代码">代码</h2><p>注：绘制<strong>边缘椭圆</strong>部分搬的是这篇文章的代码，因为本人还不太会。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Ellipse</span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets._samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">creat_data</span>(<span class="params">centers,size = <span class="number">1000</span></span>):</span><br><span class="line">    X,labels = make_blobs(n_samples=size,n_features=<span class="number">2</span>,centers=centers,cluster_std=<span class="number">1</span>,random_state=<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 把X训练集变为椭圆的形式</span></span><br><span class="line">    rng = np.random.RandomState(<span class="number">12</span>)</span><br><span class="line">    X = np.dot(X, rng.randn(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> X,labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_ellipse</span>(<span class="params">position, covariance, ax=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用给定的位置和协方差画一个椭圆&quot;&quot;&quot;</span></span><br><span class="line">    ax = ax <span class="keyword">or</span> plt.gca()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将协方差转换为主轴</span></span><br><span class="line">    <span class="keyword">if</span> covariance.shape == (<span class="number">2</span>, <span class="number">2</span>):</span><br><span class="line">        U, s, Vt = np.linalg.svd(covariance)</span><br><span class="line">        angle = np.degrees(np.arctan2(U[<span class="number">1</span>, <span class="number">0</span>], U[<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line">        width, height = <span class="number">2</span> * np.sqrt(s)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        angle = <span class="number">0</span></span><br><span class="line">        width, height = <span class="number">2</span> * np.sqrt(covariance)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#画出椭圆</span></span><br><span class="line">    <span class="keyword">for</span> nsig <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>):</span><br><span class="line">        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_gmm</span>(<span class="params">gmm, X,labels, label=<span class="literal">True</span>, ax=<span class="literal">None</span></span>):</span><br><span class="line">    ax = ax <span class="keyword">or</span> plt.gca()</span><br><span class="line">    <span class="keyword">if</span> label:</span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, s=<span class="number">40</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>, zorder=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], s=<span class="number">40</span>, zorder=<span class="number">2</span>)</span><br><span class="line">    w_factor = <span class="number">0.2</span> / gmm.weights_.<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">for</span> pos, covar, w <span class="keyword">in</span> <span class="built_in">zip</span>(gmm.means_, gmm.covariances_, gmm.weights_):</span><br><span class="line">        draw_ellipse(pos, covar, alpha=w * w_factor)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    centers = <span class="number">5</span></span><br><span class="line">    X,labels = creat_data(centers = centers)</span><br><span class="line">    <span class="comment"># 创建并训练高斯混合模型</span></span><br><span class="line">    model = GaussianMixture(n_components = centers,covariance_type=<span class="string">&#x27;full&#x27;</span>)</span><br><span class="line">    model.fit(X)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    labels_predict = model.predict(X)</span><br><span class="line">    <span class="comment"># 绘制图像</span></span><br><span class="line">    plot_gmm(model, X,labels_predict)</span><br></pre></td></tr></table></figure><p>原始数据：</p><figure><img src="https://img.issey.top/img/202210042024233.png"alt="image-20221004202410089" /><figcaption aria-hidden="true">image-20221004202410089</figcaption></figure><p>经过聚类后的数据：</p><figure><img src="https://img.issey.top/img/202210042024805.png"alt="image-20221004202456739" /><figcaption aria-hidden="true">image-20221004202456739</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类算法 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于hive的启动和连接</title>
      <link href="/article/a54e99ba2904/"/>
      <url>/article/a54e99ba2904/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>​太久没用hive了，今天想重新熟悉一下，结果发现自己甚至忘记了怎么启动。。于是特此记录篇笔记，便于以后忘记时查阅，不会写的太细。</p><p>​顺便从这篇文章开始改变自己文章的格式，以前都是乱整，想稍微更好看一点。</p><h1 id="hive的启动与连接">hive的启动与连接</h1><h2 id="启动hadoop">启动hadoop</h2><p>​以root权限登录hadoop中心节点计算机（亲测用户登录不能启动hadoop），使用以下命令开启hadoop集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>​ 可以通过以下代码查看是否成功启动hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>​ 输出长这样：</p><p><img src="https://img.issey.top/img/202209202106612.png" /></p><p>​这时候就可以访问hadoop网页了：http://ip:9870，ip为你hadoop中心节点计算机ip。</p><p>​ 顺便提一下yarn的默认端口：8088</p><h2 id="启动hive">启动hive</h2><p>​这一步与许多教程不一样，可能是版本原因。root登录你安装hive的计算机，我的就在hadoop中心计算机上，在没有配置环境变量的情况下，进入hive安装目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/hive-3.1.2/</span><br></pre></td></tr></table></figure><p>​ 然后启动hivemetastore服务，这一步可以后台启动也可以前台启动。我现在使用前台启动，因为可以看到日志。使用前台启动之后这个命令框就不能动了。接下来开另一个命令框，如果不想再开命令框，可以选择后台启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前台启动：bin/hive --service metastore</span><br><span class="line">后台启动：<span class="built_in">nohup</span> bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>​在启动metastore服务后，同样在hive安装目录下接着启动hiveserver2服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前台启动：bin/hive --service hiveserver2</span><br><span class="line">后台启动：<span class="built_in">nohup</span> bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><p>​ 如果是前台启动，成功后会看到Hive sessionID过十几秒会变一个。到此hive服务完全启动。接下来就是hive连接了。关于hive连接没啥好注意的，和mysql连接差不多。不过注意hive默认是不需要密码的。所以账号输root，密码填空就行。测试链接，如果没问题的话hiveserver2那边的命令框会跳出"OK"。</p><h2 id="一些注意事项">一些注意事项</h2><p>​今天在启动hive服务时出现了连接不上的情况，经过分析发现是因为我前台启动然后ctrl+z（因为用的Xshell所以是ctrl+z,等同ctrl+c）后程序并没有被完全杀死。然后我又开了一个后台启动，就出错了。所以退出服务后一定要检查jps，如果程序还在一定要先kill-9。</p><p>​正常情况下，开启了metastore和hiveserver2后jps显示的只有两个Runjar。如果不是两个说明可能开多了。</p><p><img src="https://img.issey.top/img/202209202128688.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记11——【GMM 1】高斯混合模型原理与推导</title>
      <link href="/article/4f4d2f7b3b7a/"/>
      <url>/article/4f4d2f7b3b7a/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><p>前置文章：</p><ol type="1"><li><ahref="https://www.issey.top/article/ee70792b2110/">机器学习笔记10——EM算法原理与详细推导| issey的博客</a></li></ol><p>参考教程：</p><ul><li><p><ahref="https://www.bilibili.com/video/BV1aE411o7qd/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&amp;vd_source=747540861ba5c41c17852ccf069029f5">【机器学习】【白板推导系列】【合集1～33】</a></p></li><li><p><ahref="https://zhuanlan.zhihu.com/p/85338773">高斯混合模型（GMM）推导及实现</a></p></li><li><p><ahref="https://blog.csdn.net/qq_43753525/article/details/111770010">高斯混合模型(GMM)推导_Ziconin广工的博客-CSDN博客_</a></p></li></ul><h1 id="前言">前言</h1><p>之前在推GMM公式时有点问题但是自己没有发现，所以现在重新写一遍GMM的推导。这篇文章涉及EM算法的部分就不过多赘述，上篇文章已经推导过EM算法。</p><h1 id="高斯混合模型简介">高斯混合模型简介</h1><p>高斯混合模型（<strong>G</strong>aussian <strong>M</strong>ixed<strong>M</strong>odel）简称GMM，指多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布。高斯混合模型通常用于解决同一集合下的数据包含多个不同的分布的情况，具体应用有聚类、密度估计、生成新数据等。</p><h1 id="gmm与k-mean">GMM与K-mean</h1><p>根据K-mean聚类算法的原理，K-mean算法的缺点之一在于<strong>无法将两个聚类中心点相同的类进行聚类</strong>，比如<spanclass="math inline">\(A\sim N(\mu,\sigma_1^2),B\simN(\mu,\sigma^2_2)\)</span>,此时将无法用K-mean算法聚类出A，B。为了解决这一缺点，提出了高斯混合模型（GMM）。GMM通过选择成分最大化后验概率完成聚类，各数据点的后验概率表示属于各类的可能性，而不是判定它完全属于某个类，所以称为<strong>软聚类</strong>。其在各类尺寸不同、聚类间有相关关系的时候可能比k-means聚类更合适。</p><h1 id="高斯混合模型的概率密度函数">高斯混合模型的概率密度函数</h1><p>下面将分别从几何模型角度和混合模型角度分别解释GMM的概率密度函数。</p><h2 id="几何角度">几何角度</h2><p>假设我们现在有以下数据分布。</p><p><img src="https://img.issey.top/img/202209191338651.jpg" /></p><p>上述图像中，红色曲线为数据分布。可以发现，如果仅用图中任一单个高斯分布来表示红色曲线是不合适的。因此，我们可以将图中两个高斯分布进行<strong>加权平均</strong>得到一个新的分布。而这个分布就是高斯混合模型。</p><p>于是，从几何模型来看，GMM的概率密度函数可表示为<strong>若干个高斯分布的加权平均</strong>：<span class="math display">\[p(x) = \sum_{k=1}^N\alpha_kN(x|\mu_k,\Sigma_k),\sum_{k=1}^N\alpha_k=1\]</span> 上述公式中，<span class="math inline">\(\alpha_k\)</span>为权值。</p><h2 id="混合模型角度">混合模型角度</h2><p>这次我们来观察一个二维高斯混合模型数据分布。</p><figure><img src="https://img.issey.top/img/202210052001357.png"alt="QQ图片20221005195822" /><figcaption aria-hidden="true">QQ图片20221005195822</figcaption></figure><p>黑色线框为两个高斯分布的等高线（从上往下看的投影）。现在在图中任取一样本点，考虑它分别属于这两个高斯分布的概率。比如你可以说其中一个样本属于第一个高斯分布的概率为0.8，属于第二个高斯分布的概率为0.2。注意，这里说的是<strong>后验概率</strong>。</p><p>现在我们引入隐变量z，<strong>用于表示对应的样本属于哪一个高斯分布</strong>。有一点需要特别注意，<strong>对于每一个样本，都有自己的隐变量。或者说，z是对于个体而言而非整体而言的。</strong></p><p><span class="math inline">\(z=c_i\)</span>表示样本属于第i类，<spanclass="math inline">\(P(z=c_i)\)</span>是隐变量的概率分布。</p><table><thead><tr class="header"><th><span class="math inline">\(z\)</span></th><th><span class="math inline">\(c_1\)</span></th><th><span class="math inline">\(c_2\)</span></th><th>...</th><th><span class="math inline">\(c_k\)</span></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(P\)</span></td><td><span class="math inline">\(p_1\)</span></td><td><span class="math inline">\(p_2\)</span></td><td>...</td><td><span class="math inline">\(p_k\)</span></td></tr></tbody></table><p>引入隐变量z后，有： <span class="math display">\[\begin{split}p(x|\theta) &amp;= \sum_{k=1}^Kp(x,z=c_k|\theta)\\&amp;= \sum_{k=1}^Kp(z=c_k|\theta)p(x|z=c_k,\theta)\\&amp;= \sum_{k=1}^KP_kN(x|\mu_k,\Sigma_k),\sum_{k=1}^KP_k=1\end{split}\]</span> 综合两个角度，高斯混合模型概率密度函数可以表示为：</p><ul><li>几何角度：<span class="math inline">\(p(x) =\sum_{k=1}^N\alpha_kN(x|\mu_k,\Sigma_k),\sum_{k=1}^N\alpha_k=1\)</span></li><li>混合模型角度：<span class="math inline">\(p(x|\theta) =\sum_{k=1}^KP_kN(x|\mu_k,\Sigma_k),\sum_{k=1}^KP_k=1\)</span></li></ul><p>可以看出，二者的表示是一样的。在几何角度中，<spanclass="math inline">\(\alpha_k\)</span>表示权值；在混合模型角度中，<spanclass="math inline">\(P_k\)</span>表示隐变量的概率分布。</p><h2 id="可能会弄混的地方">可能会弄混的地方</h2><h3id="隐变量的分布与隐变量的后验概率分布">隐变量的分布与隐变量的后验概率分布</h3><blockquote><p>我在这儿被绕晕了好久，也是重新写这篇文章的直接原因。</p><p>我主要是被<strong>隐变量的后验分布</strong>和<strong>隐变量的分布</strong>弄晕了，在这里记录一下。</p></blockquote><p>回顾GMM中隐变量的含义是：某样本属于哪一类高斯分布。</p><p>因此M-step中，求隐变量的概率分布<spanclass="math inline">\(a_k\)</span>类似抛硬币计算正面朝上的概率类似，比如抛10次硬币6次朝上。不过在EM算法中，隐变量并不通过个确定的值计算概率分布，而是通过隐变量后验概率分布计算的，这部分内容在上一篇文章（EM算法推导）中具体举例说明过。</p><p>而E-step中,求的是隐变量的后验概率分布，所以才会有<strong>每一个样本都对应一个隐变量的（后验）概率分布。</strong></p><h1 id="极大似然估计">极大似然估计</h1><blockquote><p>该部分是为了引出EM算法，可以跳过。</p></blockquote><p>现在我们要用已知样本估计k个高斯分布的参数，一般通过样本估计模型参数的方法为极大似然估计（MLE），MLE在EM算法中已经讲过。</p><p>回顾MLE目标函数: <span class="math display">\[\begin{split}\hat\theta&amp; = argmax_\theta  logP(X|\theta) \\ &amp;=argmax_\theta  log \prod_{i=1}^np(x_i|\theta)\\ &amp;=argmax_\theta  \sum_{i=1}^n logp(x_i|\theta)\end{split}\]</span> 将高斯混合模型的概率密度函数代入，得： <spanclass="math display">\[\begin{split}\hat\theta &amp;= argmax_\theta \sum_{i=1}^nlogp(x_i|\theta)\\&amp;=argmax_\theta\sum_{i=1}^nlog\sum_{k=1}^K\alpha_kN(x_i|\mu_k,\Sigma_k)\end{split}\]</span> 因为引入了隐变量，导致这个式子含有<spanclass="math inline">\(log\sum\)</span>，无法再进行MLE下一个步骤。回顾EM算法，EM算法就是拿来求解此类问题的。于是接下来需要用EM迭代求近似解。</p><h1 id="em算法求近似解">EM算法求近似解</h1><h2 id="明确变量和参数">明确变量和参数</h2><p><span class="math inline">\(X\)</span>：可观测数据集，<spanclass="math inline">\(X = (x_1,x_2,...,x_n)\)</span></p><p><span class="math inline">\(Z\)</span>：未观测数据集，<spanclass="math inline">\(Z=(z_1,z_2,...,z_n)\)</span></p><p><span class="math inline">\(\theta\)</span>：模型参数，<spanclass="math inline">\(\theta = (\alpha,\mu,\Sigma)\)</span></p><p>参数<spanclass="math inline">\(\theta\)</span>包含隐变量z的概率分布，各个高斯分布的均值和协方差矩阵：</p><p><span class="math inline">\(\alpha =(\alpha_1,\alpha_2,...,\alpha_k)\)</span></p><p><span class="math inline">\(\mu =(\mu_1,\mu_2,...,\mu_k)\)</span></p><p><span class="math inline">\(\Sigma =(\Sigma_1,\Sigma_2,...,\Sigma_k)\)</span></p><h2 id="e-step">E-step</h2><p>回顾EM算法，上一篇中我们推导出E-step实际上求的是隐变量的后验概率分布<spanclass="math inline">\(p(z_i|x_i,\theta^{(t)})\)</span>。</p><p>为了便于表示，之后也将<spanclass="math inline">\(p(z_i|x_i,\theta^{(t)})\)</span>表示为<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>。将i变成上标的形式是为了之后便于将<span class="math inline">\(z_i =c_k\)</span>简写为<span class="math inline">\(z_k^{(i)}\)</span>。</p><p>隐变量的后验概率分布： <span class="math display">\[\begin{split}\gamma_t(z_j^{(i)}) &amp;= p(z_i = c_j|x_i,\theta^{(t)})\\&amp; =\frac{p(x_i,z_i=c_j|\theta^{(t)})}{\sum_{k=1}^Kp(x_i,z_i=c_k|\theta^{(t)})}\\&amp;=\frac{p(x_i|z_i=c_j,\theta^{(t)})p(z_i=c_j|\theta^{(t)})}{\sum_{k=1}^Kp(x_i|z_i=c_k,\theta^{(t)})p(z_i=c_k|\theta^{(t)})}\\&amp;=\frac{\alpha_j^{(t)}N(x_i|\mu_j^{(t)},\Sigma_j^{(t)})}{\sum_{k=1}^Ka_k^{(t)}N(x_i|\mu_k^{(t)},\Sigma_k^{(t)})}\end{split}\]</span></p><h2 id="简化q函数">简化Q函数</h2><p>回顾E步的Q函数： <span class="math display">\[\begin{split}Q(\theta,\theta^{(t)})&amp; =E_z[logP(X,Z|\theta)|X,\theta^{(t)}] \\&amp;=\sum_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})\end{split}\]</span> 注：$_Z $ 是<spanclass="math inline">\(\sum_{z_1,z_2,...,z_n}\)</span>的简写。</p><p>因为高斯混合模型中的完整数据<spanclass="math inline">\((X,Z)\)</span> 独立同分布，未观测数据<spanclass="math inline">\(Z\)</span> 独立同分布，所以： <spanclass="math display">\[\begin{split}Q(\theta,\theta^{(t)}) &amp;=\sum_Zlog \prod_{i=1}^np(x_i,z_{i}|\theta)\prod _{i=1}^np(z_{i}|x_i,\theta^{(t)})\\&amp;=\sum_Z[ \sum_{i=1}^nlog p(x_i,z_{i}|\theta) ]\prod_{i=1}^np(z_{i}|x_i,\theta^{(t)})\end{split}\]</span></p><blockquote><p>关于最前面那个<span class="math inline">\(\sum_Z\)</span>的解释：</p><p>如果是是连续型函数，Q的表达式应该是：</p><p><span class="math inline">\(Q(\theta,\theta^{(t)}) =\int_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})dz\)</span></p><p>但是现在是离散型，所以积分就变成了求和。</p></blockquote><h3 id="展开q函数">展开Q函数</h3><p><span class="math display">\[Q(\theta,\theta^{(t)}) =\sum_Z[ logp(x_1,z_{1}|\theta)+logp(x_2,z_{2}|\theta)+...+logp(x_n,z_{n}|\theta) ]\prod _{i=1}^np(z_{i}|x_i,\theta^{(t)})\]</span></p><p><strong>只看第一项</strong> <span class="math display">\[\sum_Zlogp(x_1,z_{1}|\theta)\prod _{i=1}^np(z_{i}|x_i,\theta^{(t)})\]</span> 因为<spanclass="math inline">\(logp(x_1,z_{1}|\theta)\)</span> 只与<spanclass="math inline">\(z_{1}\)</span> 相关，而<spanclass="math inline">\(\prod_{i=1}^np(z_{i}|x_i,\theta^{(t)})\)</span>中，<spanclass="math inline">\(p(z_{1}|x_i,\theta^{(t)})\)</span> 与<spanclass="math inline">\(z_{1}\)</span>相关，所以可以将上式改写为： <spanclass="math display">\[\begin{split}&amp;\sum_Zlogp(x_1,z_{1}|\theta)\prod_{i=1}^np(z_{i}|x_i,\theta^{(t)})\\&amp;=\sum_{z_{1}}logp(x_1,z_{1}|\theta)p(z_{1}|x_1,\theta^{(t)})[\sum_{z_{2},...z_{n}}\prod_{i=2}^np(z_{i}|x_i,\theta^{(t)})]\end{split}\]</span> <strong>约去后项</strong></p><p>对于<span class="math inline">\(\sum_{z_{2},...z_{n}}\prod_{i=2}^np(z_{i}|x_i,\theta^{(t)})\)</span> ,实际上它等于1：</p><p>如同<span class="math inline">\(z_{1}\)</span> 一样，<spanclass="math inline">\(p(z_{i}|x_i,\theta^{(t)})\)</span> 只与<spanclass="math inline">\(z_{i}\)</span> 相关，所以上式展开将变为：</p><p><span class="math display">\[\begin{split}&amp;\sum_{z_{2}...z_{n}}\prod_{i=2}^np(z_{i}|x_i,\theta^{(t)}) \\&amp; =\sum_{z_{2}...z_{n}}p(z_{2}|x_2,\theta^{(t)})p(z_{3}|x_3,\theta^{(t)})...p(z_{n}|x_n,\theta^{(t)})\\&amp;=\sum_{z_{2}}p(z_{2}|x_2,\theta^{(t)})\sum_{z_{3}}p(z_{3}|x_3,\theta^{(t)})...\sum_{z_{n}}p(z_{n}|x_n,\theta^{(t)})\end{split}\]</span></p><p>而<span class="math inline">\(\sum_{z_{i}}p(z_{i}|x_i) =1\)</span>,所以全部都可以约为1。</p><p>因此<span class="math inline">\(\sum_{z_{2}...z_{n}}\prod_{i=2}^np(z_{i}|x_i,\theta^{(t)}) = 1\)</span></p><p>于是第一项将变为： <span class="math display">\[\begin{split}&amp;\sum_Zlogp(x_1,z_{1}|\theta)\prod_{i=1}^np(z_{i}|x_i,\theta^{(t)}) \\&amp;=\sum_{z_{1}}logp(x_1,z_{1}|\theta)p(z_{1}|x_1,\theta^{(t)})\end{split}\]</span> <strong>推广到整体</strong></p><p>根据第一项的化简原理，化简至所有项。 <span class="math display">\[\begin{split}&amp;\sum_Z[logp(x_1,z_{1}|\theta)+logp(x_2,z_{2}|\theta)+...+logp(x_n,z_{n}|\theta)] \prod _{i=1}^np(z_{i}|x_i,\theta^{(t)})\\ &amp;=\sum_{z_{1}}logp(x_1,z_{1}|\theta)p(z_{1}|x_1,\theta^{(t)})+...+\sum_{z_{n}}logp(x_n,z_{n}|\theta)p(z_{n}|x_n,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z_{i}}logp(x_i,z_{i}|\theta)p(z_{i}|x_i,\theta^{(t)})\end{split}\]</span></p><h3 id="结论">结论</h3><p>通过简化后，Q函数将变为： <span class="math display">\[\begin{split}Q(\theta,\theta^{(t)}) &amp;= \sum_Zlog \prod_{i=1}^np(x_i,z_{i}|\theta)\prod_{i=1}^np(z_{i}|x_i,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z_{i}}logp(x_i,z_{i}|\theta)p(z_{i}|x_i,\theta^{(t)})\\&amp;=\sum_{z_{i}}\sum_{i=1}^nlogp(x_i,z_{i}|\theta)p(z_{i}|x_i,\theta^{(t)})\\&amp;=\sum_{k=1}^K\sum_{i=1}^nlog[\alpha_kN(x_i|\mu_k,\Sigma_k)]p(z_{i}=c_k|x_i,\theta^{(t)})\\&amp;=\sum_{k=1}^K\sum_{i=1}^n[log\alpha_k+logN(x_i|\mu_k,\Sigma_k)]p(z_{i}=c_k|x_i,\theta^{(t)})\end{split}\]</span></p><h2 id="m-step">M-step</h2><p>M-step目标函数： <span class="math display">\[\theta^{(t+1)} = argmax_\theta Q(\theta,\theta^{(t)})\]</span> 关于<spanclass="math inline">\(\alpha,\mu,\Sigma\)</span>的详细求解步骤在重写这篇文章时选择跳过。因为其实我只跟着推了<spanclass="math inline">\(\alpha\)</span>，均值和协方差并没有详细推（抱歉），求解<spanclass="math inline">\(\alpha\)</span>时会用到拉格朗日乘子法。详细步骤请移步参考教程中的第二个连接：<ahref="https://zhuanlan.zhihu.com/p/85338773">高斯混合模型（GMM）推导及实现</a></p><p>解出的最终结果：</p><p><span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p><span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><h1 id="gmm总结">GMM总结</h1><h2 id="gmm聚类流程">GMM聚类流程</h2><p><strong>step1：</strong></p><p>    定义高斯分布个数K，对每个高斯分布设置初始参数值<spanclass="math inline">\(\theta^{(0)} =\alpha^{(0)},\mu^{(0)},\Sigma^{(0)}\)</span>。<strong>一般第一步不会自己设置初始值，而是通过K-mean算法计算初始值。</strong></p><p><strong>step2 E-step：</strong></p><p>    根据当前的参数<span class="math inline">\(\theta^{(t)}\)</span>,计算每一个隐变量的后验概率分布<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>。</p><p>    <span class="math inline">\(\gamma_t(z_j^{(i)}) =\frac{\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}{\sum_{k=1}^K\alpha_k^{(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}\)</span></p><p><strong>step3 M-step：</strong></p><p>    根据E-step计算出的隐变量后验概率分布，进一步计算新的<spanclass="math inline">\(\theta^{(t+1)}\)</span></p><p>    <span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p>    <span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p>    <span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><strong>step4:</strong> 循环E-step和M-step直至收敛。</p><h2 id="gmm优缺点">GMM优缺点</h2><p><strong>优点：</strong></p><ul><li><p>GMM使用均值和标准差，簇可以呈现出椭圆形，优于k-means的圆形</p></li><li><p>GMM是使用概率，故一个数据点可以属于多个簇</p></li></ul><p><strong>缺点：</strong></p><ol type="1"><li>对大规模数据和多维高斯分布，计算量大，迭代速度慢</li><li>如果初始值设置不当，收敛过程的计算代价会非常大。</li><li>EM算法求得的是<strong>局部最优解</strong>而不一定是全局最优解。</li></ol><h1 id="gmm的实现和应用">GMM的实现和应用</h1><p><ahref="https://www.issey.top/article/ca2465afd48c/">机器学习笔记12——【GMM2】高斯混合模型实现与应用 | issey的博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类算法 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo数学公式显示问题</title>
      <link href="/article/1d3b342ddcd7/"/>
      <url>/article/1d3b342ddcd7/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>周末在搬迁Hexo博客并且修复之前博客的bug，外加继续装修博客。整了一个周末终于弄好了。    </p><p>总结一下遇到的最让我无语而且耗时最长的问题吧。</p><p>Latex数学公式+公式无法换行。首先呢这是两个问题，要先让hexo支持数学公式，再来才是不能换行的问题。自己找了许许多多的教程，先后折磨了快半天，才找到了较为完美的解决方案。可能是关于这方面的教程大多都太老了，版本迭代后不太适用吧。</p><h1 id="关于hexo和主题">关于Hexo和主题</h1><p>这个问题实际<strong>与主题无关</strong>。最开始我一直纠结于主题不同解决方法是不是不一样，最后解决了发现不是。</p><p>我用的是hexo和butterfly截止目前的最新版本：</p><p>hexo：6.3.0</p><p>butterfly：4.4.0</p><h1 id="让hexo支持数学公式">让Hexo支持数学公式</h1><p>这部分我跟着这篇文章搞的：<ahref="https://blog.csdn.net/gorray/article/details/122398901">Hexo如何显示latex公式_gorray的博客-CSDN博客_hexolatex公式</a></p><p>其实要做的步骤很少：</p><p>1.首先卸载hexo-math和hexo-renderer-marked。然而hexo应该是没有自带hexo-math的，所以只需要卸载第二个就行。以防万一还是可以直接执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-math</span><br><span class="line">npm un hexo-renderer-marked</span><br></pre></td></tr></table></figure><p>2.安装hexo-renderer-pandoc渲染器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure><p>好，到此为止，需要的包就迭代好了。</p><p>3.然后是配置主题配置下的mathjax设置。我用的是butterfly，那么对应路径是： _config.yml</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h1 id="pandoc报错">Pandoc报错</h1><p>理论上到这一步就可以用了对吧，一般的教程也都这样。就算再外加一步，我学习的那篇文章里提到，接下来还应该去Pandoc官网下载<strong>最新版本</strong>pandoc：<ahref="https://pandoc.org/index.html">Pandoc - About pandoc</a></p><p>关于pandoc下载安装教程随便查一下就有，这里就不说明了。然后再在环境配置配置了pandoc路径，直到你可以在cmd输入以下命令查看它的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandoc -v</span><br></pre></td></tr></table></figure><p>我的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pandoc 2.19.2</span><br><span class="line">Compiled with pandoc-types 1.22.2.1, texmath 0.12.5.2, skylighting 0.13,</span><br><span class="line">citeproc 0.8.0.1, ipynb 0.2, hslua 2.2.1</span><br><span class="line">Scripting engine: Lua 5.4</span><br></pre></td></tr></table></figure><p>有的朋友可能到这一步就发现不对了，不要慌，接下来才是重点。</p><h2 id="一个莫名其妙的错误">一个莫名其妙的错误</h2><p>先回到hexo目录，执行hexo -s,如果你没有出现这个报错：</p><p><span class="math inline">\(\color{red}{pandoc~exited ~with ~code ~9:pandoc: Unknown~extension:~smart}\)</span></p><p>那么恭喜你，你的这个问题并不存在，可以选择跳过。但是如果你和我一样报这个错误，可能就开始头疼了。不过我终于还是找到了解决方法。</p><p>首先我是找到了这篇文章：<ahref="https://www.cnblogs.com/diralpo/p/12542450.html">配置hexo时遇到的问题- diralpo - 博客园 (cnblogs.com)</a></p><p>从这篇文章得知，导致该报错的原因是<strong>pandoc版本过低</strong>，而且还不是一般原因引起的版本过低，因为前面我们已经安装了最新版本的pandoc。但是最新版本的没起作用。于是我打开了everything查找电脑上存在的pandoc。然后发现位于Anaconda，真正问题也出在这儿。</p><p><strong>是因为Anaconda安装的pandoc版本过低，而且hexo默认使用的是Anaconda的pandoc。</strong></p><p>不信的话你去找找，那个pandoc居然是2017年的。在某一篇文章得知，pandoc版本应该在2.0以上，但那个pandoc好像是1.9。那接下来的就简单了，直接把新下载的pandoc.exe替换Anaconda里的pandoc.exe。</p><p><img src="https://img.issey.top/img/202209182134541.png" /></p><p>然后你在回去hexo -s，就没问题了。</p><h1 id="换行问题">换行问题</h1><p>其实做到上一步，换行问题也已经随之解决了。不过这里还是提一下关于这个换行。</p><p>首先，想直接通过 ，end这种写法是做不到换行的，我最开始就是纠结于这个，然而这写法本来也不规范，例如：A  B是不能达到换行的。但是在加上规范的begin和end就可以了。其次，换行公式应该写成行间公式而非行内公式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$A \\ B$  错误写法</span><br><span class="line">\begin&#123;split&#125; A\\B \end&#123;split&#125; 写在行内错误，写在行间正确。</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{split} A\\B \end{split}\]</span></p><p>这篇文章是我做的hexo公式测试：<ahref="https://www.issey.top/article/1365bcc580cd/">Latex公式测试 |issey的博客</a></p><p>如果不是网速加载问题，那么显示应该是：</p><p>    <img src="https://img.issey.top/img/202209182149084.png" /></p>]]></content>
      
      
      <categories>
          
          <category> hexo相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码测试</title>
      <link href="/article/cb3b152854a1/"/>
      <url>/article/cb3b152854a1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    original_img = plt.imread(<span class="string">&#x27;color.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shape of original_img is:&quot;</span>, original_img.shape)</span><br><span class="line">    original_img /= <span class="number">255</span></span><br><span class="line">    X_img = np.reshape(original_img, (original_img.shape[<span class="number">0</span>] * original_img.shape[<span class="number">1</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    K = <span class="number">8</span></span><br><span class="line">    model = KMeans(n_clusters=K)</span><br><span class="line">    model.fit(X_img)</span><br><span class="line">    centroids = model.cluster_centers_</span><br><span class="line">    <span class="comment"># labels得到的是质心索引</span></span><br><span class="line">    labels = model.predict(X_img)</span><br><span class="line">    <span class="comment"># print(labels[:6])</span></span><br><span class="line">    <span class="comment"># 替换样本</span></span><br><span class="line">    X_recovered = centroids[labels]</span><br><span class="line">    X_recovered = np.reshape(X_recovered, original_img.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(X_recovered*<span class="number">255</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hexo相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex公式测试</title>
      <link href="/article/1365bcc580cd/"/>
      <url>/article/1365bcc580cd/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[\mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix}\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\\frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp;0 \\\frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp;0 \\\end{vmatrix}\]</span></p><p>多行对齐：</p><p><span class="math display">\[\begin{gather}\begin{split}Adv^{Fed}&amp; = Pr^{Fed}\left ( A=1\mid x\in D_{T} \right ) - Pr^{Fed}\left (A=1\mid x\in D_{N} \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( P\left ( A=1\mid x \right) \right )-\underset{x\in D_{N}}{E^{Fed}}\left ( P\left ( A=1\mid x\right ) \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( 1-\frac{L\left ( \left (x,y \right ),F \right )}{A} \right )-\underset{x\in D_{N}}{E^{Fed}}\left( 1-\frac{L\left ( \left ( x,y \right ),F \right )}{A} \right )\\&amp; = \frac{1}{A}\cdot \left [ \underset{x\in D_{N}}{E^{Fed}}\left (L\left ( \left ( x,y \right ),F \right )\right )-\underset{x\inD_{T}}{E^{Fed}}\left ( L\left ( \left ( x,y \right ),F \right ) \right )\right ] \\\end{split}\end{gather}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> hexo相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记10——EM算法原理与详细推导</title>
      <link href="/article/ee70792b2110/"/>
      <url>/article/ee70792b2110/</url>
      
        <content type="html"><![CDATA[<blockquote><p>    本文章为EM算法笔记，本人学习和参考的文章：<ahref="https://blog.csdn.net/v_JULY_v/article/details/81708386">如何通俗理解EM算法_v_JULY_v的博客-CSDN博客_em算法</a></p><p>    在开始之前，我们需要先复习一些在概率统计中学过的东西。</p></blockquote><h1 id="似然函数">似然函数</h1><blockquote><p>    统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：</p><p>    <strong>L(θ|x)=P(X=x|θ)。</strong></p><p>    注：L(θ|x)也可以写为L(θ:x)</p></blockquote><p>    用例说明可能会更好理解：</p><p>    考虑一个抛硬币实验，每次抛硬币相互独立，我们先假设正面朝上的概率<spanclass="math inline">\(P(H) = 0.5\)</span>,那么两次正面朝上的概率就是：</p><p>    <span class="math inline">\(P(H,H) = 0.5\times 0.5 =0.25\)</span></p><p>    现在换一种角度，已知“两次抛硬币都是正面朝上”，则<strong>硬币正面朝上的概率为0.5的似然</strong>就是：</p><p>    <span class="math inline">\(L(p(H) = 0.5;HH) = P(HH|P(H) = 0.5) =0.25\)</span></p><p>    同理，硬币正面朝上的概率为0.6的似然就是0.36。</p><h1id="极大似然估计maximum-likelihood-estimatemle">极大似然估计（MaximumLikelihood Estimate，MLE）</h1><p>    <strong>极大似然估计也称为最大似然估计。</strong></p><p>    比如现在有一个袋子，里面装有白球和红球，你在里面随机抓了10个球，发现9个都是红球，那么你可以猜测从这个袋子抓出红球的概率为0.9，而抓出白球的概率为0.1。也就是说，我们<strong>根据样本来推测“为什么抓出来的样本会是这样”最可能的原因</strong>。</p><p>    这种根据样本推断<strong>最可能的</strong>模型未知参数的方法叫做极大似然估计。</p><p>    回到上面那个抛硬币的例子：如果两次抛硬币都是正面朝上。我们已经计算了当<spanclass="math inline">\(P(H) = 0.5\)</span> 时，似然函数等于0.25；当<spanclass="math inline">\(P(H) = 0.6\)</span>时，似然函数等于0.36。计算<span class="math inline">\(P(H) =1\)</span>时，似然函数将会等于1，此时是最大的。</p><p>    那么“两次抛硬币都正面朝上”的实验推出的极大似然估计就是“硬币正面朝上的概率为1”。</p><h2 id="极大似然估计应用">极大似然估计应用</h2><p>    现在我们使用抽样统计的方法从一个学校里随机抽取了100名男生和100名女生，并且统计他们各自的身高。</p><p>    假设男生的身高服从高斯分布<span class="math inline">\(X \simN(\mu_1,\sigma_1^2)\)</span> ,女生身高服从另一个高斯分布<spanclass="math inline">\(X \sim N(\mu_2,\sigma_2^2)\)</span>。不过我们只知道他们服从高斯分布模型，并不知道这两个模型的各个参数，即<spanclass="math inline">\(\mu,\sigma\)</span></p><p>是未知参数。</p><p>    现在，设<span class="math inline">\(\theta =[\mu,\sigma]^T\)</span>。我们需要利用极大似然估计，通过100个男生样本和100个女生样本分别估计各自的<spanclass="math inline">\(\theta\)</span>。因为求男生和求女生的过程一样，只是样本不同。所以接下来仅以求解男生身高对应的极大似然估计为例。</p><p>    令<spanclass="math inline">\(x_i\)</span>为来自男生样本集X的第i个样本。现在套入之前提到的似然函数<spanclass="math inline">\(L(\theta)\)</span>：</p><p>    $L() = P(X=x|θ)= P(x_1,x_2,...,x_n|) = _{i=1}^nP(x_i|) $</p><p>    现在要求<span class="math inline">\(\hat\theta\)</span>使得似然函数<span class="math inline">\(L(\theta)\)</span>最大，求得的<span class="math inline">\(\hat\theta\)</span> 即<spanclass="math inline">\(\theta\)</span>的极大似然估计。用公式写出来就是：</p><p>    <span class="math inline">\(\hat \theta =argmaxL(\theta)\)</span></p><h3 id="求解极大似然估计">求解极大似然估计</h3><p>    将似然函数化为<strong>对数似然函数：</strong></p><p>    <span class="math inline">\(\ell(\theta) = lnL(\theta) =ln\prod_{i=1}^nP(x_i|\theta) = \sum_{i=1}^nlnP(x_i|\theta)\)</span></p><p>    转化成对数似然函数后，再求导（或偏导），令导数为0，解得的参数就是极大似然估计。</p><p>    求解极大似然估计的一般步骤：</p><ol type="1"><li><p>写出似然函数</p></li><li><p>化位对数似然函数</p></li><li><p>求导数，令导数为0，得到似然方程</p></li><li><p>求解似然方程 </p></li></ol><blockquote><p>这里顺带提一下最大似然估计的适用条件：</p><p><strong>1.样本独立同分布</strong></p><p>2.已知样本服从某种分布模型，只是参数未知</p></blockquote><p>    以上都是概率统计的内容复习，及如何通过已知<strong>服从单个概率模型</strong>的样本集求解该模型的未知参数。</p><hr /><h1 id="初识em算法">初识EM算法</h1><h2 id="问题引入">问题引入</h2><p>    刚才提到，要想使用极大似然估计，必须确保样本都服从同一个分布。比如，我们通过100个男生身高样本使用极大似然估计可以算出男生身高服从的高斯模型。</p><p>    现在，考虑这样一种情况：男女生的样本混合在一起了，你不知道这200个样本哪个是男生，哪个是女生。现在要想计算男女生各自服从的高斯模型的未知参数，就需要用到EM算法。</p><h2 id="隐变量">隐变量</h2><p>    像上述问题中这种“不知道样本属于男生还是女生”在EM算法中被称为<strong>隐变量</strong>。隐变量记作<strong>Z</strong>，样本i的隐变量记作<spanclass="math inline">\(z_i\)</span>。</p><p>    隐变量指<strong>不可观测数据</strong>，比如聚类中，训练样本集给出了特征却没给出类别。这时候样本的类别就是隐变量。</p><p>    一般用<strong>Y</strong>表示可观测到的随机变量的数据，<strong>Z</strong>表示不可观测的随机变量的数据。Y与Z合起来被称为<strong>完全数据</strong>，只有Y则被称作<strong>不完全数据</strong>。</p><h2 id="直观理解em算法">直观理解EM算法</h2><p>    假设有5枚硬币，这些硬币来自A,B两类，但我们不知道它们各自属于哪类硬币。现在把这5枚硬币各抛10次，请根据样本数据推测A,B两类硬币各自的“正面朝上的极大似然估计”，并且将这5枚硬币分类。</p><p>    那么在上述例子中，“硬币种类”就是我们的隐变量，设为向量<strong>Z</strong>，<spanclass="math inline">\(Z = (z_1,z_2,z_3,z_4,z_5)\)</span></p><p>    （注：H代表正面朝上，T代表反面朝上）</p><table><thead><tr class="header"><th>硬币序号</th><th>硬币类别</th><th>结果（X）</th><th>统计</th></tr></thead><tbody><tr class="odd"><td>1</td><td><span class="math inline">\(z_1\)</span>,未知</td><td>H T T T T H T T T T</td><td>2H,8T</td></tr><tr class="even"><td>2</td><td><span class="math inline">\(z_2\)</span>,未知</td><td>H H H H T H H H H H</td><td>9H,1T</td></tr><tr class="odd"><td>3</td><td><span class="math inline">\(z_3\)</span>,未知</td><td>H T H T H H T T H H</td><td>6H,4T</td></tr><tr class="even"><td>4</td><td><span class="math inline">\(z_4\)</span>,未知</td><td>T H T T H T T T H T</td><td>3H,7T</td></tr><tr class="odd"><td>5</td><td><span class="math inline">\(z_5\)</span>,未知</td><td>T H H H T H H H T H</td><td>7H,3T</td></tr></tbody></table><p>现在使用EM算法求解：</p><p><strong>step1:随机初始化模型参数<spanclass="math inline">\(\theta^{(0)}\)</span> .</strong></p><p>    <span class="math inline">\(P(H_A)\)</span>、<spanclass="math inline">\(P(H_B)\)</span>分别是A类硬币、B类硬币正面朝上的概率，即<strong>模型未知参数</strong><spanclass="math inline">\(\theta\)</span>。</p><p>    EM算法需要设置一个初始化模型参数<spanclass="math inline">\(\theta^{(0)}\)</span> ,进而推出在<spanclass="math inline">\(\theta^{(0)}\)</span>条件下的隐变量<strong>Z</strong>。</p><p>    设<span class="math inline">\(P(H_A) = 0.2\)</span> , <spanclass="math inline">\(P(H_B) = 0.7\)</span> 。</p><p><strong>step2:计算隐变量</strong></p><p>    计算各硬币的隐变量：</p><blockquote><p>    还记得最开始提到的似然函数吗？这里就需要求似然，即<spanclass="math inline">\(L(θ|x)=P(X=x|θ)\)</span>。</p></blockquote><p>    例：</p><p>    对于1号硬币，属于A类的似然：</p><p><span class="math display">\[\begin{split} L(\theta_A^{(0)}|X = x_1)&amp;= P(X = x_1|\theta_A^{(0)})\\&amp;= \prod_{j = 1}^{10}P(x_{1,j}|\theta_A^{(0)})\\&amp;=0.2^2\times0.8^8 = 67.1089\times10^{-4}\end{split}\]</span></p><p>    对于1号硬币，属于B类的似然：</p><p><span class="math display">\[\begin{split}L(\theta_B^{(0)}|X = x_1)&amp; = P(X = x_1|\theta_B^{(0)})\\&amp;= \prod_{j = 1}^{10}P(x_{1,j}|\theta_B^{(0)})\\&amp;=0.7^2\times0.3^8 = 0.3215*10^{-4}\end{split}\]</span></p><p>    比较属于A类和属于B类的似然，发现A的似然更大，则把1号硬币归为A类，即令隐变量<spanclass="math inline">\(z_1 = A\)</span>。</p><p>   各枚硬币的隐变量计算：</p><table><colgroup><col style="width: 6%" /><col style="width: 10%" /><col style="width: 32%" /><col style="width: 32%" /><col style="width: 18%" /></colgroup><thead><tr class="header"><th>序号</th><th>统计</th><th>A类的似然(<span class="math inline">\(10^{-4}\)</span>)</th><th>B类的似然(<span class="math inline">\(10^{-4}\)</span>)</th><th>类别（隐变量）</th></tr></thead><tbody><tr class="odd"><td>1</td><td>2H,8T</td><td>67.11</td><td>0.32</td><td><span class="math inline">\(z_1\)</span> = A</td></tr><tr class="even"><td>2</td><td>9H,1T</td><td>0.004</td><td>121.06</td><td><span class="math inline">\(z_2\)</span> = B</td></tr><tr class="odd"><td>3</td><td>6H,4T</td><td>0.26</td><td>9.53</td><td><span class="math inline">\(z_3\)</span> = B</td></tr><tr class="even"><td>4</td><td>3H,7T</td><td>16.78</td><td>0.75</td><td><span class="math inline">\(z_4\)</span> = A</td></tr><tr class="odd"><td>5</td><td>7H,3T</td><td>0.06</td><td>22.24</td><td><span class="math inline">\(z_5\)</span> = B</td></tr></tbody></table><p>    我们通过假设模型未知参数，“猜出了”硬币各自属于哪一类，使得隐函数变为已知值，这时就满足了应用<strong>极大似然估计</strong>的条件，于是可以用极大似然估计求解未知参数<spanclass="math inline">\(\theta^{(1)}\)</span>。</p><p><strong>step3：极大似然估计求解模型参数<spanclass="math inline">\(\theta^{(1)}\)</span> .</strong></p><p>    根据<span class="math inline">\(\theta^{(0)}\)</span>,我们算出了1号、4号属于A类，2、3、5属于B类。于是跟新<spanclass="math inline">\(P(H_A)\)</span>和<spanclass="math inline">\(P(H_B)\)</span>:</p><p>    <span class="math inline">\(P(H_A) = \frac{2+3}{20} =0.25\)</span></p><p>    <span class="math inline">\(P(H_B) = \frac{9+6+7}{30} =0.73\)</span></p><p>    可以看出<span class="math inline">\(\theta^{(1)}\)</span>与<spanclass="math inline">\(\theta^{(0)}\)</span>并不相同。聪明的你可能想到了，接下来就是迭代的过程，重复step2和step3得到<spanclass="math inline">\(\theta^{(2)}\)</span>、<spanclass="math inline">\(\theta^{(3)}\)</span> ...</p><p>    可以证明，<strong>模型参数会随着迭代越来越接近真实值</strong>，并且一定会收敛到局部最优值。<strong>但不一定会收敛到全局最优值</strong>。</p><h2id="隐变量的期望隐变量的概率分布">隐变量的期望（隐变量的概率分布）</h2><p>    通过初步认识EM算法，你已经大致知道EM算法在干嘛了，但是EM算法在处理隐变量时，并不是通过似然函数判断某个样本的隐变量属于哪一个特定值，而是求<strong>隐变量的概率分布</strong>。再通过隐变量的概率分布进行后续运算。所以在正式开始推导EM算法前，我们还需要知道<strong>隐变量的概率分布</strong>。</p><h3 id="隐变量的概率分布">隐变量的概率分布</h3><p>    我们使用期望来简化计算，于是<strong>求隐变量的概率分布就是隐变量的期望</strong>。因为上面那个抛硬币的数据算出来的期望比较极端，所以这里重新举一个例子：现在有五枚硬币，分别来自A,B两类,但分类未知。</p><table><thead><tr class="header"><th>硬币序号</th><th>结果</th><th>统计</th></tr></thead><tbody><tr class="odd"><td>1</td><td>H H T H T</td><td>3H,2T</td></tr><tr class="even"><td>2</td><td>T T H H T</td><td>2H,3T</td></tr><tr class="odd"><td>3</td><td>H T T T T</td><td>1H,4T</td></tr><tr class="even"><td>4</td><td>H T T H H</td><td>3H,2T</td></tr><tr class="odd"><td>5</td><td>T H H T T</td><td>2H,3T</td></tr></tbody></table><p>    因为前面的过程都一样，所以我们直接来到计算出极大似然这一步。现在已经在随机固定模型参数<spanclass="math inline">\(\theta^{(0)}\)</span>的情况下计算出了每一枚硬币对应的似然：</p><table><thead><tr class="header"><th>硬币序号</th><th>A类似然</th><th>B类似然</th></tr></thead><tbody><tr class="odd"><td>1</td><td>0.00512</td><td>0.03087</td></tr><tr class="even"><td>2</td><td>0.02048</td><td>0.01323</td></tr><tr class="odd"><td>3</td><td>0.08192</td><td>0.00567</td></tr><tr class="even"><td>4</td><td>0.00512</td><td>0.03087</td></tr><tr class="odd"><td>5</td><td>0.02048</td><td>0.01323</td></tr></tbody></table><p>    在上一节中，接下来我们直接确认了隐变量的值。现在，我们用<strong>隐变量的期望</strong>来计算隐变量的概率分布<spanclass="math inline">\(Q(z)\)</span> 。</p><blockquote><p>因为每一个样本都有自己的隐变量概率分布，所以精确到样本的隐变量概率分布应该写为：<spanclass="math inline">\(Q_i(z^{(i)})\)</span>。<spanclass="math inline">\(z^{(i)}\)</span> 表示这是第<spanclass="math inline">\(i\)</span> 个样本的隐变量。</p></blockquote><p>    例如硬币1的隐变量概率分布：</p><p>$    P(z_1=A) = = 0.14$</p><p>    <span class="math inline">\(P(z_1=B) = 1-0.14 = 0.86\)</span></p><p>    其他同理，隐变量的期望如下：</p><table><thead><tr class="header"><th>硬币序号</th><th>P(<span class="math inline">\(z_i\)</span> = A)</th><th><span class="math inline">\(P(z_i=B)\)</span></th></tr></thead><tbody><tr class="odd"><td>1</td><td>0.14</td><td>0.86</td></tr><tr class="even"><td>2</td><td>0.61</td><td>0.39</td></tr><tr class="odd"><td>3</td><td>0.94</td><td>0.06</td></tr><tr class="even"><td>4</td><td>0.14</td><td>0.86</td></tr><tr class="odd"><td>5</td><td>0.61</td><td>0.39</td></tr></tbody></table><p>     再将隐变量的分布函数代入最开始的表以计算极大似然估计来计算下一轮的模型参数<spanclass="math inline">\(\theta^{(1)}\)</span> ,即新的<spanclass="math inline">\(P(H_A)\)</span>、<spanclass="math inline">\(P(H_B)\)</span>。</p><p>    计算1号各权值：</p><p>    1号是A类时正面朝上的权值： $w(A_1 = H) = 3*0.14 = 0.42 $</p><p>    1号是A类时反面朝上的权值：<span class="math inline">\(w(A_1 = T)= 2*0.14 = 0.28\)</span> </p><p>    1号是B类时正面朝上的权值：<span class="math inline">\(w(B_1 = H)= 3*0.86= 2.58\)</span></p><p>    1号是B类时反面朝上的权值：<span class="math inline">\(w(B_1 = T)= 2*0.86 = 1.72\)</span></p><table><colgroup><col style="width: 5%" /><col style="width: 8%" /><col style="width: 21%" /><col style="width: 21%" /><col style="width: 21%" /><col style="width: 21%" /></colgroup><thead><tr class="header"><th>序号</th><th>统计</th><th><span class="math inline">\(w(A_i = H)\)</span></th><th><span class="math inline">\(w(A_i = T)\)</span></th><th><span class="math inline">\(w(B_i=H)\)</span></th><th><span class="math inline">\(w(B_i = T)\)</span></th></tr></thead><tbody><tr class="odd"><td>1</td><td>3H,2T</td><td>3*0.14 = 0.42</td><td>2*0.14 = 0.28</td><td>3*0.86 = 2.58</td><td>2*0.86 = 1.72</td></tr><tr class="even"><td>2</td><td>2H,3T</td><td>2*0.61 = 1.22</td><td>3*0.61 = 1.83</td><td>2*0.39 = 0.78</td><td>3*0.39 = 1.17</td></tr><tr class="odd"><td>3</td><td>1H,4T</td><td>1*0.94 = 0.94</td><td>4*0.94 = 3.76</td><td>1*0.06 = 0.06</td><td>4*0.06 = 0.24</td></tr><tr class="even"><td>4</td><td>3H,2T</td><td>3*0.14 = 0.42</td><td>2*0.14 = 0.28</td><td>3*0.86 = 2.58</td><td>2*0.86 = 1.72</td></tr><tr class="odd"><td>5</td><td>2H,3T</td><td>2*0.61 = 1.22</td><td>3*0.61 = 1.83</td><td>2*0.39 = 0.78</td><td>3*0.39 = 1.17</td></tr><tr class="even"><td>总权值</td><td></td><td>4.22</td><td>7.98</td><td>6.78</td><td>6.02</td></tr></tbody></table><p><span class="math inline">\(\theta^{(1)}:\)</span></p><p>    <span class="math inline">\(P(H_A) = \frac{4.22}{4.22+7.98} =0.35\)</span></p><p>    <span class="math inline">\(P(H_B) = \frac{6.78}{6.78+6.02} =0.52\)</span></p><p>    这就是使用隐变量概率分布求极大既然估计的过程。</p><p>    现在推广到一般情况：</p><h1 id="em算法公式详细推导">EM算法公式详细推导</h1><h2 id="含隐变量的对数似然函数">含隐变量的对数似然函数</h2><p>   之前我们提到，极大似然估计是为了求下面这个似然函数的极大值： $    L()= P(X=x|θ)= P(x_1,x_2,...,x_n|) = _{i=1}^nP(x_i|)$</p><p>    其中，X服从独立同分布，<spanclass="math inline">\(\theta\)</span>为概率分布模型参数。</p><p>    根据极大似然方程的步骤，要先将似然函数化为对数似然函数： <spanclass="math display">\[\begin{split}    &amp;\ell(\theta)= logL(\theta) =log\prod_{i=1}^nP(x_i|\theta) \\  &amp;  =\sum_{i=1}^nlogP(x_i|\theta)\end{split}\]</span></p><p>    问题在于现在X并不是同分布的样本集，同分布样本需要由X和隐变量Z共同确定。于是引入隐变量Z的对数似然变为：</p><p><span class="math display">\[\begin{split}\ell(\theta)&amp; =\sum_{i=1}^nlogP(x_i|\theta) \\&amp;=\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}P(x_i,z|\theta) \end{split}\]</span></p><blockquote><p><strong>这里解释一下<span class="math inline">\(P(x_i|\theta) =\sum_z^{Z^{(i)}}P(x_i,z|\theta)\)</span>：</strong></p><p>此处为全概率公式，描述为：<strong>对每个样例的每种可能类别求联合分布概率和。</strong></p></blockquote><p>按照求极大似然的步骤，接下来对<spanclass="math inline">\(\ell(\theta)\)</span>应该求偏导，然后令偏导为0，求解似然方程。但是这里多了个随机变量z，像<spanclass="math inline">\(log\sum\)</span>的形式求导太过于复杂，所以我们需要转化方程：</p><h2 id="利用jensen不等式转化方程">利用jensen不等式转化方程</h2><p>    待会儿解释jensen不等式是什么，先看转化结果。</p><p>    为了使用jensen不等式，我们需要先把分子分母都乘隐变量<spanclass="math inline">\(z_i\)</span>的概率分布函数<spanclass="math inline">\(Q_i(z^{(i)})\)</span>。</p><p>    然后通过jensen不等式，可以将其变为：</p><p><span class="math display">\[\begin{split}\ell(\theta) =\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}P(x_i,z|\theta) (1)\\=\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}       (2)\\\geq\sum_{i=1}^n\sum_{z}^{Z^{(i)}}Q_i(z)log\frac{P(x_i,z|\theta)}{Q_i(z)}        (3)\end{split}\]</span></p><blockquote><p>（注：因为<span class="math inline">\(z\)</span>来自于<spanclass="math inline">\(Z^{(i)}\)</span>，所以其实<spanclass="math inline">\(z\)</span> 就是<spanclass="math inline">\(z^{(i)}\)</span> 的简写。）</p></blockquote><p>    可以看出对于（3）式就很好求导了。</p><p>    接下来说明是如何从(2)转化为(3)的。</p><h2 id="jeasen不等式转化详解">jeasen不等式转化详解</h2><p>    <strong>（国际定义）凸函数</strong>：设<spanclass="math inline">\(f(x)\)</span>为定义域为实数的函数。</p><ol type="1"><li><p>对于所有实数x，若<span class="math inline">\(f&#39;&#39;(x)\geq0\)</span> ,则<spanclass="math inline">\(f(x)\)</span>为凸函数。<strong>即下凹为凸，上凸为凹。</strong></p></li><li><p>当<strong>x</strong>为向量，如果hessian矩阵H是半正定的（<spanclass="math inline">\(H\geq0\)</span>）,那么<spanclass="math inline">\(f(\vec x)\)</span> 为凸函数。</p></li><li><p>若<span class="math inline">\(f&#39;&#39;(x)&gt;0\)</span>或<spanclass="math inline">\(H&gt;0\)</span> ，则<spanclass="math inline">\(f(x)\)</span>为严格凸函数。</p></li></ol><p>    Jeasen不等式：</p><blockquote><p>如果f是凸函数，X是随机变量，那么：<spanclass="math inline">\(E[f(X)]&gt;=f(E[X])\)</span>，通俗的说法是函数的期望大于等于期望的函数。而凹函数反之。</p><p>特别地，如果f是严格凸函数，当且仅当<span class="math inline">\(P(X =EX) = 1\)</span>。即X是常量时，有<span class="math inline">\(E[f(X)] =f(EX)\)</span>。</p></blockquote><p>    图就不画了，大概了解上述的用法即可。所以要用jeason不等式的关键在于，<strong>函数要为凸或凹函数，且需要表示出期望</strong>。</p><hr /><h3 id="如何表示期望">如何表示期望</h3><p>    通过将公式变成含有<spanclass="math inline">\(Q_i(z^{(i)})\)</span>的式子。我们可以表示出期望：  </p><p>    如果你还记得期望公式的懒人定理：</p><blockquote><p>    设Y是关于随机变量X的函数,<span class="math inline">\(Y =g(X)\)</span> ,g为连续函数，那么：</p><p>    若X是离散型随机变量，X分布律为<span class="math inline">\(P(X =x_k) = P_k,k = 1,2,3...,\)</span>若<spanclass="math inline">\(\sum_{k=1}^\infty g(x_k)p_k\)</span>绝对收敛则有：</p><p>    <span class="math inline">\(E(Y) = E[g(X)] = \sum_{k=1}^\inftyg(x_k)p_k\)</span></p><p>    若X是连续型随机变量，X的概率密度为<spanclass="math inline">\(f(x)\)</span>，若<spanclass="math inline">\(\int_{-\infty}^\infty g(x)f(x)dx\)</span>绝对收敛，则有：</p><p>    <span class="math inline">\(E(Y) = E[g(X)] =\int_{-\infty}^\infty g(x)f(x)dx\)</span></p></blockquote><p>    根据懒人定理，（2）中的<spanclass="math inline">\(\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\)</span>就是<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})}\)</span>的期望。原因：<span class="math inline">\(Q_i(z^{(i)})\)</span> 是<spanclass="math inline">\(z^{(i)}\)</span> 的分布函数，而<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})}\)</span>是关于随机变量<span class="math inline">\(z^{(i)}\)</span>的函数。</p><p>    于是，令<span class="math inline">\(g(z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})}\)</span>,</p><p>    则<span class="math inline">\(E(g(z^{(i)})) =\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\)</span> 。</p><hr /><h3 id="为什么是凹函数">为什么是凹函数</h3><p>    在（1）式中，将<spanclass="math inline">\(log\sum_{z}^{Z^{(i)}}P(x_i,z|\theta)\)</span>看作一个整体，即<span class="math inline">\(log(x)\)</span> 。由于<spanclass="math inline">\(f&#39;&#39;(log(x))&lt;0\)</span>，所以是凹函数。</p><hr /><h3 id="转化对数似然方程式为不等式">转化对数似然方程式为不等式</h3><p>    由于是凹函数，所以不等式反向：<spanclass="math inline">\(f(E[X])\geq E[f(x)]\)</span>,</p><p>   令<span class="math inline">\(f(x) = logx\)</span>,<spanclass="math inline">\(X = g(z^{(i)})\)</span>则：</p><p>    <span class="math inline">\(f(E[X]) = f(E[g(z^{(i)})]) =log\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\)</span></p><p>    $E[f(X)] = E[f(g(z^{(i)}))] = _{z}<sup>{Z</sup>{(i)}}Q_i(z)log $;</p><p>   于是：</p><p><span class="math display">\[\begin{split}\ell(\theta) =\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\\       \geq\sum_{i=1}^n\sum_{z}^{Z^{(i)}}Q_i(z)log\frac{P(x_i,z|\theta)}{Q_i(z)} =J(z,Q)\end{split}\]</span></p><p>    至此，我们得到了一个好求偏导的公式。</p><h2 id="拔高下界">拔高下界</h2><p>    回顾我们的目标：使<spanclass="math inline">\(\ell(\theta)\)</span>最大化，直至目前，我们通过jensen不等式，获得了一个可以求导的公式<spanclass="math inline">\(J(z,Q)\)</span>。现在的目的是通过<spanclass="math inline">\(J(z,Q)\)</span>获得<spanclass="math inline">\(\ell(\theta)\)</span> 最大值。</p><p>    我们可以通过不断最大化<spanclass="math inline">\(J(z,Q)\)</span>的下界来让<spanclass="math inline">\(\ell(\theta)\)</span>不断提高。为了顺利理清接下来的过程，在这里回顾一下各函数和变量的说明：</p><ul><li><p><span class="math inline">\(\theta\)</span> :模型参数（未知），我们可以假设它已知，并在不断迭代中更新<spanclass="math inline">\(\theta\)</span> 。</p></li><li><p><spanclass="math inline">\(\ell(\theta)\)</span>:对数似然方程，我们需要调整<spanclass="math inline">\(\theta\)</span>使得<spanclass="math inline">\(\ell(\theta)\)</span> 取极大值，此时$= argmax ()$</p></li><li><p><spanclass="math inline">\(Q(z)\)</span>:隐变量z的概率分布，其实就是隐变量的期望。</p></li><li><p><span class="math inline">\(J(z,Q)\)</span>:不等式右边的方程，我们可以不断调整它的最大值来使得<spanclass="math inline">\(\ell(\theta)\)</span> 最大。</p></li></ul><p><img src="https://img.issey.top/img/202209221808126.png" /></p><p><strong>step1.首先固定<span class="math inline">\(\theta\)</span>,调整<span class="math inline">\(Q(z)\)</span>使下界<spanclass="math inline">\(J(z,Q)\)</span> 上升到与<spanclass="math inline">\(\ell(\theta)\)</span> 在点<spanclass="math inline">\(\theta\)</span> 处相等。</strong></p><p><strong>step2.固定<spanclass="math inline">\(Q(z)\)</span>,并使用极大似然估计法使<spanclass="math inline">\(\theta^{(t)}\)</span>达到最大值<spanclass="math inline">\(\theta^{(t+1)}\)</span> 。</strong></p><p><strong>step3.再将<spanclass="math inline">\(\theta^{(t+1)}\)</span>固定为模型参数，重复以上过程，直至收敛达到<spanclass="math inline">\(\ell(\theta)\)</span>最大值。此时对应的 <spanclass="math inline">\(\hat\theta\)</span>即为所求。</strong></p><p>    关于EM算法一定收敛的证明过程就暂时不写了。</p><blockquote><p>    step1想解决的问题可以阐述为：在模型参数已知的情况下，隐变量的最可能概率分布是什么？答案：使得<spanclass="math inline">\(J(z,Q)\)</span>与<spanclass="math inline">\(\ell(\theta)\)</span>在<spanclass="math inline">\(\theta\)</span>处相等的<spanclass="math inline">\(Q(z)\)</span> 。</p><p>    于是引出了新的问题：什么时候<spanclass="math inline">\(J(z,Q)\)</span>才与<spanclass="math inline">\(\ell(\theta)\)</span>相等？   </p></blockquote><h3 id="什么时候jzq-elltheta">什么时候<span class="math inline">\(J(z,Q)= \ell(\theta)\)</span></h3><p>    回顾Jensen不等式，当X是常量时，<spanclass="math inline">\(E[f(X)] = f(EX)\)</span>。所以我们为了让<spanclass="math inline">\(J(z,Q) = \ell(\theta)\)</span> ,需要让<spanclass="math inline">\(X\)</span>为常数。</p><p>    于是<span class="math inline">\(X = g(z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})} = c\)</span>    ,其中<spanclass="math inline">\(c\)</span>为常数。</p><p>    在由（1）变化（2）时，曾经提到过<spanclass="math inline">\(\sum_z^{Z^{(i)}}Q_i(z^{(i)}) =1\)</span>。因为<spanclass="math inline">\(Q(z)\)</span>是隐变量z的概率分布，概率之和为1。</p><p>    现在我们可以变换公式：（先同乘分母，再化简）</p><p>    因为 ：<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})} =c\)</span>，</p><p>    所以：<spanclass="math inline">\(\sum_z^{Z^{(i)}}P(x_i,z^{(i)}|\theta) =\sum_z^{Z^{(i)}}Q_i(z^{(i)})c = c\)</span> ，即<spanclass="math inline">\(\sum_z^{Z^{(i)}}P(x_i,z^{(i)}|\theta) =c\)</span>    (1)</p><p>    又因为由<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})} =c\)</span> 可得：</p><p>    <span class="math inline">\(Q_i(Z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{c}\)</span>     (2)</p><p>    于是当<span class="math inline">\(J(z,Q) =\ell(\theta)\)</span>时,将（1）代入（2）可得：</p><p><span class="math display">\[\begin{split}Q_i(Z^{(i)})&amp; = \frac{P(x_i,z^{(i)}|\theta)}{c}\\              &amp;=\frac{P(x_i,z^{(i)}|\theta)}{\sum_z^{Z^{(i)}}P(x_i,z^{(i)}|\theta)}\\              &amp;= P(z^{(i)}|x_i;\theta)\end{split}\]</span></p><p> <strong>结论：</strong></p><p>    当<span class="math inline">\(J(z,Q) =\ell(\theta)\)</span>时，<spanclass="math inline">\(Q_i(z^{(i)})\)</span>为隐变量的条件概率分布，即隐变量的期望。</p><p>    现在我们只需要利用似然函数求隐变量的期望即可(前面有讲过怎么求)。</p><hr /><h1 id="em算法总结">EM算法总结</h1><h2 id="em算法应用场景">EM算法应用场景</h2><p>    EM算法是一种迭代算法，主要用于含有<strong>隐变量</strong>的概率模型参数的极大似然估计或极大后验估计。广泛应用于缺损数据、截尾数据、成群数据、带有讨厌参数的数据等所谓不完全数据的统计推断问题。</p><h2 id="em算法步骤">EM算法步骤</h2><p>    其实在EM算法直观理解和详细推导时已经说了两次它的步骤了，不过在这里还是再总结一下，如果你和我一样没看懂，待会儿会给出解释：</p><blockquote><p>    输入：观测变量数据Y;隐变量数据Z;联合分布<spanclass="math inline">\(P(Y,Z|\theta)\)</span>,即完全数据的概率分布；条件分布<spanclass="math inline">\(P(Z|Y,\theta)\)</span>,即未观测数据Z的条件概率分布。</p><p>    输出：模型参数<span class="math inline">\(\theta\)</span> 。</p></blockquote><p>step1：初始化模型参数<spanclass="math inline">\(\theta^{(0)}\)</span> ，进入迭代。</p><p>step2（E-step）：记<span class="math inline">\(\theta^{(i)}\)</span>为第i次迭代参数<span class="math inline">\(\theta\)</span>估计值，第<spanclass="math inline">\(i+1\)</span>次迭代的E-step将计算</p><p><span class="math display">\[\begin{split}Q(\theta,\theta^{(i)})=E_z[logP(Y,Z|\theta)|Y,\theta^{(i)}]\\               =\sum_ZlogP(Y,Z|\theta)P(Z|Y,\theta^{i}) \end{split}\]</span></p><p>step3（M-step）：利用极大似然估计，计算使<spanclass="math inline">\(Q(\theta,\theta^{(i)})\)</span> 极大化的<spanclass="math inline">\(\hat \theta\)</span>，作为第<spanclass="math inline">\(i+1\)</span> 次迭代的参数估计值<spanclass="math inline">\(\theta^{(i+1)}\)</span>。</p><p>        <span class="math inline">\(\theta^{(i+1)} = argmax_\thetaQ(\theta,\theta^{(i+1)})\)</span></p><p>step4:重复step2、step3直到收敛。收敛时的<spanclass="math inline">\(\theta\)</span> 即为所求。</p><h3 id="关于步骤的解释">关于步骤的解释</h3><p>    <spanclass="math inline">\(Q(\theta,\theta^{(i)})\)</span>：完全数据的对数似然函数<spanclass="math inline">\(P(Y,Z|\theta)\)</span>关于在给定观测数据Y和当前参数<spanclass="math inline">\(\theta^{(i)}\)</span>下对未观测数据Z的条件概率分布<spanclass="math inline">\(P(Z|Y,\theta^{(i)})\)</span> 的期望称为Q函数。</p><p>    所以这个Q函数其实就是推导过程中的<spanclass="math inline">\(Q_i(z^{(i)})\)</span>,在拔高下界时详细说明了为什么求的是它。</p><p>    也就是说，<strong>E-step求的是在固定<spanclass="math inline">\(\theta\)</span>的条件下求隐变量z的期望</strong>，等同于拔高下界中的step1。</p><p>    而M-step则是固定<span class="math inline">\(Q_i(z^{(i)})\)</span>后极大似然估计求<span class="math inline">\(\theta^{(i+1)}\)</span>的过程。等同于拔高下界中的step2。</p><h2 id="关于em算法的重要说明">关于EM算法的重要说明</h2><ol type="1"><li><p>初始化时可以随机选择<spanclass="math inline">\(\theta^{(0)}\)</span>，但是EM算法对初始值很敏感，一旦选择不好会造成很大的计算损失。 对于初始值的选择也有相关的做法，这里暂时不研究。</p></li><li><p>停止迭代的条件一般是对于较小的正数<spanclass="math inline">\(A\)</span> ,若满足<spanclass="math inline">\(|\theta^{(i+1)}-\theta^{(i)}|&lt; A\)</span>，停止迭代。</p></li><li><p>EM算法的推进还有广义EM算法和EM算法变种等等。</p></li></ol><h2 id="算法优缺点">算法优缺点</h2><p><strong>优点：</strong></p><ol type="1"><li><p>通常计算起来比较简单</p></li><li><p>收敛稳定，不需要设置超参数</p></li></ol><p><strong>缺点：</strong></p><ol type="1"><li><p>对大规模数据和多维高斯分布，计算量大，迭代速度慢</p></li><li><p>如果初始值设置不当，收敛过程的计算代价会非常大。</p></li><li><p>EM算法求得的是<strong>局部最优解</strong>而不一定是全局最优解。</p></li></ol><h1 id="em算法的应用">EM算法的应用</h1><p>    关于EM算法的应用，例如处理混合高斯模型（GMM）和聚类等等，将会在下一篇EM文章中详细介绍。</p><p>    文章链接：<ahref="https://www.issey.top/article/4f4d2f7b3b7a/">机器学习笔记11——【GMM1】高斯混合模型原理与推导 | issey的博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类算法 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记9——KNN算法实现和应用</title>
      <link href="/article/324dbb5b1fc5/"/>
      <url>/article/324dbb5b1fc5/</url>
      
        <content type="html"><![CDATA[<h1 id="knn算法简介">KNN算法简介</h1><p>k近邻法（k-nearestneighbor，k-NN）属于监督学习中的分类算法，是机器学习中最简单的算法之一，同时也是最常用的分类算法之一。这篇文章将会基于numpy手动实现KNN，在此之后，将会利用sklearn实现的KNN完成数字识别。</p><p><strong>KNN需要数据标准化。</strong></p><h1 id="算法原理">算法原理</h1><p>KNN不同于其他机器学习算法，它<strong>不需要训练过程</strong>，当使用该算法预测一个新的值时，它会根据距离最近的K个点的类型来判断自己的类型：K个点中哪个类型多就属于哪个类型。</p><p>例：</p><p><img src="https://img.issey.top/img/202209212352694.png" /></p><p>当K等于3时，根据上述规则可得预测为红类；</p><p>当K等于5时，根据上述规则可得预测为蓝类。</p><h2 id="基本流程">基本流程</h2><ol type="1"><li><p>计算新数据与每一个已知样本的距离</p></li><li><p>按距离从小到大排序</p></li><li><p>选出距离最小的K个点</p></li><li><p>通过这K个点的分类决策新数据所属类别</p></li></ol><h1 id="knn算法的三要素">KNN算法的三要素</h1><p>根据上面的例子，可以看出不同K的选择对预测结果有较大影响。实验表明，K值越小，越容易发生过拟合；K值越大，越容易发生欠拟合。除<strong>K值的选择</strong>外，影响KNN算法的还有<strong>距离度量</strong>、<strong>分类决策规则</strong>。</p><p>分类决策规则一般采用多数表决，即最近的K个样本中哪个种类多就属于哪一类。</p><h2 id="距离度量">距离度量</h2><p>有同维度特征空间向量<span class="math inline">\(\vec A =(a_1,a_2,...,a_n),\vec B = (b_1,b_2,...,b_n)\)</span> ，则<spanclass="math inline">\(\vec A\)</span>,<span class="math inline">\(\vecB\)</span> 的<span class="math inline">\(L_P\)</span> 距离为：</p><p><span class="math inline">\(L_p(\vec A,\vec B) =(\sum_{i=1}^n|a_i-b_i|^p)^{\frac 1 p}\)</span></p><p>曼哈顿距离(P=1)：<span class="math inline">\(L_1 =\sum_{i=1}^n|a_i-b_i|\)</span></p><p>欧式距离(P=2):<span class="math inline">\(L_2 =\sqrt{\sum_{i=1}^n|a_i-b_i|^2}\)</span></p><p>切比雪夫距离(<span class="math inline">\(P=\infty\)</span>)：<spanclass="math inline">\(L_{\infty} =max(|a_1-b_1|,|a_2-b_2|,...,|a_n-b_n|)\)</span></p><p><strong>一般采用欧式距离作为距离度量。</strong></p><h2 id="k值选择-交叉验证">K值选择-交叉验证</h2><p>交叉验证简介：将原始数据集可以分为训练集、验证集、测试集。利用训练集和验证集测试模型好坏的方法叫做交叉验证。<strong>注：交叉验证是没有用到测试集的！如果用测试集进行交叉验证，会导致预测结果乐观化。</strong></p><p>通过交叉验证方法可以得到最合适的K值。</p><h1 id="knn算法的优缺点以及改进方法">KNN算法的优缺点以及改进方法</h1><p>优点：</p><ol type="1"><li><p>原理简单，便于实现。</p></li><li><p>KNN是惰性模型，不需要训练。</p></li><li><p>预测效果好。</p></li><li><p>对异常值不敏感。</p></li></ol><p>缺点：通过KNN算法原理可以得知，如果要预测一个新数据，需要将它和每一个样本都计算一次距离，然后排序，选出最小的K个样本进行决策。那么预测一次的时间复杂度为<spanclass="math inline">\(O(n)\)</span>，时间复杂度较高。并且需要将所有样本都储存在内存，空间复杂度较高，即对内存要求较高。所以KNN算法的缺点主要是：</p><ol type="1"><li><p>效率比较低，运行时间可能比较长。</p></li><li><p>对内存要求较高。</p></li></ol><p>KNN适用于数据集较小的分类。对于数据集较大的可使用神经网络进行分类。</p><h2 id="knn改进算法介绍">KNN改进算法介绍</h2><blockquote><p>这里不会说明各算法的原理与实现，只会简单介绍，参考文章：<ahref="https://jishuin.proginn.com/p/763bfbd3a197">盘点高效的KNN实现算法-技术圈(proginn.com)</a></p></blockquote><p>上述提到的均是KNN的<strong>线性扫描</strong>实现方法，即暴力法。通过分析，可知线性扫描不适用于数据规模较大的数据集。</p><p>实际上，KNN的实现方法高大10种，下面将介绍几种改进方法。</p><h3 id="kd树">KD树</h3><p>KD树是一种<strong>树形结构</strong>存储算法。</p><p><strong>适用范围：</strong></p><p><strong>KD树在特征维度小于20时效率最高</strong>，一般适用于训练样本数远大于空间维数时的k近邻搜索。当空间维数接近训练实例数时，效率会迅速下降，几乎接近线形扫描。</p><h2 id="ball树">Ball树</h2><p>ball树是KD树的一种改良算法。</p><h2 id="annoy">Annoy</h2><p>Annoy，全称“<strong>A</strong>pproximate <strong>N</strong>earest <strong>N</strong>eighbors <strong>O</strong>h <strong>Y</strong>eah”，是一种适合实际应用的快速相似查找算法。</p><p>Annoy 同样通过建立一个二叉树，使得每个点查找时间复杂度是O(logn)。和KD树不同的是，Annoy没有对k维特征进行切分。</p><h2 id="hnsw">HNSW</h2><p>HNSW（<strong>H</strong>ierarchcal <strong>N</strong>avigable <strong>S</strong>mall <strong>W</strong>orldgraphs）是基于<strong>图存储</strong>的数据结构。</p><p>上述四种算法中，Annoy和HNSW是可以在实际业务中落地的算法。</p><h1id="knn算法手动实现完成鸢尾花分类">KNN算法手动实现完成鸢尾花分类</h1><h2 id="主体部分">主体部分</h2><p>详细解释都写代码里了，因为主体部分实在太短了没必要分开写，就直接写在一个模块了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用KNN预测数据类别</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_KNN</span>(<span class="params">X,X_train,y_train,K</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param X: 需要预测的数据集</span></span><br><span class="line"><span class="string">    :param X_train: X训练集</span></span><br><span class="line"><span class="string">    :param y_train: Y训练集</span></span><br><span class="line"><span class="string">    :param K: K个最近</span></span><br><span class="line"><span class="string">    :return: 预测集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataSize = X_train.shape[<span class="number">0</span>]</span><br><span class="line">    y_predict = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">        <span class="comment"># 计算未知数据到每一个样本的欧氏距离</span></span><br><span class="line">        diff = np.tile(x,(dataSize,<span class="number">1</span>)) - X_train <span class="comment"># 把X扩大然后矩阵相减</span></span><br><span class="line">        squaredDist = np.<span class="built_in">sum</span>(diff**<span class="number">2</span>,axis=<span class="number">1</span>) <span class="comment"># axis = 1计算每一行的和</span></span><br><span class="line">        distance = squaredDist ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对距离递增排序获取最前面K个样本的种类并统计各种类出现次数</span></span><br><span class="line">        nearIds = distance.argsort() <span class="comment"># 按值排序，得到对应下标数组</span></span><br><span class="line">        classesCount = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">            y = y_train[nearIds[i]] <span class="comment"># 得到对应的种类</span></span><br><span class="line">            classesCount[y] = classesCount.get(y,<span class="number">0</span>)+<span class="number">1</span> <span class="comment"># 0为设置默认值</span></span><br><span class="line">        <span class="comment"># print(classesCount)</span></span><br><span class="line">        <span class="comment"># 对字典按值进行递减排序</span></span><br><span class="line">        sortClassesCount = <span class="built_in">sorted</span>(classesCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">                                                        <span class="comment"># 获取对象第二个元素           逆序</span></span><br><span class="line">        y_predict.append(sortClassesCount[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># 预测种类为出现次数最多的那一类</span></span><br><span class="line">    <span class="keyword">return</span> y_predict</span><br></pre></td></tr></table></figure><p>到这里，就已经手动实现了KNN算法，可以拿来预测类别了，确实很简单。</p><p>但我们还需要利用交叉验证集选择最好的K，本来想借助sklearn的交叉验证，但是对我们自己手写的KNN好像不太好用，所以还是自己写一个简易的吧。</p><p>不过像拆分训练集和计算Score这种事就交给sklearn吧。</p><h2 id="交叉验证选择最适k值">交叉验证选择最适K值</h2><p>注意，交叉验证集不能涉及到测试集！</p><p>这里用多次交叉验证计算score取均值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseK</span>(<span class="params">X_train_old,y_train_old,K = <span class="number">10</span>,C = <span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param X_train_old: 原训练特征集</span></span><br><span class="line"><span class="string">    :param y_train_old: 原训练目标集</span></span><br><span class="line"><span class="string">    :param K: 最大K</span></span><br><span class="line"><span class="string">    :param C: 计算次数</span></span><br><span class="line"><span class="string">    :return: 最好的K值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># K从1取到maxK，分别计算Score</span></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,K+<span class="number">1</span>):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,C+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 拆分训练集为训练集和验证集</span></span><br><span class="line">            X_train, X_check, y_train, y_check = train_test_split(X_train_old, y_train_old, train_size=<span class="number">0.6</span>)</span><br><span class="line">            y_predict = run_KNN(X_check,X_train,y_train,k)</span><br><span class="line">            <span class="comment"># 计算Score</span></span><br><span class="line">            score = score + f1_score(y_check, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">        score = score/(C)</span><br><span class="line">        <span class="comment"># print(f&quot;k = &#123;k&#125;,score = &#123;score&#125;&quot;)</span></span><br><span class="line">        scores.append(score)</span><br><span class="line"></span><br><span class="line">    scores = np.array(scores)</span><br><span class="line">    bestK = scores.argmax()</span><br><span class="line">    plt_KScore(scores,K,bestK)</span><br><span class="line">    <span class="keyword">return</span> bestK</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制K对应的Score图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plt_KScore</span>(<span class="params">scores,K,bestK</span>):</span><br><span class="line">    k_range = <span class="built_in">range</span>(<span class="number">1</span>,K+<span class="number">1</span>)</span><br><span class="line">    plt.plot(k_range,scores[<span class="number">1</span>:])</span><br><span class="line">    plt.scatter(bestK,scores[bestK],marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Value of K in KNN&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Score&#x27;</span>)</span><br><span class="line">    plt.show()<span class="string">&#x27;Score&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>下面将测试交叉验证集选择K值。</p><p>这里使用sklearn自带的鸢尾花数据集，标准化之前的文章已经手动实现过，这里直接用sklearn自带。</p><h2 id="导入数据和预处理">导入数据和预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    iris = datasets.load_iris()</span><br><span class="line">    dataSet = iris.data</span><br><span class="line">    target = iris.target</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(dataSet,target, train_size=<span class="number">0.7</span>, random_state=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 数据标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    X_train = scaler.fit_transform(X_train)  <span class="comment"># 标准化训练集X</span></span><br><span class="line">    X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure><p>现在开始测试交叉验证集选择K：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">start = time.time()</span><br><span class="line">K = chooseK(X_train,y_train,<span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;bestK is <span class="subst">&#123;K&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;run time is <span class="subst">&#123;end-start&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://img.issey.top/img/202209212353043.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bestK <span class="keyword">is</span> <span class="number">7</span></span><br><span class="line">run time <span class="keyword">is</span> <span class="number">2.0105364322662354</span></span><br></pre></td></tr></table></figure><p>实际上会发现每次计算出来的“BestK”都不一样，不过都在那几个值跳动，这是因为在分割测试集和验证集时没有设置随机种子的原因。</p><p>现在，让我们用最好的K值来测试分类效果吧！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    start = time.time()</span><br><span class="line">best_K = chooseK(X_train,y_train,<span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;bestK is <span class="subst">&#123;best_K&#125;</span>&quot;</span>)</span><br><span class="line">y_predict = run_KNN(X_test,X_train,y_train,best_K)</span><br><span class="line">score = f1_score(y_test, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;predict score is <span class="subst">&#123;score&#125;</span>&quot;</span>)</span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;run time is <span class="subst">&#123;end-start&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bestK <span class="keyword">is</span> <span class="number">4</span></span><br><span class="line">predict score <span class="keyword">is</span> <span class="number">0.9778242192035296</span></span><br><span class="line">run time <span class="keyword">is</span> <span class="number">0.400089502334594704</span></span><br></pre></td></tr></table></figure><p>可以看出bestK和上一次算的不一样，不过最后分类的结果还是非常好的。</p><h1id="利用sklearn实现knn完成鸢尾花分类">利用Sklearn实现KNN完成鸢尾花分类</h1><blockquote><p>关于sklearn封装的KNN的参数说明详细请参考文章： <ahref="https://blog.csdn.net/codedz/article/details/108862498?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166239390116782412571393%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=166239390116782412571393&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-108862498-null-null.142%5Ev46%5Econtrol&amp;utm_term=KNN&amp;spm=1018.2226.3001.4187">KNN算法详解及实现__dingzhen的博客-CSDN博客_knn</a></p><p>读懂每一个需要的参数很很很重要！！<strong>一定要去看参数说明</strong>。</p><p>sklearn的KNN实现方式包括了之前提到的<strong>线性扫描</strong>、<strong>KDTree</strong>和<strong>ballTree</strong>，没有规定特定参数时它会自己选则实现方法。详细说明都在链接里了。</p></blockquote><p>这次我们用Sklearn封装的KNN来完成上面提到的例子，即鸢尾花分类。同样也用了交叉验证寻找最佳K值。</p><p>详见代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证找最好K</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findBestK</span>(<span class="params">X_train,y_train,max_k = <span class="number">50</span></span>):</span><br><span class="line">    k_scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,max_k+<span class="number">1</span>):</span><br><span class="line">        Knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">        <span class="comment"># 交叉验证，scoring=&#x27;f1_weighted&#x27; 使用的与之前手写的交叉验证使用同一个F1-score规则</span></span><br><span class="line">        scores = cross_val_score(Knn,X_train,y_train,cv=<span class="number">5</span>,scoring=<span class="string">&#x27;f1_weighted&#x27;</span>)</span><br><span class="line">        k_scores.append(scores.mean())</span><br><span class="line">    k_scores = np.array(k_scores)</span><br><span class="line">    bestK = k_scores.argmax()</span><br><span class="line">    plt_KScore(k_scores,max_k,bestK)</span><br><span class="line">    <span class="keyword">return</span> bestK</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制K对应的Score图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plt_KScore</span>(<span class="params">scores,K,bestK</span>):</span><br><span class="line">    k_range = <span class="built_in">range</span>(<span class="number">1</span>,K+<span class="number">1</span>)</span><br><span class="line">    plt.plot(k_range,scores[<span class="number">1</span>:])</span><br><span class="line">    plt.scatter(bestK,scores[bestK],marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Value of K in KNN&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Score&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">    iris = datasets.load_iris()</span><br><span class="line">    dataSet = iris.data</span><br><span class="line">    target = iris.target</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(dataSet,target, train_size=<span class="number">0.7</span>, random_state=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 数据标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    X_train = scaler.fit_transform(X_train)  <span class="comment"># 标准化训练集X</span></span><br><span class="line">    X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 寻找最好K值</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    best_K = findBestK(X_train,y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;best_k is <span class="subst">&#123;best_K&#125;</span>&quot;</span>)</span><br><span class="line">    model = KNeighborsClassifier(n_neighbors=best_K)</span><br><span class="line">    model.fit(X_train,y_train)</span><br><span class="line">    y_predict = model.predict(X_test)</span><br><span class="line">    score = f1_score(y_test, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;predict score is <span class="subst">&#123;score&#125;</span>&quot;</span>)</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;run time is <span class="subst">&#123;end - start&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>交叉验证集选择K：</p><p><img src="https://img.issey.top/img/202209212354908.png" /></p><p>最终结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">best_k <span class="keyword">is</span> <span class="number">11</span></span><br><span class="line">predict score <span class="keyword">is</span> <span class="number">0.9778242192035296</span></span><br><span class="line">run time <span class="keyword">is</span> <span class="number">0.7521629333496094</span></span><br></pre></td></tr></table></figure><p>通过自己实现的KNN与sklearn实现的KNN对比，发现：</p><ol type="1"><li><p>自己实现的KNN寻找到的最佳K值不固定，而sklearn每次寻找的最佳K值都一样。导致自己实现的KNN最佳K值不固定的原因是拆分数据集和验证集时每次的随机种子不固定。偶尔有几次自己算出来的结果和sklearn算出来的结果完全一致。</p></li><li><p>使用同种实现方式（即线性扫描实现KNN）时，sklearn用的时间比自己实现的要长那么一点点，暂时不清楚原因。</p></li></ol><h1id="利用sklearn的knn完成手写数字识别">利用Sklearn的KNN完成手写数字识别</h1><p>到此为止，你已经学会了如何自己实现KNN算法，并且取得了不错的分类效果。现在我们用sklrean封装好的KNN来完成一些有趣的事吧！</p><p>手写数据识别是计算机视觉方面的入门级图像识别，一般都是用神经网络实现的，不过我们可以使用KNN来完成数据规模比较小的手写数字识别。</p><blockquote><p>在此之前，请确保你已经安装tensorflow和keras，我们的数据集将来自keras。</p><p>当然也可以自己找数据。</p></blockquote><p>需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br></pre></td></tr></table></figure><h2 id="导入数据并查看数字图像">导入数据并查看数字图像</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    (X_train,y_train),(X_test,y_test) = mnist.load_data()</span><br><span class="line">    <span class="comment"># 绘制训练集的部分数字图像</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.imshow(X_train[i],cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212354303.png" /></p><p>查看X_train训练集的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">    <span class="built_in">print</span>(X_train.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure><p>看得出是个三维的，大概意思是60000张图片，<spanclass="math inline">\(28\times28\)</span>像素，且是灰色(因为如果是彩色，应该是<spanclass="math inline">\(28\times28\times3\)</span>)</p><p>要用KNN算法，需要先将三维转化为二维矩阵<spanclass="math inline">\((60000,28 \times 28)\)</span>，每一行代表一张图片，共60000张。</p><h2 id="数据预处理">数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将训练集和测试集转化为二维数组</span></span><br><span class="line">X_train = X_train.reshape([-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">X_test = X_test.reshape([-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">X_train /= <span class="number">255</span></span><br><span class="line">X_test /= <span class="number">255</span></span><br></pre></td></tr></table></figure><h1 id="选择最佳k值">选择最佳K值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉验证法求最好K值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findBestK</span>(<span class="params">X_train,y_train,max_k = <span class="number">50</span></span>):</span><br><span class="line">    k_scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,max_k+<span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;k&#125;</span> time&quot;</span>)</span><br><span class="line">        Knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">        <span class="comment"># 交叉验证，scoring=&#x27;f1_weighted&#x27; 使用的与之前手写的交叉验证使用同一个F1-score规则</span></span><br><span class="line">        scores = cross_val_score(Knn,X_train,y_train,cv=<span class="number">5</span>,scoring=<span class="string">&#x27;f1_weighted&#x27;</span>)</span><br><span class="line">        k_scores.append(scores.mean())</span><br><span class="line">    k_scores = np.array(k_scores)</span><br><span class="line">    bestK = k_scores.argmax()</span><br><span class="line">    <span class="comment"># plt_KScore(k_scores,max_k,bestK)</span></span><br><span class="line">    <span class="keyword">return</span> bestK</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制K对应的Score图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plt_KScore</span>(<span class="params">scores,K,bestK</span>):</span><br><span class="line">    k_range = <span class="built_in">range</span>(<span class="number">1</span>,K+<span class="number">1</span>)</span><br><span class="line">    plt.plot(k_range,scores[<span class="number">1</span>:])</span><br><span class="line">    plt.scatter(bestK,scores[bestK],marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Value of K in KNN&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Score&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接主函数内</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="comment"># 选择最佳K值，这一步如果觉得耗时太长，其实可以去掉</span></span><br><span class="line"><span class="comment"># 为了缩短时间，我们只取10000个数据用于选择最佳K值</span></span><br><span class="line">best_K = findBestK(X_train[:<span class="number">10000</span>], y_train[:<span class="number">10000</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;best_k is <span class="subst">&#123;best_K&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;run time is <span class="subst">&#123;end - start&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_k <span class="keyword">is</span> <span class="number">4</span></span><br><span class="line">run time <span class="keyword">is</span> <span class="number">54.95526051521301</span></span><br></pre></td></tr></table></figure><p>这一步太慢了！！整整耗时55秒。强烈建议不要进行，如果需要最佳K值，上面已经算出来了。</p><h2 id="训练模型测试模型得分">训练模型，测试模型得分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">start = time.time()</span><br><span class="line">    <span class="comment"># 选择最佳K值，这一步如果觉得耗时太长，其实可以去掉</span></span><br><span class="line">    <span class="comment"># 为了缩短时间，我们只取10000个数据用于选择最佳K值</span></span><br><span class="line">    <span class="comment"># best_K = findBestK(X_train[:10000], y_train[:10000])</span></span><br><span class="line">    <span class="comment"># print(f&quot;best_k is &#123;best_K&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    model = KNeighborsClassifier(n_neighbors=<span class="number">4</span>)</span><br><span class="line">    model.fit(X_train,y_train)</span><br><span class="line">    <span class="comment"># 计算得分</span></span><br><span class="line">    y_predict = model.predict(X_test)</span><br><span class="line">    score = f1_score(y_test, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;predict score is <span class="subst">&#123;score&#125;</span>&quot;</span>)</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;run time is <span class="subst">&#123;end - start&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict score <span class="keyword">is</span> <span class="number">0.9681286795453637</span></span><br><span class="line">run time <span class="keyword">is</span> <span class="number">7.25964617729187</span></span><br></pre></td></tr></table></figure><p>可以看出模型很不错！到此为止，使用KNN进行手写数字识别可以说完成了。</p><p>但是，作为一个合格的玩具，它还没完。</p><p>接下来是可选内容：把它变成可以玩的玩具！</p><h1 id="完成手写数字识别玩具">完成手写数字识别玩具</h1><p>现在假设你拿到了这样一张图片：</p><p><img src="https://img.issey.top/img/202209212354594.png" /></p><p>很显然，它是彩色的，但是我们之前训练用的图片是灰色的，而且，中间是白色的,像这样：</p><p><img src="https://img.issey.top/img/202209212355641.png" /></p><p>另外，两张像素可能不一样，前者像素不确定，后者为<spanclass="math inline">\(28\times28\)</span>所以我们需要先把图片做一些预处理:</p><ol type="1"><li><p>将彩色图片变为灰色图片（即将三图层变为单图层）</p></li><li><p>将图片像素压缩为<spanclass="math inline">\(28\times28\)</span></p></li><li><p>可以将图片二值化（即0和255），可能识别效果会更好。</p></li></ol><h2 id="图片预处理">图片预处理</h2><p>接下来会用到opencv-python包，但是注意高版本的tensorflow和opencv-python可能出现不兼容的问题。最开始我用的tensorflow2.9.0，对应的numpy为1.23。但是安装opencv后会自动把numpy降为1.16，导致tensorflow用不了。所以这里可能需要切换一下环境。</p><p>我用的环境：tensorflow2.6.0+opencv4.5.5+numpy1.19.5</p><p>tensorflow版本降低后，数据集的导入可能会发生变化。</p><p>改变导入形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br></pre></td></tr></table></figure><h3 id="将彩色图片以灰度图片导入">将彩色图片以灰度图片导入</h3><p>查看原图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">&#x27;number1.png&#x27;</span> <span class="comment"># path为图片储存位置</span></span><br><span class="line"><span class="comment"># 原图像</span></span><br><span class="line">img1 = plt.imread(path) </span><br><span class="line">plt.imshow(img1) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212356531.png" /></p><p>以灰度图片导入并查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">imgOriginal = cv2.imread(path,<span class="number">0</span>)</span><br><span class="line">img = np.asarray(imgOriginal)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;before resize shape is:<span class="subst">&#123;img.shape&#125;</span>&quot;</span>)</span><br><span class="line">img = cv2.resize(img,(<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;after resize shape is:<span class="subst">&#123;img.shape&#125;</span>&quot;</span>)</span><br><span class="line">plt.imshow(img,cmap=<span class="string">&#x27;Greys_r&#x27;</span>) <span class="comment"># cmap=&#x27;Greys_r&#x27; 使plt显示灰度图像</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>如果不加cmap='Greys_r'，matplotlib会把这张灰色图片由低维（1维）映射到高维（3维），然后以彩色图片的形式展出，这显然不符合我们这里的需求。</p><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">before resize shape <span class="keyword">is</span>:(<span class="number">188</span>, <span class="number">186</span>)</span><br><span class="line">after resize shape <span class="keyword">is</span>:(<span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212357703.png" /></p><h3 id="黑白颠倒以及二值化">黑白颠倒以及二值化</h3><p>我们的样本数据集为黑底白字，所以需要将这张图片也要变为黑底白字。同时我们可以将其二值化，变为黑白图像：</p><p>这里的二值化只是简单实现了下，还有更好的二值化方法，以后再学了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二值化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Binarization</span>(<span class="params">img,Reverse = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param img:</span></span><br><span class="line"><span class="string">    :param Reverse: 是否黑白颠倒</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> Reverse == <span class="literal">False</span>:</span><br><span class="line">        img = np.where(img &gt; <span class="number">160</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = np.where(img &gt; <span class="number">160</span>,<span class="number">0</span>,<span class="number">255</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><p>现在将图片二值化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = Binarization(img,Reverse = <span class="literal">True</span>)</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&#x27;Greys_r&#x27;</span>)  <span class="comment"># cmap=&#x27;Greys_r&#x27; 使plt显示灰度图像</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212357103.png" /></p><p>最后不要忘记了归一化和转化矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = img/<span class="number">255.0</span></span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br></pre></td></tr></table></figure><h2 id="使用模型识别手写数字">使用模型识别手写数字</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict = model.predict(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;手写数字识别为:<span class="subst">&#123;predict[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">手写数字识别为:<span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="完整代码">完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证法求最好K值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findBestK</span>(<span class="params">X_train,y_train,max_k = <span class="number">50</span></span>):</span><br><span class="line">    k_scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,max_k+<span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;k&#125;</span> time&quot;</span>)</span><br><span class="line">        Knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">        <span class="comment"># 交叉验证，scoring=&#x27;f1_weighted&#x27; 使用的与之前手写的交叉验证使用同一个F1-score规则</span></span><br><span class="line">        scores = cross_val_score(Knn,X_train,y_train,cv=<span class="number">5</span>,scoring=<span class="string">&#x27;f1_weighted&#x27;</span>)</span><br><span class="line">        k_scores.append(scores.mean())</span><br><span class="line">    k_scores = np.array(k_scores)</span><br><span class="line">    bestK = k_scores.argmax()</span><br><span class="line">    <span class="comment"># plt_KScore(k_scores,max_k,bestK)</span></span><br><span class="line">    <span class="keyword">return</span> bestK</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制K对应的Score图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plt_KScore</span>(<span class="params">scores,K,bestK</span>):</span><br><span class="line">    k_range = <span class="built_in">range</span>(<span class="number">1</span>,K+<span class="number">1</span>)</span><br><span class="line">    plt.plot(k_range,scores[<span class="number">1</span>:])</span><br><span class="line">    plt.scatter(bestK,scores[bestK],marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Value of K in KNN&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Score&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型得分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_score</span>(<span class="params">model,X_test,y_test</span>):</span><br><span class="line">    y_predict = model.predict(X_test)</span><br><span class="line">    score = f1_score(y_test, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;predict score is <span class="subst">&#123;score&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二值化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Binarization</span>(<span class="params">img,Reverse</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param img:</span></span><br><span class="line"><span class="string">    :param Reverse: 是否黑白颠倒</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> Reverse == <span class="literal">False</span>:</span><br><span class="line">        img = np.where(img &gt; <span class="number">160</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = np.where(img &gt; <span class="number">160</span>,<span class="number">0</span>,<span class="number">255</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 识别数字</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">identification_number</span>(<span class="params">path,model = <span class="literal">None</span>,Reverse = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># 原图像</span></span><br><span class="line">    <span class="comment"># img1 = plt.imread(path)</span></span><br><span class="line">    <span class="comment"># plt.imshow(img1)</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    imgOriginal = cv2.imread(path,<span class="number">0</span>)</span><br><span class="line">    img = np.asarray(imgOriginal)</span><br><span class="line">    img = cv2.resize(img,(<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">    <span class="comment"># plt.imshow(img,cmap=&#x27;Greys_r&#x27;) # cmap=&#x27;Greys_r&#x27; 使plt显示灰度图像</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    img = Binarization(img,Reverse)</span><br><span class="line">    <span class="comment"># plt.imshow(img, cmap=&#x27;Greys_r&#x27;)  # cmap=&#x27;Greys_r&#x27; 使plt显示灰度图像</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    X = img/<span class="number">255.0</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    predict = model.predict(X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;手写数字识别为:<span class="subst">&#123;predict[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    (X_train,y_train),(X_test,y_test) = mnist.load_data()</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    X_train = X_train /<span class="number">255.0</span></span><br><span class="line">    X_test = X_test /<span class="number">255.0</span></span><br><span class="line">    <span class="comment"># 将训练集和测试集转化为二维数组</span></span><br><span class="line">    X_train = X_train.reshape([-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">    X_test = X_test.reshape([-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start = time.time()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择最佳K值，这一步如果觉得耗时太长，其实可以去掉</span></span><br><span class="line">    <span class="comment"># 为了缩短时间，我们只取10000个数据用于选择最佳K值</span></span><br><span class="line">    <span class="comment"># best_K = findBestK(X_train[:10000], y_train[:10000])</span></span><br><span class="line">    <span class="comment"># print(f&quot;best_k is &#123;best_K&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建模型，喂数据</span></span><br><span class="line">    model = KNeighborsClassifier(n_neighbors=<span class="number">4</span>)</span><br><span class="line">    model.fit(X_train,y_train)</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # 计算得分</span></span><br><span class="line">    <span class="comment"># cal_score(model,X_test,y_test)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 识别图片</span></span><br><span class="line">    img_path = <span class="string">&#x27;number1.png&#x27;</span></span><br><span class="line">    identification_number(img_path,model,Reverse=<span class="literal">True</span>) <span class="comment"># 在这里修改是否需要颠倒图像</span></span><br><span class="line">    <span class="comment"># end = time.time()</span></span><br></pre></td></tr></table></figure><h2 id="对模型的总结和评价">对模型的总结和评价</h2><p>实际上因为没有用神经网络来做这个手写数字识别模型，加上二值化写的比较草率，它对于真实手写数字识别效果并没有预料的好。待会儿会举两个识别自己手写数字的例子。</p><p>在二值化时阈值的选择影响比较大，最开始阈值设置的200，结果自己手写的数字二值化后变成了一片白或黑色。后来调整到160-170左右，感觉比较合适。</p><p>咱也是第一次做手写数字识别玩具，整个过程还是很有意思的。</p><p>自己写的数字：</p><p><img src="https://img.issey.top/img/202209212358266.jpg" /></p><p>二值化和预测结果：</p><p><img src="https://img.issey.top/img/202209212358121.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">手写数字识别为:<span class="number">5</span></span><br></pre></td></tr></table></figure><p>识别失败：</p><p><img src="https://img.issey.top/img/202209212358309.jpg" /></p><p><img src="https://img.issey.top/img/202209212359068.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">手写数字识别为:<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记8——K-mean聚类</title>
      <link href="/article/a4ce0017872e/"/>
      <url>/article/a4ce0017872e/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：请确保已知晓无监督学习和聚类的定义。</p></blockquote><h1 id="k-mean聚类算法">K-mean聚类算法</h1><p>K-mean又称K均值算法，意思是将数据集分为K簇，每个簇以属于该簇的数据集均值为该簇的中心（或质心）。K-mean算法流程大致为：</p><ol type="1"><li><p>首先随机选择K个初始聚类中心。</p></li><li><p>计算各个点到每个聚类中心的距离，将各个点归到距离最近的簇中。</p></li><li><p>用每个簇的数据重新计算各自的中心。</p></li><li><p>重复2、3过程，直到簇中心不再变化或者超过最大迭代次数。</p></li></ol><p>可以看出，K-mean算法相对来说很简单，容易实现。但也导致它有一些缺点，关于K-mean算法的优缺点将在后面讨论。</p><p>在上述过程中，第二步中提到了计算距离，接下来先讨论距离计算。</p><h1 id="距离计算">距离计算</h1><p>不同情况的距离度量选择是不同的，下面介绍常用的三种：</p><h2 id="欧氏距离">欧氏距离</h2><p><span class="math inline">\(d(x,y) =\sqrt{(x_1-y_1)^2+...+(x_n-y_n)^2} =\sqrt{\sum_{i=1}^n(x_i-y_i)^2}\)</span></p><h2 id="曼哈顿距离">曼哈顿距离</h2><p><span class="math inline">\(d(x,y) =\sum_{i=1}^n|x_i-y_i|\)</span></p><h2 id="余弦相似度">余弦相似度</h2><p><span class="math inline">\(\cos(\theta) = \frac{\vec A\cdot\vecB}{||\vec A||*||\vec B||}\)</span></p><h1 id="k-mean算法实现">K-mean算法实现</h1><p>数据集可以私发</p><p>需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br></pre></td></tr></table></figure><p>StandardScal包是拿来标准化数据集的，关于手动实现Z-score标准化请见：</p><p><ahref="https://blog.csdn.net/qq_52466006/article/details/126066995?spm=1001.2014.3001.5501">多元线性回归模型_TwilightSparkle.的博客-CSDN博客</a></p><p>下面每一步都对照上面说到的步骤。</p><h2 id="步骤1随机选择初始质心">步骤1：随机选择初始质心</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机选择样本最为初始质心</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_centroid</span>(<span class="params">daraSet,K</span>):</span><br><span class="line">    randIdx = np.random.permutation(daraSet.shape[<span class="number">0</span>])</span><br><span class="line">    centroids = daraSet[randIdx[:K]]</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure><h2id="步骤2欧式距离计算以及寻找最近质心">步骤2：欧式距离计算以及寻找最近质心</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 寻找最近质心索引</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_closed_centroid</span>(<span class="params">dataSet,centroids</span>):</span><br><span class="line">    K = centroids.shape[<span class="number">0</span>] <span class="comment"># 质心个数</span></span><br><span class="line">    idx = np.zeros(dataSet.shape[<span class="number">0</span>],dtype=<span class="built_in">int</span>) <span class="comment"># 样本到最近质心的索引</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dataSet.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># 计算一个样本到所有质心的欧氏距离</span></span><br><span class="line">        diff = np.tile(dataSet[i],(K,<span class="number">1</span>)) - centroids <span class="comment"># tile可以复制矩阵,用法自查</span></span><br><span class="line">        squaredDist = np.<span class="built_in">sum</span>(diff**<span class="number">2</span>,axis=<span class="number">1</span>) <span class="comment"># axis = 1计算每一行的和</span></span><br><span class="line">        distance = squaredDist ** <span class="number">0.5</span></span><br><span class="line">        <span class="comment"># 选出距离最小的质点索引</span></span><br><span class="line">        idx[i] = np.argmin(distance)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure><h2id="步骤3根据分好的簇重新计算质心">步骤3：根据分好的簇重新计算质心</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算质心</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_centroids</span>(<span class="params">dataSet,idx,K,oldCentroids</span>):</span><br><span class="line">    m,n = dataSet.shape</span><br><span class="line">    centroids = np.zeros((K, n))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        <span class="comment"># 提取属于同一类的样本</span></span><br><span class="line">        points = np.array(dataSet[np.where(idx == k)])</span><br><span class="line">        <span class="comment"># 计算质心</span></span><br><span class="line">        centroids[k] = points.mean(axis=<span class="number">0</span>) <span class="comment"># 计算每一列的均值</span></span><br><span class="line">        <span class="comment"># 计算变化量</span></span><br><span class="line">    changed = centroids - oldCentroids</span><br><span class="line">    <span class="keyword">return</span> centroids,changed</span><br></pre></td></tr></table></figure><h2 id="步骤4组合k-mean各模块">步骤4：组合K-mean各模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 组合K—mean</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_Kmean</span>(<span class="params">dataSet,K,max_iters = <span class="number">100</span></span>):</span><br><span class="line">    m,n = dataSet.shape</span><br><span class="line">    idx = []</span><br><span class="line">    <span class="comment"># 初始化质点</span></span><br><span class="line">    centroids = init_centroid(dataSet,K)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iters):</span><br><span class="line">        <span class="comment"># 寻找最近质点(分组)</span></span><br><span class="line">        idx = find_closed_centroid(dataSet,centroids)</span><br><span class="line">        <span class="comment"># 计算新质点</span></span><br><span class="line">        centroids,changed = compute_centroids(dataSet,idx,K,centroids)</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">all</span>(changed == <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure><h2 id="绘图">绘图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Kmean_plot</span>(<span class="params">dataSet,idx,K</span>):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        <span class="comment"># 提取属于同一类的样本</span></span><br><span class="line">        points = np.array(dataSet[np.where(idx == k)])</span><br><span class="line">        plt.scatter(points[:, <span class="number">0</span>], points[:, <span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="主函数含数据标准化">主函数（含数据标准化）</h2><p>K-mean算法需要数据标准化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataSet = np.load(<span class="string">&#x27;ex7_X.npy&#x27;</span>)</span><br><span class="line">    <span class="comment"># K-mean需要数据标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    dataSet = scaler.fit_transform(dataSet)</span><br><span class="line">    K = <span class="number">3</span></span><br><span class="line">    classification = run_Kmean(dataSet,K)</span><br><span class="line">    Kmean_plot(dataSet,classification,K)</span><br></pre></td></tr></table></figure><h2 id="运行结果">运行结果：</h2><p><img src="https://img.issey.top/img/202209212345412.png" /></p><h1 id="使用k-mean进行图像压缩">使用K-mean进行图像压缩</h1><p>现在用K-mean来做一些有趣的事情吧！</p><p>原始图像：</p><p><img src="https://img.issey.top/img/202209212346032.png" /></p><p>我们需要将这张图像变为只有8个颜色构成的图像。</p><h2 id="数据预处理">数据预处理</h2><p>需要将图片转化为2维矩阵，这张图是<spanclass="math inline">\(128\times128\times3\)</span>的，需要变为<spanclass="math inline">\((128\times128,3)\)</span>的矩阵，并且需要归一化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">original_img = plt.imread(<span class="string">&#x27;bird_small.png&#x27;</span>)</span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">original_img/=<span class="number">255</span></span><br><span class="line"><span class="comment"># 将图像转化为m*3的二维矩阵，m为图像像素，这里m应为128*128。</span></span><br><span class="line">X_img = np.reshape(original_img, (original_img.shape[<span class="number">0</span>] * original_img.shape[<span class="number">1</span>], <span class="number">3</span>))</span><br><span class="line"><span class="comment"># print(X_img.shape)</span></span><br></pre></td></tr></table></figure><h2 id="压缩图像">压缩图像</h2><p>需要在函数run_Kmean的返回值里多加一个centroids，其余代码不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">K = <span class="number">8</span></span><br><span class="line"><span class="comment"># max_iters = 10</span></span><br><span class="line">idx,centroids = run_Kmean(X_img,K)</span><br><span class="line"><span class="comment"># 现在，将每一个像素覆盖为它所属的簇质心的值</span></span><br><span class="line">X_recovered = centroids[idx]</span><br><span class="line">X_recovered = np.reshape(X_recovered, original_img.shape)</span><br></pre></td></tr></table></figure><h2 id="绘制图像">绘制图像</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Display original image</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax[<span class="number">0</span>].imshow(original_img * <span class="number">255</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Original&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_axis_off() <span class="comment"># 不显示坐标轴</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Display compressed image</span></span><br><span class="line">ax[<span class="number">1</span>].imshow(X_recovered * <span class="number">255</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Compressed with %d colours&#x27;</span> % K)</span><br><span class="line">ax[<span class="number">1</span>].set_axis_off()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://img.issey.top/img/202209212347246.png" /></p><h1 id="使用sklearn的k-mean">使用sklearn的K-mean</h1><p>网上随便找了张图片，来压缩吧！</p><p><img src="https://img.issey.top/img/202209212347019.png" /></p><p>写代码时发现一个很奇怪的问题，这张图它居然是4个图层的，变为二维数组时需要注意一下，不是变成<spanclass="math inline">\(m*3\)</span> 而是<spanclass="math inline">\(m*4\)</span>。(后来发现只要是QQ截图的都是4图层的。)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    original_img = plt.imread(<span class="string">&#x27;color.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shape of original_img is:&quot;</span>, original_img.shape)</span><br><span class="line">    original_img /= <span class="number">255</span></span><br><span class="line">    X_img = np.reshape(original_img, (original_img.shape[<span class="number">0</span>] * original_img.shape[<span class="number">1</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    K = <span class="number">8</span></span><br><span class="line">    model = KMeans(n_clusters=K)</span><br><span class="line">    model.fit(X_img)</span><br><span class="line">    centroids = model.cluster_centers_</span><br><span class="line">    <span class="comment"># labels得到的是质心索引</span></span><br><span class="line">    labels = model.predict(X_img)</span><br><span class="line">    <span class="comment"># print(labels[:6])</span></span><br><span class="line">    <span class="comment"># 替换样本</span></span><br><span class="line">    X_recovered = centroids[labels]</span><br><span class="line">    X_recovered = np.reshape(X_recovered, original_img.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(X_recovered*<span class="number">255</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>压缩后：</p><p><img src="https://img.issey.top/img/202209212347108.png" /></p><h1 id="k-mean算法的缺点">K-mean算法的缺点</h1><ol type="1"><li><p>算法中的K是事先给定的，但是K的选择在许多时候是难以估计的，很多时候不知道数据集应该分为多少类最合适。</p></li><li><p>K-mean初始质心选择如果选的不好，有可能无法得到有效的聚类结果。可能会局部收敛而非全局收敛。为了解决该问题，提出了二分K-mean算法，实验表明二分K-mean的聚类效果要好于普通的K-mean算法。</p></li><li><p>数据集较大时收敛较慢。</p></li><li><p>如果遇到非球状的数据，K-mean算法不适用。</p></li></ol><p><img src="https://img.issey.top/img/202209212348571.png" /></p><p>该图来源：<ahref="https://blog.csdn.net/qq_43741312/article/details/97128745">K-means聚类算法原理及python实现_杨Zz.的博客-CSDN博客_kmeans算法python实现</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类算法 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记7——决策树原理与应用</title>
      <link href="/article/2f3e74c9632f/"/>
      <url>/article/2f3e74c9632f/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：关于决策树的含义这里就不详细解释了，简单来说决策树就是根据数据集的最优特征构造一棵树，再用这棵树来预测新的数据属于哪一类。决策树是一种基本的分类算法。（实际上也可以用于回归，这里不做讨论）</p><p>参考文章：<ahref="https://blog.csdn.net/jiaoyangwm/article/details/79525237">机器学习实战（三）——决策树_呆呆的猫的博客-CSDN博客_决策树</a></p><p><ahref="https://blog.csdn.net/fuqiuai/article/details/79456971?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166202030616782425158024%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=166202030616782425158024&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79456971-null-null.142%5Ev44%5Econtrol&amp;utm_term=C4.5&amp;spm=1018.2226.3001.4187">数据挖掘领域十大经典算法之—C4.5算法（超详细附代码）_fuqiuai的博客-CSDN博客_c4.5</a></p></blockquote><h1 id="构造决策树的算法">构造决策树的算法</h1><p>一般构建决策树的算法有三种：ID3，C4.5，CART。此三种算法的区别在于：</p><ul><li><p>ID3：特征划分基于信息增益。ID3 仅仅适用于分类问题；ID3仅仅能够处理离散属性。</p></li><li><p>C4.5：特征划分基于信息增益比。可以处理连续值。</p></li><li><p>CART：特征划分基于基尼系数。并且CART算法只能构造出二叉树。CART既可以用于分类问题，也可以用于回归问题。</p></li></ul><h1 id="信息增益与信息增益比">信息增益与信息增益比</h1><h2 id="熵">熵</h2><p>简单来说，在这里熵是用来衡量数据集纯净度的。熵的值越小，代表数据集纯净度越高。反之，熵越大代表数据集越混乱。</p><p>熵的计算公式为：</p><p><span class="math inline">\(H =-\sum_{i=1}^np(x_i)log_2p(x_i)\)</span></p><p><span class="math inline">\(x_i\)</span>为第i个分类，<spanclass="math inline">\(p(x_i)\)</span>为选择该分类的概率。</p><h2 id="经验熵">经验熵</h2><p>当上诉的<spanclass="math inline">\(p(x_i)\)</span>（即选则某一类的概率）是由数据估计（特别是最大似然估计）得到时，此时的熵被称为经验熵。举个例子：</p><table><thead><tr class="header"><th style="text-align: center;">类别</th><th style="text-align: center;">个数</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">A</td><td style="text-align: center;">4</td></tr><tr class="even"><td style="text-align: center;">B</td><td style="text-align: center;">6</td></tr></tbody></table><p>如果<span class="math inline">\(p(A) = \frac 4 {10},P(B) = \frac 6{10}\)</span>,那么此时算出的熵就是经验熵。</p><p>经验熵的公式可以写为：</p><p><span class="math inline">\(H(D) = -\sum_{k=1}^n \frac{|c_k|}{|D|}log_2\frac {|c_k|}{|D|}\)</span></p><p>使用上式可算出例中的经验熵<spanclass="math inline">\(H(D)\)</span>为：</p><p>$H(D) = -log_2-log_2 = 0.971 $</p><h2 id="条件熵">条件熵</h2><p>条件熵<spanclass="math inline">\(H(Y|X)\)</span>表示在已知随机变量<spanclass="math inline">\(X\)</span>的条件下，随机变量<spanclass="math inline">\(Y\)</span>的不确定性。</p><p>公式：</p><p><span class="math display">\[\begin{split} H(Y|X) &amp;= \sum_{i=1}^nP(x_i)H(Y|X = x_i)\\&amp;=-\sum_{i=1}^nP(x_i)\sum_{j=1}^mP(y_i|x_i)log_2P(y_i|x_i)\\&amp;=-\sum_{i=1}^n\sum_{j=1}^mP(x_i,y_j)log_2P(y_j|x_i)   \end{split}\]</span></p><p>这个条件熵，可能最开始不太好理解，建议参考这篇文章，里面有详细的说明和例题，这里就不举例了。<ahref="https://blog.csdn.net/xwd18280820053/article/details/70739368">通俗理解条件熵_AI_盲的博客-CSDN博客_条件熵</a></p><p>经验熵对应的条件熵为<strong>经验条件熵</strong>，一般我们计算的都是经验条件熵。<strong>注：经验条件熵必须会自己手算</strong>。</p><p>特别的，令<span class="math inline">\(0log_20 = 0\)</span>。</p><h2 id="信息增益">信息增益</h2><p>信息增益指知道了某个条件后，事件的不确定性的下降程度。特征A对训练集D的信息增益<spanclass="math inline">\(g(D,A)\)</span>被定义为集合D的经验熵<spanclass="math inline">\(H(D)\)</span>与给定特征A的条件下的经验条件熵<spanclass="math inline">\(H(D|A)\)</span>之差：</p><p><span class="math inline">\(G(D,A)=H(D)-H(D|A)\)</span></p><p>如果一个特征的信息增益越大，说明使用这个特征划分后的样本集可以拥有更好的纯净度。所以在ID3算法中，每一轮我们会计算每个未使用特征的信息增益，然后选择信息增益最大的那个特征作为划分节点。但是这样会有个问题，假设每个属性中每种类别都只有一个样本，那这样属性信息熵就等于零，根据信息增益就无法选择出有效分类特征。</p><p>所以提出了信息增益率作为标准的C4.5算法。</p><h2 id="信息增益率">信息增益率</h2><p>信息增益率<spanclass="math inline">\(GainRatio(D,A)\)</span>由信息增益<spanclass="math inline">\(G(D,A)\)</span>和分裂信息度量<spanclass="math inline">\(SplitInformation(D,A)\)</span>共同定义，公式如下：</p><p><span class="math inline">\(GainRatio(D,A) =\frac{G(D,A)}{SplitInformation(D,A)}\)</span></p><p><span class="math inline">\(SplitInformation(D,A) =-\sum_{i=1}^n\frac{|S_i|}{|S|}log_2\frac{|S_i|}{S}\)</span></p><p>也有人把信息增益率的公式写为：</p><p><span class="math inline">\(GainRatio(D,A) =\frac{G(D,A)}{H(D)}\)</span>，<spanclass="math inline">\(H(D)\)</span>为训练集D的经验熵。</p><p>需要注意的是，增益率准则对可取值数目较少的属性所有偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p><h2 id="关于cart算法和基尼系数">关于CART算法和基尼系数</h2><p>不在这里进行说明，之后可能会单独开一篇文章。</p><h1 id="决策树的构建">决策树的构建</h1><p>上面已经介绍了构建决策树需要用到的子模块，即经验熵的计算和最优特征的选择。接下来介绍决策树的整体构建步骤。</p><p>构建决策树的核心都是递归算法，结束递归的条件又多种，将在之后的决策树剪枝中介绍。</p><h2 id="id3算法">ID3算法</h2><p>核心是在各个节点上对应信息增益准则选择特征，递归构建决策树。</p><p>具体方法：</p><ol type="1"><li><p>从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。</p></li><li><p>由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到达到预剪枝条件或没有特征可以选择为止。</p></li><li><p>得到一个决策树。</p></li></ol><h2 id="c4.5算法">C4.5算法</h2><p>与ID3算法的区别在于将信息增益率作为选择特征的标准。</p><h1 id="决策树的剪枝">决策树的剪枝</h1><p>如果不进行剪枝，那么决策树生成算法递归地产生决策树，直到没有特征选择为止。这样生成的决策树往往会导致<strong>过拟合</strong>。过拟合在前面的文章已经解释过，这里不再进行说明。为了防止过拟合，我们需要对决策树进行剪枝。</p><p>决策树的剪枝分为预剪枝和后剪枝两大类。</p><h2 id="预剪枝">预剪枝</h2><p>预剪枝指在生成决策树的过程中提前停止树的生长。常用的预剪枝有以下几种：</p><ol type="1"><li><p>当树到达一定深度的时候，停止树的生长。</p></li><li><p>当信息增益，增益率和基尼指数增益小于某个阈值的时候不在生长。</p></li><li><p>达到当前节点的样本数量小于某个阈值的时候。</p></li><li><p>计算每次分裂对测试集的准确性提升，当小于某个阈值，或不再提升甚至有所下降时，停止生长。</p></li></ol><p>优缺点：</p><p>优点：思想简单，算法高效，采用了贪心的思想，适合大规模问题。<br />缺点：提前停止生长，有可能存在欠拟合的风险。</p><h2 id="后剪枝">后剪枝</h2><p>后剪枝是先从训练集生成一颗完整的决策树，然后自底向上的对决策树进行剪枝。后剪枝一般不常用，这里暂时不拓展。贴一个连接：<ahref="https://blog.csdn.net/zr1213159840/article/details/112334211">西瓜书决策树预剪枝后剪枝过程详解_铁锤2号的博客-CSDN博客</a></p><p>缺点：耗时太长。</p><h1 id="利用sklearn实现决策树">利用sklearn实现决策树</h1><blockquote><p>关于决策树的手动实现：<ahref="https://blog.csdn.net/jiaoyangwm/article/details/79525237">机器学习实战（三）——决策树_呆呆的猫的博客-CSDN博客_决策树</a></p><p>这篇文章写的十分完整，甚至包括决策树的储存与绘图，这里就不贴手动实现的代码了，因为感觉贴了也和他的长得几乎一样？并且文章中还提到了sklearn决策树的各种参数说明，可以说是十分详尽了。不过他代码中注释写错了一些，自己跟着走一遍就看得懂了。</p></blockquote><h2 id="导入并分割数据集">导入并分割数据集</h2><p>数据集下载：<ahref="https://www.kaggle.com/datasets/brynja/wineuci">Classifying winevarieties | Kaggle</a></p><p>这个数据集sklearn其实已经内置了，只需要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line">wine = load_wine()</span><br><span class="line"><span class="built_in">print</span>(wine.data) </span><br><span class="line"><span class="built_in">print</span>(wine.target) <span class="comment"># 分类</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.423e+01</span> <span class="number">1.710e+00</span> <span class="number">2.430e+00</span> ... <span class="number">1.040e+00</span> <span class="number">3.920e+00</span> <span class="number">1.065e+03</span>]</span><br><span class="line"> [<span class="number">1.320e+01</span> <span class="number">1.780e+00</span> <span class="number">2.140e+00</span> ... <span class="number">1.050e+00</span> <span class="number">3.400e+00</span> <span class="number">1.050e+03</span>]</span><br><span class="line"> [<span class="number">1.316e+01</span> <span class="number">2.360e+00</span> <span class="number">2.670e+00</span> ... <span class="number">1.030e+00</span> <span class="number">3.170e+00</span> <span class="number">1.185e+03</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">1.327e+01</span> <span class="number">4.280e+00</span> <span class="number">2.260e+00</span> ... <span class="number">5.900e-01</span> <span class="number">1.560e+00</span> <span class="number">8.350e+02</span>]</span><br><span class="line"> [<span class="number">1.317e+01</span> <span class="number">2.590e+00</span> <span class="number">2.370e+00</span> ... <span class="number">6.000e-01</span> <span class="number">1.620e+00</span> <span class="number">8.400e+02</span>]</span><br><span class="line"> [<span class="number">1.413e+01</span> <span class="number">4.100e+00</span> <span class="number">2.740e+00</span> ... <span class="number">6.100e-01</span> <span class="number">1.600e+00</span> <span class="number">5.600e+02</span>]]</span><br><span class="line">[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>可以更直观的展示数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更直观的展示</span></span><br><span class="line"><span class="built_in">print</span>(pd.concat([pd.DataFrame(wine.data), pd.DataFrame(wine.target)], axis=<span class="number">1</span>)) <span class="comment"># 按列拼接</span></span><br><span class="line"><span class="built_in">print</span>(wine.feature_names) <span class="comment"># 特征名称</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">        <span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>      <span class="number">4</span>     <span class="number">5</span>   ...    <span class="number">8</span>      <span class="number">9</span>     <span class="number">10</span>    <span class="number">11</span>      <span class="number">12</span>  <span class="number">0</span> </span><br><span class="line"><span class="number">0</span>    <span class="number">14.23</span>  <span class="number">1.71</span>  <span class="number">2.43</span>  <span class="number">15.6</span>  <span class="number">127.0</span>  <span class="number">2.80</span>  ...  <span class="number">2.29</span>   <span class="number">5.64</span>  <span class="number">1.04</span>  <span class="number">3.92</span>  <span class="number">1065.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">13.20</span>  <span class="number">1.78</span>  <span class="number">2.14</span>  <span class="number">11.2</span>  <span class="number">100.0</span>  <span class="number">2.65</span>  ...  <span class="number">1.28</span>   <span class="number">4.38</span>  <span class="number">1.05</span>  <span class="number">3.40</span>  <span class="number">1050.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">13.16</span>  <span class="number">2.36</span>  <span class="number">2.67</span>  <span class="number">18.6</span>  <span class="number">101.0</span>  <span class="number">2.80</span>  ...  <span class="number">2.81</span>   <span class="number">5.68</span>  <span class="number">1.03</span>  <span class="number">3.17</span>  <span class="number">1185.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">14.37</span>  <span class="number">1.95</span>  <span class="number">2.50</span>  <span class="number">16.8</span>  <span class="number">113.0</span>  <span class="number">3.85</span>  ...  <span class="number">2.18</span>   <span class="number">7.80</span>  <span class="number">0.86</span>  <span class="number">3.45</span>  <span class="number">1480.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">4</span>    <span class="number">13.24</span>  <span class="number">2.59</span>  <span class="number">2.87</span>  <span class="number">21.0</span>  <span class="number">118.0</span>  <span class="number">2.80</span>  ...  <span class="number">1.82</span>   <span class="number">4.32</span>  <span class="number">1.04</span>  <span class="number">2.93</span>   <span class="number">735.0</span>   <span class="number">0</span></span><br><span class="line">..     ...   ...   ...   ...    ...   ...  ...   ...    ...   ...   ...     ...  ..</span><br><span class="line"><span class="number">173</span>  <span class="number">13.71</span>  <span class="number">5.65</span>  <span class="number">2.45</span>  <span class="number">20.5</span>   <span class="number">95.0</span>  <span class="number">1.68</span>  ...  <span class="number">1.06</span>   <span class="number">7.70</span>  <span class="number">0.64</span>  <span class="number">1.74</span>   <span class="number">740.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">174</span>  <span class="number">13.40</span>  <span class="number">3.91</span>  <span class="number">2.48</span>  <span class="number">23.0</span>  <span class="number">102.0</span>  <span class="number">1.80</span>  ...  <span class="number">1.41</span>   <span class="number">7.30</span>  <span class="number">0.70</span>  <span class="number">1.56</span>   <span class="number">750.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">175</span>  <span class="number">13.27</span>  <span class="number">4.28</span>  <span class="number">2.26</span>  <span class="number">20.0</span>  <span class="number">120.0</span>  <span class="number">1.59</span>  ...  <span class="number">1.35</span>  <span class="number">10.20</span>  <span class="number">0.59</span>  <span class="number">1.56</span>   <span class="number">835.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">176</span>  <span class="number">13.17</span>  <span class="number">2.59</span>  <span class="number">2.37</span>  <span class="number">20.0</span>  <span class="number">120.0</span>  <span class="number">1.65</span>  ...  <span class="number">1.46</span>   <span class="number">9.30</span>  <span class="number">0.60</span>  <span class="number">1.62</span>   <span class="number">840.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">177</span>  <span class="number">14.13</span>  <span class="number">4.10</span>  <span class="number">2.74</span>  <span class="number">24.5</span>   <span class="number">96.0</span>  <span class="number">2.05</span>  ...  <span class="number">1.35</span>   <span class="number">9.20</span>  <span class="number">0.61</span>  <span class="number">1.60</span>   <span class="number">560.0</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">[<span class="number">178</span> rows x <span class="number">14</span> columns]</span><br><span class="line">[<span class="string">&#x27;alcohol&#x27;</span>, <span class="string">&#x27;malic_acid&#x27;</span>, <span class="string">&#x27;ash&#x27;</span>, <span class="string">&#x27;alcalinity_of_ash&#x27;</span>, <span class="string">&#x27;magnesium&#x27;</span>, <span class="string">&#x27;total_phenols&#x27;</span>, <span class="string">&#x27;flavanoids&#x27;</span>, <span class="string">&#x27;nonflavanoid_phenols&#x27;</span>, <span class="string">&#x27;proanthocyanins&#x27;</span>, <span class="string">&#x27;color_intensity&#x27;</span>, <span class="string">&#x27;hue&#x27;</span>, <span class="string">&#x27;od280/od315_of_diluted_wines&#x27;</span>, <span class="string">&#x27;proline&#x27;</span>]</span><br></pre></td></tr></table></figure><p>分割数据集为训练集和测试集（7：3）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test,y_train,y_test = train_test_split(wine.data,wine.target,train_size=<span class="number">0.7</span>)</span><br><span class="line"><span class="built_in">print</span>(pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">        <span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>      <span class="number">4</span>     <span class="number">5</span>   ...    <span class="number">8</span>     <span class="number">9</span>     <span class="number">10</span>    <span class="number">11</span>      <span class="number">12</span>  <span class="number">0</span> </span><br><span class="line"><span class="number">0</span>    <span class="number">12.08</span>  <span class="number">1.83</span>  <span class="number">2.32</span>  <span class="number">18.5</span>   <span class="number">81.0</span>  <span class="number">1.60</span>  ...  <span class="number">1.64</span>  <span class="number">2.40</span>  <span class="number">1.08</span>  <span class="number">2.27</span>   <span class="number">480.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">1</span>    <span class="number">11.82</span>  <span class="number">1.72</span>  <span class="number">1.88</span>  <span class="number">19.5</span>   <span class="number">86.0</span>  <span class="number">2.50</span>  ...  <span class="number">1.42</span>  <span class="number">2.06</span>  <span class="number">0.94</span>  <span class="number">2.44</span>   <span class="number">415.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">2</span>    <span class="number">13.72</span>  <span class="number">1.43</span>  <span class="number">2.50</span>  <span class="number">16.7</span>  <span class="number">108.0</span>  <span class="number">3.40</span>  ...  <span class="number">2.04</span>  <span class="number">6.80</span>  <span class="number">0.89</span>  <span class="number">2.87</span>  <span class="number">1285.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">13.49</span>  <span class="number">3.59</span>  <span class="number">2.19</span>  <span class="number">19.5</span>   <span class="number">88.0</span>  <span class="number">1.62</span>  ...  <span class="number">0.88</span>  <span class="number">5.70</span>  <span class="number">0.81</span>  <span class="number">1.82</span>   <span class="number">580.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">4</span>    <span class="number">13.05</span>  <span class="number">2.05</span>  <span class="number">3.22</span>  <span class="number">25.0</span>  <span class="number">124.0</span>  <span class="number">2.63</span>  ...  <span class="number">1.92</span>  <span class="number">3.58</span>  <span class="number">1.13</span>  <span class="number">3.20</span>   <span class="number">830.0</span>   <span class="number">0</span></span><br><span class="line">..     ...   ...   ...   ...    ...   ...  ...   ...   ...   ...   ...     ...  ..</span><br><span class="line"><span class="number">119</span>  <span class="number">12.42</span>  <span class="number">4.43</span>  <span class="number">2.73</span>  <span class="number">26.5</span>  <span class="number">102.0</span>  <span class="number">2.20</span>  ...  <span class="number">1.71</span>  <span class="number">2.08</span>  <span class="number">0.92</span>  <span class="number">3.12</span>   <span class="number">365.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">120</span>  <span class="number">14.22</span>  <span class="number">1.70</span>  <span class="number">2.30</span>  <span class="number">16.3</span>  <span class="number">118.0</span>  <span class="number">3.20</span>  ...  <span class="number">2.03</span>  <span class="number">6.38</span>  <span class="number">0.94</span>  <span class="number">3.31</span>   <span class="number">970.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">121</span>  <span class="number">13.16</span>  <span class="number">2.36</span>  <span class="number">2.67</span>  <span class="number">18.6</span>  <span class="number">101.0</span>  <span class="number">2.80</span>  ...  <span class="number">2.81</span>  <span class="number">5.68</span>  <span class="number">1.03</span>  <span class="number">3.17</span>  <span class="number">1185.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">122</span>  <span class="number">11.84</span>  <span class="number">2.89</span>  <span class="number">2.23</span>  <span class="number">18.0</span>  <span class="number">112.0</span>  <span class="number">1.72</span>  ...  <span class="number">0.95</span>  <span class="number">2.65</span>  <span class="number">0.96</span>  <span class="number">2.52</span>   <span class="number">500.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">123</span>  <span class="number">12.86</span>  <span class="number">1.35</span>  <span class="number">2.32</span>  <span class="number">18.0</span>  <span class="number">122.0</span>  <span class="number">1.51</span>  ...  <span class="number">0.94</span>  <span class="number">4.10</span>  <span class="number">0.76</span>  <span class="number">1.29</span>   <span class="number">630.0</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">[<span class="number">124</span> rows x <span class="number">14</span> columns]</span><br></pre></td></tr></table></figure><p>决策树算法一般不需要标准化，所以这里省去了标准化。</p><h2 id="建立并训练决策树">建立并训练决策树</h2><p>sklearn并没有实现C4.5算法，所以这里用的ID3,所以要用C4.5算法还是需要手动实现。sklearn默认使用的是CART算法（基尼系数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dec_tree = tree.DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line"><span class="comment"># 如果criterion不填则默认使用gini系数。</span></span><br><span class="line">dec_tree = dec_tree.fit(X_train,y_train)</span><br></pre></td></tr></table></figure><h2 id="测试决策树">测试决策树</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_predict = dec_tree.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y predict value is:<span class="subst">&#123;y_predict&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y ture value is:<span class="subst">&#123;y_test&#125;</span>&quot;</span>)</span><br><span class="line">f1_score = f1_score(y_test, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;F1-Score is <span class="subst">&#123;f1_score&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y predict value <span class="keyword">is</span>:[<span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">y ture value <span class="keyword">is</span>:[<span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">F1-Score <span class="keyword">is</span> <span class="number">0.9266835016835017</span></span><br></pre></td></tr></table></figure><h2 id="使用graphviz绘制决策树">使用Graphviz绘制决策树</h2><p>下载graphviz：<a href="https://graphviz.org/download/">Download |Graphviz</a></p><p>配置环境变量：</p><p><img src="https://img.issey.top/img/202209212337604.png" /></p><p>pip安装graphviz：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install graphviz -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>然后重启Pycharm，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">feature_name = [<span class="string">&#x27;酒精&#x27;</span>, <span class="string">&#x27;苹果酸&#x27;</span>, <span class="string">&#x27;灰&#x27;</span>, <span class="string">&#x27;灰的碱性&#x27;</span>, <span class="string">&#x27;镁&#x27;</span>, <span class="string">&#x27;总酚&#x27;</span>, <span class="string">&#x27;类黄酮&#x27;</span>, <span class="string">&#x27;非黄烷类酚类&#x27;</span>, <span class="string">&#x27;花青素&#x27;</span>, <span class="string">&#x27;颜色强度&#x27;</span>, <span class="string">&#x27;色调&#x27;</span>, <span class="string">&#x27;稀释葡萄酒&#x27;</span>, <span class="string">&#x27;脯氨酸&#x27;</span>]</span><br><span class="line">dot_data = tree.export_graphviz(dec_tree</span><br><span class="line">                                , feature_names=feature_name</span><br><span class="line">                                , class_names=[<span class="string">&#x27;琴酒&#x27;</span>, <span class="string">&#x27;雪莉&#x27;</span>, <span class="string">&#x27;贝尔摩德&#x27;</span>]</span><br><span class="line">                                , filled=<span class="literal">True</span></span><br><span class="line">                                , rounded=<span class="literal">True</span></span><br><span class="line">                                , fontname=<span class="string">&quot;Microsoft YaHei&quot;</span>)  <span class="comment"># 圆角</span></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.view()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212336007.png" /></p><h2 id="保存和读取决策树">保存和读取决策树</h2><p>决策树的训练过程往往很浪费时间，所以我们可以把训练好的决策树保存起来方便直接调用。这里介绍模型保存方法，不仅仅适用于决策树，而适用于所有模型。</p><h3 id="使用pickle保存模型">使用pickle保存模型</h3><p>存储模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;储存模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;train_model.pkl&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(model,f)</span><br></pre></td></tr></table></figure><p>新开一个py读取模型并查看模型:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;读取模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_model</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        model = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">dec_tree = read_model(<span class="string">&#x27;train_model.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">feature_name = [<span class="string">&#x27;酒精&#x27;</span>, <span class="string">&#x27;苹果酸&#x27;</span>, <span class="string">&#x27;灰&#x27;</span>, <span class="string">&#x27;灰的碱性&#x27;</span>, <span class="string">&#x27;镁&#x27;</span>, <span class="string">&#x27;总酚&#x27;</span>, <span class="string">&#x27;类黄酮&#x27;</span>, <span class="string">&#x27;非黄烷类酚类&#x27;</span>, <span class="string">&#x27;花青素&#x27;</span>, <span class="string">&#x27;颜色强度&#x27;</span>, <span class="string">&#x27;色调&#x27;</span>, <span class="string">&#x27;稀释葡萄酒&#x27;</span>, <span class="string">&#x27;脯氨酸&#x27;</span>]</span><br><span class="line">dot_data = tree.export_graphviz(dec_tree</span><br><span class="line">                                , feature_names=feature_name</span><br><span class="line">                                , class_names=[<span class="string">&#x27;琴酒&#x27;</span>, <span class="string">&#x27;雪莉&#x27;</span>, <span class="string">&#x27;贝尔摩德&#x27;</span>]</span><br><span class="line">                                , filled=<span class="literal">True</span></span><br><span class="line">                                , rounded=<span class="literal">True</span></span><br><span class="line">                                , fontname=<span class="string">&quot;Microsoft YaHei&quot;</span>)  <span class="comment"># 圆角</span></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.view()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212337291.png" /></p><h1 id="决策树的优缺点">决策树的优缺点</h1><p>优点：</p><ul><li><p>可以可视化，易于理解</p></li><li><p>决策树几乎不需要预处理</p></li></ul><p>缺点：</p><ul><li><p>决策树可能会创建一棵过于复杂的树，容易过拟合，需要选择合适的剪枝策略</p></li><li><p>决策树对训练集十分敏感，哪怕训练集稍微有一点变化，也可能会产生一棵完全不同的决策树。（可以在分割训练集时把随机种子去掉，会发现每次的决策树都不一样）</p></li></ul><p>最后还是贴一个C4.5的实现文章连接吧：<ahref="https://blog.csdn.net/weixin_38273255/article/details/88981925?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%9E%E7%8E%B0C4.5&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-88981925.142%5Ev44%5Econtrol&amp;spm=1018.2226.3001.4187">python实现C4.5_张##的博客-CSDN博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记6——【支持向量机1】支持向量机的原理与推导</title>
      <link href="/article/1c4151e0792a/"/>
      <url>/article/1c4151e0792a/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：以下为支持向量机学习笔记，参考教程：</p><p><ahref="https://www.bilibili.com/video/BV1qf4y1x7kB?p=1&amp;vd_source=747540861ba5c41c17852ccf069029f5">(强推)浙江大学-机器学习_哔哩哔哩_bilibili</a></p><p><ahref="https://blog.csdn.net/v_JULY_v/article/details/7624837">支持向量机通俗导论（理解SVM的三层境界）</a></p><p><ahref="https://www.bilibili.com/video/BV1Ca411M7KA?p=1&amp;vd_source=747540861ba5c41c17852ccf069029f5">【太...完整了！】上海交大和腾讯强强联合的机器学习与深度学习课程分享！-人工智能/AI/神经网络_哔哩哔哩_bilibili</a></p></blockquote><h1 id="线性可分与线性不可分">线性可分与线性不可分</h1><p>    简单来说，线性可分就是可以用线性函数将两类样本分开。在二维中，表现为一条直线，在三维中为一个平面，而更高维中为超平面。如果不存在这样的线性函数，则为线性不可分。</p><p><img src="https://img.issey.top/img/202209211133192.png" /></p><p>    事实：如果一个数据集是线性可分的，那一定存在无数多个超平面将各个类别分开。</p><h1id="支持向量机support-vector-machine">支持向量机（Support Vector Machine）</h1><p>    支持向量机（简称SVM）最初是一种解决二分类的有监督学习算法，SVM的目的为：在给定两类样本的数据集的前提下，寻找一个将两类样本分隔开的超平面，并且使得两类样本之间的边界间隔(margin)最大化。最终得到的超平面被称为决策边界(decisionboundary)。</p><p>    示例（二维）：</p><p><img src="https://img.issey.top/img/202209211134546.png" /></p><p>    上面的三个超平面都可以将不同类别的样本分开，但是哪个是最好的呢？</p><p>    如果这里判断超平面“好坏”的标准为哪条直线对样本误差的容忍程度最高，那么直线2显然是最好的。支持向量机就是基于最优化理论，来寻找线2的算法。</p><p>    注意：在支持向量机中，样本输出值都是-1或1。</p><h1 id="最大间隔分离超平面">最大间隔分离超平面</h1><p><img src="https://img.issey.top/img/202209211134871.png" /></p><p>    <strong>上面那个图可能不太标准，红圈圈住的样本应该刚好在虚线上才对！</strong></p><p>    我们假定两类数据中间有一个超平面，将这个超平面向两边平移，直到刚好擦过样本为止（图中两条虚线），我们定义这两个超平面刚好经过的训练样本为这个数据集的<strong>支持向量</strong>(图中红圈所示样本)，把这两个超平面中间的距离叫做<strong>间隔（margin）</strong>。支持向量机要找的是使间隔最大的那个超平面。并且，求得的超平面只能有一个，所以这个超平面应该处于上线两超平面的中间，即到支持向量距离相等。</p><p>    于是，支持向量机寻找的最优分类超平面应该满足：</p><ol type="1"><li><p>该超平面分开了两类</p></li><li><p>该超平面最大化了间隔</p></li><li><p>该超平面处于间隔的中间，到所有支持向量距离相等。</p></li></ol><h1 id="线性可分支持向量机硬间隔">线性可分支持向量机（硬间隔）</h1><h2 id="线性可分的数学定义">线性可分的数学定义</h2><p>    一个训练样本集<span class="math inline">\({(\vec x_i,y_i)},i = 1\sim n\)</span> 线性可分，是指存在<span class="math inline">\((\vecw,b)\)</span> 使得：</p><p>    <span class="math inline">\(当 y_i = +1时，\vec w \cdot \vecx_i+b&gt;0,\)</span></p><p>    <span class="math inline">\(当y_i = -1时，\vec w \cdot \vec x_i+b&lt;0\)</span>.</p><p>    （注：有时会看到<spanclass="math inline">\(W^Tx_i+b\)</span>的形式，意思都一样的。）</p><h2 id="svm目标函数">SVM目标函数</h2><p>    假定训练样本集线性可分，那么支持向量机寻找的最大化间隔超平面为：</p><p>    已知训练样本集<span class="math inline">\({(\vec x_i,y_i)},i =1\sim n,y_i = -1  or 1\)</span>；</p><p>    求解<span class="math inline">\((\vec w,b)\)</span>使得：</p><p>    <span class="math inline">\(最小化(Minimize):\frac 1 2||\vecw||^2\)</span></p><p>    <span class="math inline">\(限制条件：y_i(\vec w\cdot \vec x_i+b) \geq1,(i = 1 \sim n)\)</span></p><p>    其中<span class="math inline">\(||\vec w||^2\)</span>(向量w模的平方)为，<span class="math inline">\(||\vec w||^2 =w_1^2+w_2^2+...+w_m^2 = \sum_{i=1}^m w_i^2\)</span></p><p>    我们可以看出，需要求的目标函数其实是凸优化（ConvexOptimization）中的二次规划问题。关于目标函数求解，用的是<strong>拉格朗日乘子法</strong>以及<strong>拉格朗日对偶问题</strong>求解。拉格朗日乘子法和对偶问题暂时不叙述。这里直接使用凸优化求解包进行求解。</p><h3 id="目标函数推导过程">目标函数推导过程</h3><blockquote><p>事实1：</p><p><span class="math inline">\(\vec w \cdot \vec x + b =0\)</span>与<span class="math inline">\(a(\vec w\cdot \vec x+b) = 0，(a\neq 0)\)</span> 表示同一个超平面。</p><p>事实2：一个点<span class="math inline">\(X_0\)</span>到超平面<spanclass="math inline">\(\vec w\cdot \vec x+b = 0\)</span>的距离为：</p><p><span class="math inline">\(d = \frac{|\vec w\cdot \vecx_0+b|}{||\vec w||}\)</span></p></blockquote><p>    假设我们已知最终要求的超平面为：<span class="math inline">\(\vecw\cdot \vec x+b\)</span>，因为这个超平面在间隔最大的两个平行的超平面正中间，而且上下两个超平面都经过支持向量，<strong>所以支持向量到所求超平面的距离应该都是相同的。</strong></p><p>    于是，我们可以根据事实1，将<span class="math inline">\((\vecw,b)\)</span> 放缩为<span class="math inline">\((a\vec w,ab)\)</span>,最终使得：</p><p>    在支持向量<span class="math inline">\(X_0\)</span>上有<spanclass="math inline">\(|\vec w\cdot \vec x_0+b| = 1\)</span>；</p><p>    那么显而易见在非支持向量上有<span class="math inline">\(|\vecw\cdot \vec x_0+b|&gt;1\)</span>。</p><p>   （有点难懂，我的理解是a对于每一个支持向量都是一个不同的值，使其满足上述条件，也就是说a并不是一个定值。但是无论a怎么变，都表示的同一个超平面，所以对后续没有影响。至于非支持向量上为什么绝对值都大于1，是因为事实2，因为支持向量离超平面距离是最近的，所以分母相同的情况下，非支持向量作为分子自然就更大。）</p><p>    变换后，根据事实2，支持向量<spanclass="math inline">\(X_0\)</span>到超平面的距离将会变为：<spanclass="math inline">\(d = \frac{|\vec w\cdot \vec x_0+b|}{||\vec w||} =\frac {1}{||\vec w||}\)</span></p><p>    我们的目标是使支持向量到超平面的距离最大，也就是<spanclass="math inline">\(maximize(\frac 1 {||\vec w||})\)</span></p><p>    又因<span class="math inline">\(maximize(\frac 1 {||\vec w||}) =minimize(||\vec w||)\)</span></p><p>    于是我们将问题优化的目标函数定为：<spanclass="math inline">\(最小化(Minimize):\frac 1 2||\vecw||^2\)</span></p><p>    而 最小化<span class="math inline">\(\frac 1 2||\vecw||^2\)</span> 与 最小化$||w|| $是完全等价的。之所以写成这种形式，是为了后续求导更加方便。</p><p>    同时，因为我们将<span class="math inline">\((\vecw,b)\)</span>放缩为<span class="math inline">\((a\vec w,ab)\)</span>,我们可以得到限制条件：</p><p>    <span class="math inline">\(y_i(\vec w\cdot \vec x_i +b) \geq1,(i= 1 \sim n)\)</span></p><p>    其中，<span class="math inline">\(y_i = -1  or  1\)</span></p><h2 id="凸优化中的二次规划">凸优化中的二次规划</h2><ol type="1"><li><p>目标函数为二次项</p></li><li><p>限制条件是一次项</p></li></ol><p>    因为我们的目标函数<span class="math inline">\(\frac 1 2||\vecw||^2 = \frac 1 2(w_1^2+w_2^2+...+w_m^2)\)</span>为二次项,限制条件<spanclass="math inline">\(y_i(\vec w\cdot \vec x_i +b) \geq1,(i = 1 \simn)\)</span>为一次项，所以满足二次规划。</p><p>   凸优化的二次规划问题要么无解，要么只有唯一最小值解。于是，我们就可以用梯度下降算法求解啦！另外，只要一个优化问题是凸的，我们总能找到高效快速的算法去解决它。<strong>线性可分条件下的支持向量机是凸优化问题</strong>，因此能迅速找到高效的算法解决。不过我们不会详细探讨求解凸优化问题，关于凸优化求解是一门专门的课程，有兴趣可以学习《凸优化理论》这门课程。</p><h1 id="线性支持向量机软间隔">线性支持向量机（软间隔）</h1><h2 id="硬间隔与软间隔">硬间隔与软间隔</h2><p>硬间隔：间隔内不存在样本。训练集完全分类正确，损失函数不存在，损失值为0。也就是说，找到的超平面完全分离两类。上述都是硬间隔。硬间隔容易受到极端值影响，泛化能力不强，于是我们提出了软间隔。</p><p>软间隔：间隔内允许样本存在。允许一定量的样本分类错误，不过这些错误样本范围不会超过间隔区间。软间隔是硬间隔SVM的拓展版本。</p><h2 id="松弛因子">松弛因子</h2><blockquote><p>注：因为线性支持向量机模拟出的直线允许误差存在，所以根据线性可分的定义，线性支持向量机其实属于线性不可分。</p></blockquote><p>若数据线性不可分，则增加松弛因子<span class="math inline">\(\zeta_i\geq0\)</span> ,使函数间隔加上松弛变量大于等于1。于是，</p><p>约束条件变为:<span class="math inline">\(y_i(\vec w\cdot \vec x_i+b)\geq 1-\zeta_i\)</span></p><p>目标函数变为：<span class="math inline">\(minimize_{w,b}(\frac 12||\vec w||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>其中，C为惩罚因子，是为了防止松弛因子过大加入的一个代价。当C等于无穷大，只有当<spanclass="math inline">\(\zeta_i =0\)</span>时才有最小值，因此，当C为无穷大时，退化为线性可分支持向量机。</p><p>目标函数求解依然是代入拉格朗日乘子，转化为对偶问题并求解。</p><h1 id="合页损失函数hinge-loss-function">合页损失函数（hinge lossfunction）</h1><p>公式：<span class="math inline">\(L(y(\vec w\cdot \vec x+b)) =[1-y(\vec w\cdot \vec x+b)]_+\)</span></p><p>下标“+”表示以下情况取正值：</p><p><span class="math inline">\([z]_+ =\left\{\begin{aligned} z, z&gt;0\\ 0,z\leq0\end{aligned} \right.\)</span></p><p>当函数间隔<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&gt;1\)</span>时，即当分类正确并在（软）间隔之外时，损失为0。否则损失为<spanclass="math inline">\(1-y_i(\vec w\cdot \vec x+b)\)</span></p><p><img src="https://img.issey.top/img/202209211135657.jpg" /></p><p>当样本正确分类，<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&gt;0\)</span>,反之小于0。</p><p><span class="math inline">\(|y_i(\vec w\cdot \vec x+b)|\)</span>表示样本与决策边界的距离。绝对值越大，距离决策边界越远。</p><p>于是：</p><p>当<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&gt;0\)</span>,即分类正确情况下，距离决策边界越远区分程度越好。</p><p>当<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&lt;0\)</span>,即分类错误情况下，距离决策边界越远区分程度越差。</p><h2 id="svm的损失函数">SVM的损失函数</h2><p>SVM有另一种解释，即最小化以下目标函数：</p><p><span class="math inline">\(\sum_{i=1}^N[1-y_i(\vec w\cdot \vecx_i+b)]_++\lambda||\vec w||^2\)</span></p><p>这里不提供相关证明，详情见文章：<ahref="https://blog.csdn.net/lynn_001/article/details/84198007">线性支持向量机-合页损失函数(HingeLoss)_搏击俱乐部_的博客-CSDN博客_支持向量机损失函数</a></p><p>也就是说，SVM目标函数实际上就是合页损失函数加上<spanclass="math inline">\(\lambda||\vec w||^2\)</span>。</p><h1 id="非线性支持向量机">非线性支持向量机</h1><h2 id="将特征空间从低位映射到高维">将特征空间从低位映射到高维</h2><p>当遇到如下图所示的非线性数据时，支持向量机的处理是将该训练集的特征从低维映射到高维，在高维仍然采用线性超平面对数据进行分类。</p><p><img src="https://img.issey.top/img/202209211135086.png" /></p><p>现有以下假设：</p><p>假设1：在一个M维空间上随机取N个训练样本，随机的对每个训练样本赋予标签+1或-1，设这些训练样本线性可分的概率为<spanclass="math inline">\(P(M)\)</span>。那么当M趋于无穷大时，<spanclass="math inline">\(P(M)=1\)</span>。</p><p>这里略去该假设的证明。</p><p>也就是说，一个训练集在低维上不可分，但它到高维的映射将会是可分的。于是，支持向量机将训练样本由低维映射到高维以增加线性可分的概率。</p><p>我们设<spanclass="math inline">\(\phi(x)\)</span>为x在高维上的映射，那么假定<spanclass="math inline">\(\phi(x)\)</span>形式已知的条件下，引入松弛变量的目标函数将会变为：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vecw||^2+C\sum_{i=1}^N\zeta_i),    \zeta_i\geq0,(i=1\sim n)\)</span></p><p>限制条件：</p><p><span class="math inline">\(y_i[\vec w\cdot\phi(\vec x_i)+b]\geq1-\zeta_i,(i=1 \sim n)\)</span></p><p>注意：这里的<span class="math inline">\(\vec w\)</span>是与高维的<span class="math inline">\(\phi(\vec x)\)</span> 对应的。</p><p>我们可以看到，转化为高维后同样可以采用凸优化的二次规划求解。</p><h2 id="核函数kernel-function">核函数（Kernel Function）</h2><p>注意：接下来向量将会用<span class="math inline">\(X\)</span>表示，向量点乘则变为矩阵乘法，例如：</p><p><span class="math inline">\(\vec x_1\cdot\vec x_2\)</span>将变为<spanclass="math inline">\(X_1^TX_2\)</span> 。</p><p>根据低维映射到高维的规则，重点在于如何找到<spanclass="math inline">\(\phi(X)\)</span>使得线性不可分训练集在高维线性可分。实际上，我们可以不用知道 <spanclass="math inline">\(\phi(X)\)</span>的具体形式。取而代之，如果对于任意两个向量<spanclass="math inline">\(X_1,X_2\)</span>,有<spanclass="math inline">\(K(X_1,X_2) = \phi(X_1)^T\phi(X_2)\)</span>,那么我们仍然可以通过一些技巧完成测试样本的预测。</p><p>我们定义<span class="math inline">\(K(X_1,X_2)\)</span>为核函数。易得，核函数是一个实数。</p><p>可以证明，<span class="math inline">\(K(X_1,X_2)\)</span>与<spanclass="math inline">\(\phi(X_1),\phi(X_2)\)</span>是一一对应的关系，证明略。另外，核函数必须满足以下条件才能写成两个向量内积的形式：</p><p><span class="math inline">\(K(X_1,X_2)\)</span>能写成<spanclass="math inline">\(\phi(X_1)^T\phi(X_2)\)</span> 的充要条件：</p><ol type="1"><li><p><span class="math inline">\(K(X_1,X_2) =K(X_2,X_1)\)</span>,即交换性</p></li><li><p>$C_i(i=1N),N有<em>{i=1}^N</em>{j=1}^NC_iC_jK(X_iX_j) $,即半正定性</p></li></ol><p>接下来，我们将研究如何在已知<spanclass="math inline">\(K(X_1,X_2)\)</span>而不知道<spanclass="math inline">\(\phi(X)\)</span>的条件下求解支持向量机的目标函数。</p><h2 id="对偶问题与kkt条件">对偶问题与KKT条件</h2><h3 id="原问题与对偶问题">原问题与对偶问题</h3><p>原问题（Prime problem）定义：</p><p>    最小化（Minimize）：<span class="math inline">\(f(w)\)</span></p><p>    限制条件（Subject to）： <span class="math display">\[\begin{split}&amp;g_i(w) \leq0 ,i=1\sim K\\&amp;h_i(w) = 0,i=1\simm\end{split}\]</span> 注：自变量为<spanclass="math inline">\(w\)</span>,目标函数是<spanclass="math inline">\(f(w)\)</span>，限制条件：有K个不等式，分别用<spanclass="math inline">\(g_i(w)\)</span> 来表示，等式有m个，分别用<spanclass="math inline">\(h_i(m)\)</span> 表示 。</p><p>为了定义对偶问题，我们先定义一个函数：</p><p><span class="math inline">\(L(w,a,\beta) =f(w)+a^Tg(w)+\beta^Th(w)\)</span></p><p>其中，</p><p><span class="math inline">\(a = [a_1,a_2,...,a_K]^T\)</span>,</p><p><span class="math inline">\(\beta =[\beta_1,\beta_2,...\beta_M]^T\)</span>,</p><p><span class="math inline">\(g(w) =[g_1(w),g_2(w),...,g_K(w)]^T\)</span></p><p><span class="math inline">\(h(w) =[h_1(w),h_2(w),...,h_M(w)]^T\)</span></p><p>然后，定义对偶问题如下：</p><p><span class="math inline">\(最大化：\theta(a,\beta) = inf L(w,a,\beta),所有定义域内的w\)</span>    </p><p>限制条件：<span class="math inline">\(a_i \geq0,i=1\simK\)</span></p><p>对偶问题是：最大化<spanclass="math inline">\(\theta(a,\beta)\)</span>, 它等于<spanclass="math inline">\(L(w,a,\beta)\)</span> 去遍历所有定义域上的<spanclass="math inline">\(w\)</span>找到使<spanclass="math inline">\(L(w,a,\beta)\)</span>最小的那个<spanclass="math inline">\(w\)</span>，同时将求得的<spanclass="math inline">\(L(w,a,\beta)\)</span> 赋值为<spanclass="math inline">\(\theta(a,\beta)\)</span>。注意限制条件。</p><p>联合原问题与对偶问题，有以下定理：</p><p>定理1：若<span class="math inline">\(w^*\)</span>是原问题的解，<spanclass="math inline">\((a^*,\beta^*)\)</span>是对偶问题的解，那么：<spanclass="math inline">\(f(w^*)\geq \theta(a^*,\beta^*)\)</span></p><p>证明略。</p><p>这个定理说明：原问题的解<spanclass="math inline">\(f(w^*)\)</span>总是大于等于对偶问题的解<spanclass="math inline">\(\theta(a^*,\beta^*)\)</span> 。</p><p>我们将<span class="math inline">\(f(w^*)-\theta(a^*,\beta^*)\)</span> 定义为对偶差距(DoalityGap)。根据定理1，对偶差距大于等于0。</p><h3 id="强对偶定理">强对偶定理</h3><p>如果<span class="math inline">\(g(w) =Aw+b,h(w)=Cw+d,f(w)\)</span>为凸函数，则有<spanclass="math inline">\(f(w^*) =\theta(a^*,\beta^*)\)</span>,对偶差距为0。</p><p>简单来说就是，<strong>如果原问题的目标函数是凸函数，而限制条件是线性函数</strong>那么<spanclass="math inline">\(f(w^*) = \theta(a^*,\beta^*)\)</span>。证明略。</p><h3 id="kkt条件">KKT条件</h3><p>如果强对偶定理成立，即<span class="math inline">\(f(w^*) =\theta(a^*,\beta^*)\)</span> ,则定理1中必然能推出：对于所有的<spanclass="math inline">\(i=1\sim K\)</span> ,要么<spanclass="math inline">\(a_i = 0\)</span>，要么<spanclass="math inline">\(g_i(w^*) = 0\)</span>。这个条件被称为KKT条件。</p><h2id="将支持向量机目标函数转化为对偶问题并求解">将支持向量机目标函数转化为对偶问题并求解</h2><h3 id="目标函数转化为原问题形式">目标函数转化为原问题形式</h3><p>回顾一下现在的支持向量机目标函数：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vecw||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>限制条件：</p><p><span class="math inline">\(\zeta_i\geq0,(i=1\sim n)\)</span></p><p><span class="math inline">\(y_i[\vec w\cdot\phi(\vec x_i)+b]\geq1-\zeta_i,(i=1 \sim n)\)</span></p><p>对比原问题(Prime problem)的形式：</p><p>最小化（Minimize）：<span class="math inline">\(f(w)\)</span></p><p>限制条件（Subject to）: <span class="math display">\[\begin{split}&amp;g_i(w) \leq0 ,i=1\sim K\\&amp;h_i(w) = 0,i=1\simm\end{split}\]</span> 注意到，原问题中不等式<span class="math inline">\(g_i(w)\leq0\)</span>,而支持向量机的限制条件中两个不等式都是大于等于0的。所以我们要先将支持向量机中的限制条件转为小于等于。</p><ul><li><p>将<span class="math inline">\(\zeta_i\)</span>转为相反数</p></li><li><p>展开并化简第二个不等式</p></li></ul><p>于是，目标函数将变为：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vecw||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>限制条件：</p><p><span class="math inline">\(\zeta_i\leq0,(i=1\sim n)\)</span></p><p><spanclass="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib\leq0(i=1\simN)\)</span></p><p>因为目标函数是凸的，而其限制条件都是线性函数，所以满足强对偶定理。</p><h3 id="利用对偶定理求解">利用对偶定理求解</h3><p>现在，对偶问题中的<span class="math inline">\(w\)</span>就是这里的<spanclass="math inline">\((w,b,\zeta_i)\)</span>,而不等式<spanclass="math inline">\(g_i(w)\leq0\)</span>是这里限制条件（两部分）：</p><p><span class="math inline">\(\zeta_i\leq0,(i=1\sim n)\)</span></p><p><spanclass="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib\leq0(i=1\simN)\)</span></p><p>另外，因为限制条件不存在等式，所以不存在对偶问题中的<spanclass="math inline">\(h_i(w)\)</span>。</p><p>然后，对偶问题可以写成如下形式：</p><p><span class="math inline">\(最大化：\theta(a,\beta) =inf_{w,\zeta,b}\{\frac 12||w||^2-C\sum_{i=1}^N\beta_i\zeta_i+\sum_{i=1}^Na_i[1+\zeta_i-y_iw^T\phi(X_i)-y_ib]\}\)</span></p><p>限制条件：</p><ol type="1"><li><span class="math inline">\(a_i\geq0\)</span></li></ol><p>(2)<span class="math inline">\(\beta_i\geq0\)</span></p><h4id="如何将原目标函数转化为对偶问题">如何将原目标函数转化为对偶问题</h4><p>先对<spanclass="math inline">\((w,b,\zeta_i)\)</span>求导并令导数为0：</p><ol type="1"><li><p><span class="math inline">\(\frac {\partial \theta}{\partial w} =0\)</span>推出<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span></p></li><li><p><span class="math inline">\(\frac {\partial \theta}{\partial\zeta_i} = 0\)</span>推出<spanclass="math inline">\(a_i+\beta_i=C\)</span></p></li><li><p><span class="math inline">\(\frac {\partial \theta}{\partial b} =0\)</span>推出<spanclass="math inline">\(\sum_{i=1}^Na_iy_i=0\)</span></p></li></ol><p>(详细过程略)</p><p>于是，可以将支持向量机原目标函数化为以下对偶问题：</p><p><span class="math inline">\(最大化：\theta(a,\beta) =\sum_{i=1}^Na_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_ja_ia_j\phi(X_i)^T\)</span></p><p>限制条件：</p><p>(1)  <span class="math inline">\(0 \leq a_i\leq C,(i=1\simN)\)</span></p><p>(2)<span class="math inline">\(\sum_{i=1}^Na_iy_i=0,(i=1\simN)\)</span></p><p>可以看出，这个对偶问题也是一个凸优化的二次规划问题，可以通过最优化算法快速求解。（利用凸优化包）</p><h3 id="如何求解这个对偶问题">如何求解这个对偶问题</h3><p>由于<span class="math inline">\(K(X_i,X_j) =\phi(X_i)^T\phi(X_j)\)</span>,所以我们只需要知道核函数，就可以求解这个对偶问题了。当我们求解了这个对偶问题，解出了所有的<spanclass="math inline">\(a_i\)</span> 。我们可以继续观察<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span>,因为<spanclass="math inline">\(\phi(X_i)\)</span> 不具有显式表达，所以<spanclass="math inline">\(w\)</span>也不具有显式表达。但是我们可以推导：即使<spanclass="math inline">\(w\)</span>不具有显示表达，我们也可以通过核函数算出<spanclass="math inline">\(w^T\phi(X)+b\)</span>的值。</p><h4 id="首先如何求b">首先，如何求b</h4><p>由于<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span> ,则</p><p><span class="math inline">\(w^T\phi(X_i) =\sum_{j=1}^Na_jy_j\phi(X_j)^T\phi(X_i) =\sum_{j=1}^Na_jy_jK(X_j,X_i)\)</span></p><p>其次，根据KKT条件，我们可以推出：</p><ol type="1"><li><p><span class="math inline">\(a_i[1+\zeta_i-y_iw^T\phi(X_i)-y_ib] =0\)</span></p></li><li><p><span class="math inline">\(\beta_i\zeta_i=0\)</span>即<spanclass="math inline">\((c-a_i)\zeta_i = 0\)</span></p></li></ol><p>另外，如果对某个i，<span class="math inline">\(a_i \not= 0且a_i \not=c\)</span>，则根据上面KKT推出的两个公式必有<spanclass="math inline">\(\zeta_i = 0\)</span>,且<spanclass="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib = 0\)</span> 。</p><p>而这时<span class="math inline">\(y_iw^T\phi(X_i) =\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)\)</span></p><p>所以，只需要找一个<span class="math inline">\(0&lt;a_i&lt;c\)</span>，</p><p><span class="math inline">\(b =\frac{1-\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)}{y_i}\)</span></p><h4 id="如何求wtphixb-核函数戏法kernel-trick">如何求<spanclass="math inline">\(w^T\phi(X)+b\)</span>    —— 核函数戏法（KernelTrick）</h4><p>将<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span>代入得：</p><p><span class="math display">\[\begin{split}w^T\phi(X)+b &amp;=w\\&amp;=\sum_{i=1}^Na_iy_i\phi(X_i)^T\phi(X)+b\\&amp; =\sum_{i=1}^Na_iy_iK(X_i,X)+b\end{split}\]</span></p><p>我们发现，即使不知道<spanclass="math inline">\(\phi(X)和w\)</span>的显式形式，也可以通过核函数求得<spanclass="math inline">\(w^T\phi(X)+b\)</span>，这一结论被称为核函数戏法。</p><p>最后，我们可以用如下的判别标准来判定一个样本属于哪一类别：</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \geq0\)</span>，那么<span class="math inline">\(X \in C_1\)</span> ;</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \leq0\)</span>，那么<span class="math inline">\(X \in C_2\)</span> 。</p><h2id="总结支持向量机训练和测试流程">总结：支持向量机训练和测试流程</h2><p><strong>训练过程：</strong></p><p>输入训练集${(X_i,y_i)},i=1N <spanclass="math inline">\(,其中，\)</span>y_i = +1或-1$ 。</p><p>接下来，求解如下目标函数：</p><p><span class="math inline">\(最大化：\theta(a,\beta) =\sum_{i=1}^Na_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_ja_ia_j\phi(X_i)^T\)</span></p><p>限制条件：</p><p>(1)<span class="math inline">\(0 \leq a_i\leq C,(i=1\simN)\)</span></p><p>(2)<span class="math inline">\(\sum_{i=1}^Na_iy_i=0,(i=1\simN)\)</span></p><p>求出<span class="math inline">\(a\)</span> 。</p><p>然后，求出b：</p><p>    找一个<span class="math inline">\(a_i \not= 0且a_i\not=c\)</span>,</p><p>    <span class="math inline">\(b =\frac{1-\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)}{y_i}\)</span></p><p>一旦求出了<spanclass="math inline">\(a，b\)</span>,就完成了支持向量机的训练过程。</p><p><strong>测试过程：</strong></p><p>给出一个测试数据X，预测它的类别y。</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \geq0\)</span>，那么<span class="math inline">\(y = +1\)</span> ;</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b &lt;0\)</span>，那么<span class="math inline">\(y = -1\)</span> ;</p><p>关于支持向量机的具体应用以及更多细节比如核函数的选择、超参数的控制等等将会在下一章支持向量机中进行说明。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习笔记1——神经网络的搭建与简单应用</title>
      <link href="/article/f84b08dc3a2c/"/>
      <url>/article/f84b08dc3a2c/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ul><li><ahref="https://www.bilibili.com/video/BV1nt4y1h7jc?vd_source=747540861ba5c41c17852ccf069029f5">[双语人译|带测试]吴恩达2022机器学习专项课程(二）：高级学习算法Advanced Learning Algorithms</a></li></ul><h1 id="前言">前言</h1><p>在写这篇文章时，对于神经网络的理解还不够，并没有说明神经网络到底是什么，并且推导神经网络后向传播的过程。这篇文章个人觉得是失败的，但同时也有一定价值。</p><p>此文章说明了神经网络的环境搭建tensorflow以及配置其能够GPU加速的过程，同时利用tensorflow做了一个简单的案例，为后续神经网络的学习解决了杂碎的问题。</p><p>本来我不打算发出来，但是考虑到不管是pytorch还是tensorflow，搭建能够调用gpu环境的过程都是很容易出错的，于是还是决定发出来。因为当时我自己搭建这个环境因为各种版本问题还有其他奇怪的问题，加上现有的大部分教程由于版本迭代问题已经不适合，浪费了起码半天的时间。</p><h1 id="神经网络与深度学习">神经网络与深度学习</h1><p>    <ahref="https://baike.baidu.com/item/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/16600562?fr=aladdin">神经网络</a>诞生于一个尝试创建能够模拟大脑的软件的动机。对于神经网络的研究始于20世纪50年代。中途被冷落了两次。在2005年，神经网络东山再起，其中的一些算法被深度学习重新命名，实际上深度学习和神经网路阐释着非常相似的事情。网络上有不少区别深度学习和神经网络的文章，但就当下的环境来说，深度学习其实就是神经网络，深度学习不过是业界为神经网络取的一种营销名字。</p><h1 id="使用tensorflow搭建神经网络">使用Tensorflow搭建神经网络</h1><p>    常用的人工神经网络搭建框架有Tensorflow和Pytorch等，这里使用tensorflow进行搭建。</p><h2 id="环境搭建和导包遇到的问题">环境搭建和导包遇到的问题：</h2><h3 id="问题1">问题1：</h3><p>    最开始是用Acaconda把tensorflow和Keras都安装了，然后导入时报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AssertionError: Duplicate registrations <span class="keyword">for</span> <span class="built_in">type</span> <span class="string">&#x27;optimizer&#x27;</span></span><br></pre></td></tr></table></figure><p>    然后尝试卸载Keras，问题解决。而且卸载后condalist还找到了一个Keras，说明刚才可能多装了一个。</p><h3 id="问题2">问题2：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> xxx</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;tensorflow.keras&#x27;</span></span><br></pre></td></tr></table></figure><p>    最开始以为是Keras和tensorflow版本不搭配问题（当时还没分清Keras和tensorflow.keras是两个东西），检查了版本后无误，最终在这篇文章中找到了答案：<ahref="https://blog.csdn.net/Eric_Blog_CSDN/article/details/88420234">keras学习-No module named ' tensorflow.keras ' 报错，看清tf.keras与keras_Eric_Blog_csdn的博客-CSDN博客</a></p><p>    原因是安装路径中间多了个python，所以导入应该这样写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> xxx</span><br></pre></td></tr></table></figure><h3id="tensorflowgpu相关文件打开不了">tensorflow：GPU相关文件打开不了</h3><p>    各种报错：</p><p><img src="https://img.issey.top/img/202209211150591.png" /></p><p>    大概是说找不到调用GPU的这些文件，如果不解决它，就无法做到向量化，不能向量化那训练速度就会极慢，所以不能放着不管。    首先你得安装适合版本的Cudatoolkit，然后安装cuDNN，再安装tensorflow-gpu。参考：<ahref="https://blog.csdn.net/weixin_56197703/article/details/125192385">安装tensorflow的GPU版本（详细图文教程)</a>.</p><p>    难崩。搞了半天，终于把这个问题解决了。这个环境真的搞得我绝望，各个版本试了又试，强烈建议按照上面贴的这个教程来！！！</p><p>     最后选择的版本搭配：CUDA 11.6.0+cuDNN 8.4.0+tensorflow-gpu2.9.1。如果你用的pycharm，一定一定要把环境换成conda！！！这是我血与泪的教训。</p><p>    终于：</p><p><img src="https://img.issey.top/img/202209211150099.png" /></p><h2 id="搭建一个简单的神经网络">搭建一个简单的神经网络</h2><h3 id="问题引入">问题引入：</h3><p>    现在有一批经过烘烤的咖啡豆，以及它们的烘烤温度和烘烤时间，现在我们知道哪些咖啡豆是好的（GoodRoast）和坏的（BadRoast），请根据已有数据构建一个神经网络，推测在什么温度范围内和烘烤时间范围内咖啡豆是好的。</p><p>    数据绘图：</p><p><img src="https://img.issey.top/img/202209211150809.png" /></p><p>    注：中间的线不用管它。</p><h3 id="导入需要的包">导入需要的包：</h3><p>    这里只给出神经网络相关包，并不是接下来用到的所有包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure><p>Keras是一个高级的Python神经网络框架，已经被添加到TensorFlow中，成为其默认的框架，为TensorFlow 提供更高级的API。</p><p>这里除了这种导包方式还有另一种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>    但是注意你不能同时导入keras和tensorflow内置的keras，不然因为混用报错。我用的tensorflow2.9.0，用第二种方法还是会报错，所以改为了第一种，暂时不清除是不是版本问题。</p><h3 id="数据规范化">数据规范化</h3><p>    与之前学过的相同，对数据进行规范化可以加快反向传播的速度（暂时不用管反向传播是什么）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X,Y = load_coffee_data()</span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">norm_l = tf.keras.layers.Normalization(axis=-<span class="number">1</span>)</span><br><span class="line">norm_l.adapt(X)  <span class="comment"># learns mean, variance</span></span><br><span class="line">Xn = norm_l(X)</span><br></pre></td></tr></table></figure><p>X:（200,2）矩阵，200个样例，2个特征（温度和烘烤时间）；</p><p>Y：长200的列表，值为0或1，1代表Good Roast</p><p>    复制数据，增加训练集的数量并减少训练代数？（不太懂，可能就是要增加训练集个数吧）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Xt = np.tile(Xn,(<span class="number">1000</span>,<span class="number">1</span>))</span><br><span class="line">Yt= np.tile(Y,(<span class="number">1000</span>,<span class="number">1</span>))  </span><br></pre></td></tr></table></figure><h3 id="构建神经网络模型">构建神经网络模型</h3><p>    我们打算构建的神经网络模型长这样：</p><p><img src="https://img.issey.top/img/202209211151193.png" /></p><p>    这个模型的激活函数都是sigmoid函数。可以看出，它一共有两层，第一层（layer1）一共有三个神经元，第二层（layer2）有一个神经元。像这样的layer又叫做密集层（Dense）。</p><p>    现在考虑一个样例：输入值 <span class="math inline">\(\vec x =(x_1,x_2)\)</span>,因为layer1有三个神经元，每个神经元能算出一个值，所以<spanclass="math inline">\(\vec x\)</span>进入layer 1可以算出一个向量<spanclass="math inline">\(\vec a^{[1]} =(a_1^{[1]},a_2^{[1]},a_3^{[1]})\)</span> ,上[ ]表示所在层数。然后<spanclass="math inline">\(\vec a^{[1]}\)</span>进入layer 2,同理，可算出<spanclass="math inline">\(\vec a^{[2]} = (a_1^{[2]})\)</span>。当<spanclass="math inline">\(a_1^{[2]}&gt;=0.5\)</span>时，我们认为它是好的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">1234</span>)  <span class="comment"># applied to achieve consistent results</span></span><br><span class="line">layer_1 = Dense(<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer1&#x27;</span>)</span><br><span class="line">layer_2 = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer2&#x27;</span>)</span><br><span class="line">model = Sequential([layer_1,layer_2])  </span><br></pre></td></tr></table></figure><p>    设置随机数可以使每次训结果一样，便于对照。构建模型也可以写成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential(</span><br><span class="line">    [</span><br><span class="line">        tf.keras.Input(shape=(<span class="number">2</span>,)),</span><br><span class="line">        Dense(<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer1&#x27;</span>),</span><br><span class="line">        Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer2&#x27;</span>)</span><br><span class="line">     ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>    tf.keras.Input(shape=(2,）这一句可以去掉。加上这一句可以为让模型塑形。下面是查看模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209211151155.png" /></p><p>    检查模型各层的参数是否正确：</p><p>    权重W应为（输入的特征数，层中单元数），偏差b应为层数单元数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W1, b1 = model.get_layer(<span class="string">&quot;layer1&quot;</span>).get_weights()</span><br><span class="line">W2, b2 = model.get_layer(<span class="string">&quot;layer2&quot;</span>).get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;W1<span class="subst">&#123;W1.shape&#125;</span>:\n&quot;</span>, W1, <span class="string">f&quot;\nb1<span class="subst">&#123;b1.shape&#125;</span>:&quot;</span>, b1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;W2<span class="subst">&#123;W2.shape&#125;</span>:\n&quot;</span>, W2, <span class="string">f&quot;\nb2<span class="subst">&#123;b2.shape&#125;</span>:&quot;</span>, b2)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209211151293.png" /></p><p>    对应的值是模拟的初始值，随机数种子不同时会发现这些值会变化。我们这里注意它们的shape就行。</p><h3 id="训练模型">训练模型</h3><p>    下面的语句将在之后的文章详细介绍：</p><p>model.compile：定义损失函数和指定编译优化。</p><p>model.fit：运行梯度下降并训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss = tf.keras.losses.BinaryCrossentropy(),</span><br><span class="line">    optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(</span><br><span class="line">    Xt,Yt,            </span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>    在训练结束后，各权值就会更新成训练好的权值了：</p><p><img src="https://img.issey.top/img/202209211152191.png" /></p><p>    关于model.fit里的epochs参数：指训练集应该在训练期间被应用多少次，这里指定的10次。为了提高训练效率，训练集被分成了n个批次，一个批次大小为32，我们有200000个数据，所以被分成了6250批次。</p><p>    </p><p>    补充：训练一般比较慢，为了不每次改代码都重新训练一次，我们可以将以前的训练的结果手动赋给各层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.array([</span><br><span class="line">    [-<span class="number">0.13</span>,  <span class="number">14.3</span>, -<span class="number">11.1</span>],</span><br><span class="line">    [-<span class="number">8.92</span>, <span class="number">11.85</span>, -<span class="number">0.25</span>]] )</span><br><span class="line">b1 = np.array([-<span class="number">11.16</span>, -<span class="number">1.76</span>,  -<span class="number">12.1</span>])</span><br><span class="line">W2 = np.array([</span><br><span class="line">    [-<span class="number">45.71</span>],</span><br><span class="line">    [-<span class="number">42.95</span>],</span><br><span class="line">    [-<span class="number">50.19</span>]])</span><br><span class="line">b2 = np.array([<span class="number">26.14</span>])</span><br><span class="line">model.get_layer(<span class="string">&quot;layer1&quot;</span>).set_weights([W1,b1])</span><br><span class="line">model.get_layer(<span class="string">&quot;layer2&quot;</span>).set_weights([W2,b2])</span><br></pre></td></tr></table></figure><h3 id="模型预测">模型预测</h3><p>    预测，并将概率转化为决策：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">X_test = np.array([</span><br><span class="line">    [<span class="number">200</span>,<span class="number">13.9</span>],  <span class="comment"># postive example</span></span><br><span class="line">    [<span class="number">200</span>,<span class="number">17</span>]])   <span class="comment"># negative example</span></span><br><span class="line">X_testn = norm_l(X_test)</span><br><span class="line">predictions = model.predict(X_testn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predictions = \n&quot;</span>, predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yhat = np.zeros_like(predictions)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predictions)):</span><br><span class="line">    <span class="keyword">if</span> predictions[i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">        yhat[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        yhat[i] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;decisions = \n<span class="subst">&#123;yhat&#125;</span>&quot;</span>)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更简洁的代码</span></span><br><span class="line">yhat = (predictions &gt;= <span class="number">0.5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;decisions = \n<span class="subst">&#123;yhat&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209211152954.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记5——过拟合与正则化</title>
      <link href="/article/63ddafc1034a/"/>
      <url>/article/63ddafc1034a/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合与正则化">过拟合与正则化</h1><blockquote><p>前言：这篇文章皆以回归模型为例。</p></blockquote><p>在开始之前，你可能先要了解以下几个概念：</p><h2 id="前置知识">前置知识</h2><p><strong>偏差与方差</strong></p><blockquote><p>在机器学习中，偏差描述的是根据样本拟合出的模型输出结果与真实结果的差距，损失函数就是依据模型偏差的大小进行反向传播的。降低偏差，就需要复杂化模型，增加模型参数，但容易造成过拟合。方差描述的是样本上训练出的模型在测试集上的表现，降低方差，继续要简化模型，减少模型的参数，但容易造成欠拟合。根本原因是，我们总是希望用有限的训练样本去估计无限的真实数据。假定我们可以获得所有可能的数据集合，并在这个数据集上将损失函数最小化，则这样的模型称之为“真实模型”。但实际应用中，并不能获得且训练所有可能的数据，所以真实模型一定存在，但无法获得。<ahref="https://baike.baidu.com/item/%E6%AC%A0%E6%8B%9F%E5%90%88/22692155?fr=aladdin">欠拟合_百度百科(baidu.com)</a></p></blockquote><p><strong>泛化能力</strong></p><p>通俗来讲，就是训练出的模型对新鲜样本的适应能力。</p><h2 id="什么是欠拟合与过拟合">什么是欠拟合与过拟合</h2><p>示例：正常拟合情况</p><p><img src="https://img.issey.top/img/202209211121846.png" /></p><p>欠拟合：表现为高偏差。欠拟合模型在训练集、验证集和测试集表现均不佳。就像一个不好好学习的学生，在模拟考试和新的考试都考不好。</p><p><img src="https://img.issey.top/img/202209211121782.png" /></p><p>过拟合：表现为高方差。过拟合模型在训练集上表现很好，但遇到陌生数据时就表现得很差，即模型的泛化能力很差。就像一个学生在做模拟卷时太过于努力了，但是他学会的太贴合模拟卷的题型，但是遇到新的考试就做的很差。</p><p><img src="https://img.issey.top/img/202209211122845.png" /></p><p>下图是逻辑回归的过拟合示例：</p><p><img src="https://img.issey.top/img/202209211122089.png" /></p><p>注：红线为正常拟合情况</p><h2 id="导致欠拟合与过拟合的原因">导致欠拟合与过拟合的原因</h2><p><strong>欠拟合（underfitting</strong>）：</p><ul><li><p>特征量太少</p></li><li><p>模型复杂度过低</p></li></ul><p><strong>过拟合（overfitting）：</strong>这里只讨论回归模型的过拟合原因</p><ul><li><p>样本数量太少</p></li><li><p>模型复杂度过高</p></li><li><p>特征量太多</p></li><li><p>样本的噪音数据干扰过大</p></li></ul><h2 id="解决方法">解决方法</h2><p><strong>欠拟合</strong>：</p><p>相对较好解决。</p><ol type="1"><li><p>选择更复杂的模型</p></li><li><p>增加更多特征</p></li><li><p>调整学习率、训练次数等参数</p></li></ol><p>过拟合：</p><ol type="1"><li><p>增加训练数据量，数据量越多，过拟合可能越小</p></li><li><p>减小模型复杂度。</p></li><li><p>减少特征数目。去除某些特征，可以提高模型泛化能力。</p></li><li><p>正则化（推荐）</p></li></ol><h2 id="正则化">正则化</h2><p>这里只对线性回归和逻辑回归的正则化进行说明，其他模型和范数暂不拓展。</p><p>现在有下（右图）的过拟合模型</p><p><img src="https://img.issey.top/img/202209211122413.png" /></p><p>一种解决过拟合的方式是，我们可以手动筛选特征，直接将<spanclass="math inline">\(x^3,x^4\)</span>去除。于是我们可以得到左边的模型。但还有一种不那么暴力的方法，我们如果可以让<spanclass="math inline">\(w_3,w_4\)</span>尽量小，比如0.0000001，那么就相当于消除了<spanclass="math inline">\(x^3,x^4\)</span>。</p><p>这是原来的代价函数：</p><p><span class="math inline">\(J(\vec w,b) =\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2\)</span></p><p>现在，我们让<span class="math inline">\(J(\vecw,b)+1000w_3^2+1000w_4^2\)</span>，如果要让现在的代价函数尽量小，<spanclass="math inline">\(w_3,w_4\)</span>必须足够小。否则代价函数会非常非常大。于是我们就可以求得很小很小的<spanclass="math inline">\(w_3,w_4\)</span>，有效的消除了<spanclass="math inline">\(x^3,x^4\)</span>这两项特征的影响。</p><p>上面这个例子就是正则化的思想，我们正则化了<spanclass="math inline">\(x^3,x^4\)</span>两个特征。</p><p>然而，实际中我们可能有许多种特征，比如100个特征，我们可能分不清哪些特征是重要的，哪些应该被正则化。</p><p>所以正则化一般的实现方式是正则化所有特征。不过一般不会正则化常数b。</p><p>正则化后的线性回归代价函数：</p><p><span class="math inline">\(J(\vec w,b) =\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j =0}^{n-1}w_j^2\)</span></p><p>当<spanclass="math inline">\(\lambda\)</span>过小时，正则化效果较差。比如等于0时没有正则化。当<spanclass="math inline">\(\lambda\)</span>过大时，可能会发生欠拟合。比如<spanclass="math inline">\(\lambda =10^{10}\)</span>，拟合出来的可能是一条几乎平行于x轴的直线<spanclass="math inline">\(f(x) = b\)</span>.</p><h2 id="正则化后的线性回归和逻辑回归">正则化后的线性回归和逻辑回归</h2><h3 id="代价函数">代价函数</h3><p><strong>线性回归</strong></p><p>公式：<span class="math inline">\(J(\vec w,b) =\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j =0}^{n-1}w_j^2\)</span></p><p>其中，<span class="math inline">\(f_{\vec w,b}(x^{(i)}) = \vecw\cdot\vec x^{(i)}+b\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性回归正则化代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_linear_reg</span>(<span class="params">X, y, w, b, lambda_ = <span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        total_cost (scalar):  损失值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = np.dot(X[i],w) + b</span><br><span class="line">        cost = cost + (f_wb_i - y[i])**<span class="number">2</span></span><br><span class="line">    cost = cost / (<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">    reg_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        reg_cost += w[j]**<span class="number">2</span></span><br><span class="line">    reg_cost = (lambda_/(<span class="number">2</span>*m)) * reg_cost</span><br><span class="line"></span><br><span class="line">    total_cost = cost + reg_cost</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><p><strong>逻辑回归</strong></p><p>公式：</p><p><span class="math inline">\(J(\vec w,b) =\frac{1}{m}\sum_{i=0}^{m-1}[-y^{(i)}log(f_{\vec w,b}(\vecx^{(i)})-(1-y^{(i)})log(1-f_{\vec w,b}(\vecx^{(i)})]+\frac{\lambda}{2m}\sum_{j = 0}^{n-1}w_j^2\)</span></p><p>其中，</p><p><span class="math inline">\(f_{\vec w,b}(\vec x^{(i)}) = sigmoid(\vecw \cdot \vec x+b)\)</span></p><p><span class="math inline">\(sigmoid(z) =\frac{1}{1+e^{-z}}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param z: 标量</span></span><br><span class="line"><span class="string">    :return: 标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> g   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑函数正则化代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic_reg</span>(<span class="params">X, y, w, b, lambda_ = <span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        total_cost (scalar):  损失值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z_i = np.dot(X[i], w) + b</span><br><span class="line">        f_wb_i = sigmoid(z_i)</span><br><span class="line">        cost += -y[i] * np.log(f_wb_i) - (<span class="number">1</span> - y[i]) * np.log(<span class="number">1</span> - f_wb_i)</span><br><span class="line"></span><br><span class="line">    cost = cost / m</span><br><span class="line"></span><br><span class="line">    reg_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        reg_cost += (w[j] ** <span class="number">2</span>)</span><br><span class="line">    reg_cost = (lambda_ / (<span class="number">2</span> * m)) * reg_cost</span><br><span class="line"></span><br><span class="line">    total_cost = cost + reg_cost</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><h3 id="梯度计算求偏导">梯度计算（求偏导）</h3><p>线性回归和逻辑回归的梯度计算出来格式都一样，注意里面的<spanclass="math inline">\(f_{\vec w,b}(\vec x)\)</span>指代不同即可。</p><p>公式：<span class="math inline">\(\frac{\partial J(\vecw,b)}{\partial b} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}+\frac {\lambda}{m}w_j\)</span></p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>线性回归中,<span class="math inline">\(f_{\vec w,b}(\vec x^{(i)}) =\vec w \cdot \vec x+b\)</span></p><p>逻辑回归中,<span class="math inline">\(f_{\vec w,b}(x) = sigmoid(\vecw\cdot \vec x+b)\)</span></p><p>线性回归：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_linear_reg</span>(<span class="params">X, y, w, b, lambda_</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        dj_dw (ndarray (n,)): 代价函数对w的偏导</span></span><br><span class="line"><span class="string">        dj_db (scalar): 代价函数对b的偏导</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m, n = X.shape </span><br><span class="line">    dj_dw = np.zeros((n,))</span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        err = (np.dot(X[i], w) + b) - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err * X[i, j]</span><br><span class="line">        dj_db = dj_db + err</span><br><span class="line">    dj_dw = dj_dw / m</span><br><span class="line">    dj_db = dj_db / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dj_dw[j] = dj_dw[j] + (lambda_ / m) * w[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw</span><br></pre></td></tr></table></figure><p>逻辑回归：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逻辑回归梯度计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic_reg</span>(<span class="params">X, y, w, b, lambda_</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        dj_dw (ndarray (n,)): 代价函数对w的偏导</span></span><br><span class="line"><span class="string">        dj_db (scalar): 代价函数对b的偏导</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))</span><br><span class="line">    dj_db = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i], w) + b)</span><br><span class="line">        err_i = f_wb_i - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i, j]</span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw / m</span><br><span class="line">    dj_db = dj_db / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dj_dw[j] = dj_dw[j] + (lambda_ / m) * w[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw</span><br></pre></td></tr></table></figure><p>梯度迭代就不写了，之前的文章有详细写过，没什么变化。</p><h2 id="使用正则化后过拟合模型的变化">使用正则化后过拟合模型的变化</h2><p>这是正则化之后的结果：</p><p><img src="https://img.issey.top/img/202209211123110.png" /></p><p><img src="https://img.issey.top/img/202209211123447.png" /></p><p>嗯...对比最开始的两张过拟合，已经好得多了，虽然比起红线模拟的模型还是有一些差距。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型评估 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记4——逻辑回归</title>
      <link href="/article/bba895870268/"/>
      <url>/article/bba895870268/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：请确保你已学会了前几个线性回归的内容。之前涉及的相关概念在此文章不会再提及。</p></blockquote><h1 id="什么是逻辑回归">什么是逻辑回归？</h1><p>逻辑回归是一种广义的线性回归模型，主要用于解决二分类问题。所谓二分类问题，就是比如判断一个邮件是否是垃圾邮件、根据某些特征判断肿瘤是否为恶性肿瘤等问题，我们可以将是/否表示为1/0。简单的二分类数据图如下：</p><p><img src="https://img.issey.top/img/202209211106913.png" /></p><p>（注：左图为只有一个特征的数据模拟图，右图为有两个特征的数据模拟图。）</p><p>以仅有一个特征的二分类（左图）为例，如果我们模拟传统线性回归<spanclass="math inline">\(f(x) = \vec w\cdot \vecx+b\)</span>，并选择0.5作为阈值：当<span class="math inline">\(y \geq0.5\)</span>时我们认为它是种类1，当<spanclass="math inline">\(y&lt;0.5\)</span>时，我们认为它是种类0，那么可以得到以下图像：</p><p><img src="https://img.issey.top/img/202209211106198.png" /></p><p>但是，当我们再加一些数据，它就会模拟成这样：</p><p><img src="https://img.issey.top/img/202209211106951.png" /></p><p>很明显能看出，这样的预测并不准确。</p><p>所以当对二分类问题进行回归分析时，采用传统的线性回归函数进行拟合并不是一个好的方案。于是我们将使用另一种函数——<strong>Sigmoid函数</strong>。</p><h1 id="sigmoid函数">Sigmoid函数</h1><p>Sigmoid函数又称为Logistic函数（逻辑函数），Sigmoid函数输出值在<spanclass="math inline">\((0,1)\)</span>之间，而且可以解决离群点对拟合线性回归的影响，Sigmoid函数在诸多领域都有涉及，这里不再拓展。</p><p>Sigmoid函数表达式：<span class="math inline">\(g(z) =\frac{1}{1+e^{-z}}\)</span></p><p>函数图像：</p><p><img src="https://img.issey.top/img/202209211106626.png" /></p><p><strong>现在使用sigmoid函数拟合上面的例子，当</strong><spanclass="math inline">\(g(z)\geq0.5\)</span><strong>时，我们认为它属于种类1，当</strong><spanclass="math inline">\(g(z)&lt;0.5\)</span><strong>时，我们认为它属于种类0</strong>,于是可以得到下面的图像：</p><p><img src="https://img.issey.top/img/202209211106325.png" /></p><p>（注：橙色的线为决策边界，后面会详细解释。）</p><p>很明显看出该函数拟合的很好。</p><h1 id="决策边界">决策边界</h1><p>上面提到，我们使用sigmoid函数进行预测时，当<spanclass="math inline">\(g(z) \geq 0.5\)</span>，我们认为是种类1，当<spanclass="math inline">\(g(z)&lt;0.5\)</span>时，我们认为是种类0。那么，什么时候<spanclass="math inline">\(g(z)\)</span>等于0.5呢？观察sigmoid函数的图像，当<spanclass="math inline">\(z\)</span>等于0时，<spanclass="math inline">\(g(z) = 0.5\)</span>。所以我们令<spanclass="math inline">\(z = f_{\vec w,b}(\vec x) = \vec w\cdot\vecx+b\)</span>,决策边界即<span class="math inline">\(f_{\vec w,b}(\vec x)= 0\)</span>。</p><p>以上述右图为例（即以有两个特征的二分类为例），决策边界最终求出的函数为<spanclass="math inline">\(f(x) = x_0+x_1 -3\)</span>。当<spanclass="math inline">\(f(x) \geq 0\)</span>，属于种类1；当<spanclass="math inline">\(f(x)&lt;0\)</span>，属于种类0。图像为：</p><p><img src="https://img.issey.top/img/202209211107168.png" /></p><p>可以看到决策边界将两个不同的种类分开了。</p><p>又比如，当<span class="math inline">\(f_{\vec w,b}(\vecx)\)</span>为更复杂的多项式时，可以画出以下图像：</p><p><img src="https://img.issey.top/img/202209211107746.png" /></p><h1 id="逻辑回归的损失函数">逻辑回归的损失函数</h1><h2 id="为什么平方误差模型不可行">为什么平方误差模型不可行？</h2><p>在之前的线性回归中，我们使用平方误差作为我们的代价函数（损失函数）：、</p><p><span class="math display">\[\begin{split}J(\vec{w},b) = \frac {1} {2m}\sum_{i = 0}^{m-1}(\haty^{(i)}-y^{(i)})^2 \\=\frac {1} {2m}\sum_{i = 0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2\end{split}\]</span></p><p>其中，<span class="math inline">\((i)\)</span>为样例序号。</p><p>那时<span class="math inline">\(f_{\vec w,b}(\vec x) = \vecw\cdot\vec x+b\)</span>,平方误差代价函数可以平滑下降直到一个最低点。</p><p>下图是一元一次线性回归<span class="math inline">\(f(x) =wx+b\)</span>的代价函数图像：</p><p><img src="https://img.issey.top/img/202209211108683.png" /></p><p>但是逻辑回归的函数为<span class="math inline">\(sigmoid(\vecw\cdot\vec x+b) = \frac{1}{1+e^{-z}}\)</span>，其中，<spanclass="math inline">\(z = \vec w\cdot\vec x+b\)</span>。</p><p>如果再使用平方误差作为代价函数，它的图像将会是凹凸不平的：</p><p><img src="https://img.issey.top/img/202209211108105.png" /></p><p>这意味着梯度下降算法很可能无法找到最低点，会卡在某个极小值点。</p><p><img src="https://img.issey.top/img/202209211108417.png" /></p><p>为了解决这个问题，我们将使用另一种模型作为我们的代价函数：</p><h2 id="对数损失函数">对数损失函数</h2><blockquote><p>为什么使用对数损失函数作为逻辑回归的损失函数而不使用其他函数？</p><p>使用对数损失函数作为逻辑回归的损失函数是由极大似然估计推导所得，这里不进行拓展。</p></blockquote><h3 id="单个样例损失">单个样例损失：</h3><p><span class="math display">\[L(\hat y,y) = \left\{\begin{aligned}-log(\hat y)        if\quad y = 1\\-log(1-\hat y)        if\quad y = 0\\\end{aligned}\right.\]</span></p><p>其中，<span class="math inline">\(\hat y = g_{\vec w,b}(\vec x) =sigmoid(\vec w\cdot \vec x+b)，\hat y\in (0,1)\)</span></p><p>解释一下含义：</p><p>当真实值为1时，图像为：</p><p><img src="https://img.issey.top/img/202209211109293.png" /></p><p>可以看出，在真实值y为1的情况下：当预测值<spanclass="math inline">\(\haty\)</span>接近1时，计算出的损失很小；而当<spanclass="math inline">\(\haty\)</span>接近0时，计算出的损失很大很大。换成人话，就是比如说某人真实情况是“很胖”，但是预测出的却是“很廋”，这时预测值和真实值的差距就很大很大。</p><p>同理，当真实值为0时，图像为：</p><p><img src="https://img.issey.top/img/202209211109732.png" /></p><p>在真实值y为0的情况下：当预测值<span class="math inline">\(\haty\)</span>接近0时，计算出的损失很小；而当<spanclass="math inline">\(\hat y\)</span>接近0时，计算出的损失很大很大。</p><p>化简<span class="math inline">\(L(\hat y,y)\)</span>,可得<spanclass="math inline">\(L(\hat y,y) = -ylog(\hat y)-(1-y)log(1-\haty)\)</span></p><h3 id="逻辑回归损失函数">逻辑回归损失函数</h3><p>上式求和即可得：</p><p>公式：<span class="math inline">\(J(\vec w,b) = \frac 1 m\sum_{i=0}^{m-1}L(g_{\vec w,b}(\vec x),y)\)</span></p><p>让我们用这个新的代价函数模拟一下最开始左图的代价函数模型：</p><p><img src="https://img.issey.top/img/202209211110457.png" /></p><p>现在这个图像很适合使用梯度下降算法来找到它的最低点。</p><h1 id="梯度下降算法">梯度下降算法</h1><p>步骤与线性回归相同，同时进行以下直到收敛：</p><p><span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><p>其中，</p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial w_j}= \frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>这两个偏导求出来的公式和线性回归的几乎长得一样，但是并不代表他们一样。</p><p>这里<span class="math inline">\(g_{\vec w,b}(x) = sigmoid(\vec w\cdot\vec x+b)\)</span></p><blockquote><p>求导过程请参考文章：<ahref="https://blog.csdn.net/Piratesa/article/details/100586729">逻辑回归梯度下降法</a></p></blockquote><p>现在只需要利用该算法求得<span class="math inline">\(\vecw,b\)</span>即可。</p><blockquote><p>关于多分类问题：多分类问题也可以使用逻辑回归解决。之后会出专门的文章，这里暂时不写。</p></blockquote><h1 id="补充f1-score评价指标">补充：F1-score评价指标</h1><h2 id="f1-score简介">F1-Score简介</h2><p>F1分数（F1 Score），是<ahref="https://so.csdn.net/so/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6&amp;spm=1001.2101.3001.7020">统计学</a>中用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的精确率和召回率。F1分数可以看作是模型精确率和召回率的一种加权平均，它的最大值是1，最小值是0。</p><h3 id="相关概念">相关概念</h3><p>下面先介绍几个概念：</p><ul><li><p>TP（rue Positive）：正样本被判定为正样本</p></li><li><p>FP（False Positive）：负样本被判定为正样本</p></li><li><p>TN（True Negative）：负样本被判定为负样本</p></li><li><p>FN（False Negative）：正样本被判定为负样本</p></li></ul><p>精确度/查准率：指分类器预测为正例中正样本所占比重：</p><p><span class="math inline">\(Precision = \frac {TP}{TP+FP}\)</span></p><p>召回率/查全率：指预测为正例占总正例比重：</p><p><span class="math inline">\(Recall = \frac {TP}{TP+FN}\)</span></p><p>F-Score算法将同时使用以上两个公式，此外，介绍另一种常用的准确率概念：</p><p>准确率，指分类器判断正确的占总样本的比重：</p><p><span class="math inline">\(Accuracy = \frac{TP+TN}{TP+TN+FP+FN}\)</span></p><h3 id="f-score">F-Score</h3><p>具体来源等等就不拓展了，有兴趣可以自查。</p><p>F-Score是可以综合考虑精确度（Precision）和召回率（Recall）的调和值，公式如下：</p><p><span class="math inline">\(F Score =(1+\beta^2)*\frac{Precision*Recall}{\beta^2Precision+Recall}\)</span></p><p>当<spanclass="math inline">\(\beta=1\)</span>时，被称为F1-Score或F1-Measure。此时精确度和召回率权重相同。</p><p>当我们认为精确度更重要，调整<spanclass="math inline">\(\beta\)</span>&lt;1；</p><p>当我们认为召回率更重要，调整<spanclass="math inline">\(\beta\)</span>&gt;1。</p><h1 id="示例及代码">示例及代码：</h1><p>数据来源：</p><p>https://www.kaggle.com/datasets/muratkokludataset/pumpkin-seeds-dataset</p><h2 id="问题描述">问题描述：</h2><p>现在有两类南瓜种子（CERCEVELIK,URGUP_SIVRISI）以及它们的一些特征：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@ATTRIBUTE Area    INTEGER</span><br><span class="line">@ATTRIBUTE Perimeter    REAL</span><br><span class="line">@ATTRIBUTE Major_Axis_Length    REAL</span><br><span class="line">@ATTRIBUTE Minor_Axis_Length    REAL</span><br><span class="line">@ATTRIBUTE Convex_Area    INTEGER</span><br><span class="line">@ATTRIBUTE Equiv_Diameter    REAL</span><br><span class="line">@ATTRIBUTE Eccentricity    REAL</span><br><span class="line">@ATTRIBUTE Solidity    REAL</span><br><span class="line">@ATTRIBUTE Extent    REAL</span><br><span class="line">@ATTRIBUTE Roundness    REAL</span><br><span class="line">@ATTRIBUTE Aspect_Ration    REAL</span><br><span class="line">@ATTRIBUTE Compactness    REAL</span><br></pre></td></tr></table></figure><p>现在请根据已知数据集对这两类南瓜种子进行（逻辑回归）分类并判断准确率。</p><blockquote><p>代码说明：会使用sklearn进行数据分割以及模型评价（F1-score），逻辑回归部分全部自主实现。</p></blockquote><p>所需要的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math,copy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><h2 id="数据预处理">数据预处理</h2><h3 id="特征选择与数据集拆分">特征选择与数据集拆分</h3><p>为了可视化，这里只选择其中两个特征作为特征集（Major_Axis_Length、Minor_Axis_Length）。</p><p>导入和提取数据、拆分训练集和数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">data = pd.read_excel(<span class="string">r&#x27;.\Pumpkin_Seeds_Dataset\Pumpkin_Seeds_Dataset.xlsx&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换数据</span></span><br><span class="line">color = []</span><br><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">&#x27;Class&#x27;</span>]:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&#x27;Çerçevelik&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="string">&#x27;Ürgüp Sivrisi&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取所需数据</span></span><br><span class="line">x_1 = np.array(df[<span class="string">&#x27;Major_Axis_Length&#x27;</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">x_2 = np.array(df[<span class="string">&#x27;Minor_Axis_Length&#x27;</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X_features = np.c_[x_1,x_2]</span><br><span class="line">Y_target = np.array(label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X_features,Y_target,train_size=<span class="number">0.5</span>,random_state=<span class="number">45</span>)</span><br></pre></td></tr></table></figure><h3 id="特征缩放z-score标准化">特征缩放（Z-score标准化）</h3><p>注意：特征缩放一定要用对地方，应该在拆分完训练集和测试集后，<strong>仅对训练集使用，不应该把训练集和测试集放在一起标准化</strong>，对测试集的操作应该是在对训练集标准化后，使用通过训练集标准化时计算得到的平均值、方差等进行标准化。</p><p>这里使用Z-score标准化进行特征缩放。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Z-score标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Zscore</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;x是(m,n)的矩阵，m为样本个数，n为特征数目&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 找每列(特征)均值</span></span><br><span class="line">    mu = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 找每列(特征)标准差</span></span><br><span class="line">    sigma = np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = (X - mu) / sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm,mu,sigma</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准化特征集</span></span><br><span class="line">X_train,mu,sigma = Zscore(X_train)</span><br><span class="line"><span class="comment"># 绘制训练集</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y_train:</span><br><span class="line">     <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">         color.append(<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         color.append(<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">plt.scatter(X_train[:,<span class="number">0</span>],X_train[:,<span class="number">1</span>],color = color)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Major_Axis_Length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Minor_Axis_Length&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>标准化后的训练集：</p><p><img src="https://img.issey.top/img/202209211110611.png" /></p><p>注：橘色为Çerçevelik种类，蓝色为Ürgüp Sivrisi种类。</p><h2 id="实现逻辑回归">实现逻辑回归</h2><h3 id="sigmoid函数-1">sigmoid函数</h3><p>公式：<span class="math inline">\(g(z) =\frac{1}{1+e^{-z}}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param z: 标量</span></span><br><span class="line"><span class="string">    :return: 标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure><h3 id="损失函数">损失函数</h3><p>公式：<span class="math inline">\(J(\vec w,b) = \frac 1 m\sum_{i=0}^{m-1}L(g_{\vec w,b}(\vec x),y)\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逻辑回归损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 特征集，矩阵</span></span><br><span class="line"><span class="string">    :param y: 目标值，列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return: 对数损失值，标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z_i = np.dot(X[i],w)+b</span><br><span class="line">        g_wb_i = sigmoid(z_i)</span><br><span class="line">        cost += -y[i]*np.log(g_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-g_wb_i)</span><br><span class="line">    cost = cost/m</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="梯度计算函数求偏导">梯度计算函数（求偏导）</h3><p>公式：</p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>其中，<span class="math inline">\(g_{\vec w,b}(x) = sigmoid(\vecw\cdot \vec x+b)\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 特征集，矩阵</span></span><br><span class="line"><span class="string">    :param y: 目标值，列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        dj_dw: 对数损失函数对向量w的偏导</span></span><br><span class="line"><span class="string">        dj_db: 对数损失函数对b的偏导</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))</span><br><span class="line">    dj_db = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        g_wb_x = sigmoid(np.dot(X[i],w)+b)</span><br><span class="line">        dj_db = dj_db + g_wb_x - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + (g_wb_x - y[i])*X[i,j]</span><br><span class="line">    dj_dw = dj_dw/m</span><br><span class="line">    dj_db = dj_db/m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dj_dw,dj_db</span><br></pre></td></tr></table></figure><h3 id="梯度迭代函数">梯度迭代函数</h3><p>公式：</p><p><span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_regression</span>(<span class="params">X_train,y_train,alpha,num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X_train: 特征集，矩阵</span></span><br><span class="line"><span class="string">    :param y_train: 目标值，列表</span></span><br><span class="line"><span class="string">    :param alpha: 学习率，标量</span></span><br><span class="line"><span class="string">    :param num_iters: 训练次数，标量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        w: 训练得出的w，向量</span></span><br><span class="line"><span class="string">        b: 训练得出的b，标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m,n = X_train.shape</span><br><span class="line">    init_w = np.zeros((n,)) <span class="comment"># n个特征，所以w有n个</span></span><br><span class="line">    init_b = <span class="number">0</span></span><br><span class="line">    w = copy.deepcopy(init_w)</span><br><span class="line">    b = init_b</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(i)</span><br><span class="line">        dj_dw,dj_db = compute_gradient_logistic(X_train,y_train,w,b)</span><br><span class="line">        w = w - alpha*dj_dw</span><br><span class="line">        b = b - alpha*dj_db</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h3 id="训练数据集绘制拟合决策边界">训练数据集，绘制拟合决策边界</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_w,model_b = logistic_regression(X_train,y_train,alpha=<span class="number">0.4</span>,num_iters=<span class="number">10000</span>)</span><br><span class="line">x_line = np.c_[np.arange(-<span class="number">4</span>,<span class="number">4</span>,<span class="number">0.1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>),np.arange(-<span class="number">4</span>,<span class="number">4</span>,<span class="number">0.1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)]</span><br><span class="line"><span class="built_in">print</span>(model_w)</span><br><span class="line">y_line = np.dot(x_line,model_w)+model_b</span><br><span class="line"></span><br><span class="line">plt.plot(x_line,y_line,label = <span class="string">&#x27;Predicted Value&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这是当训练次数为10000时得出的决策边界：</p><p><img src="https://img.issey.top/img/202209211111755.png" /></p><h2 id="模型预测和评价">模型预测和评价</h2><p>算出<span class="math inline">\(\vec w,b\)</span>后，代入<spanclass="math inline">\(sigmoid(\vec w\cdot\vecx+b)\)</span>，阈值为0.5。大于等于0.5为1类，小于0.5为0类。</p><p>注意要先将特征测试集标准化。</p><p>这里使用F1-Score（F1分数）进行评价。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line"><span class="keyword">def</span>  <span class="title function_">model_predict</span>(<span class="params">X,w,b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 测试集</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return: 预测值（列表）</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">        <span class="keyword">if</span> sigmoid(np.dot(i,w)+b) &gt;= <span class="number">0.5</span>:</span><br><span class="line">            y.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">X_test = (X_test - mu) / sigma</span><br><span class="line">y_predict = model_predict(X_test,model_w,model_b)</span><br><span class="line">f1_score = f1_score(y_test,y_predict,average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;F1分数为:<span class="subst">&#123;<span class="built_in">round</span>(f1_score,<span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://img.issey.top/img/202209211111674.png" /></p><h3 id="补充现在将特征集扩大到所有">补充：现在将特征集扩大到所有</h3><p>直接上结果：</p><p><img src="https://img.issey.top/img/202209211111590.png" /></p><p>稍微比原来准确了一丢丢。</p><h1 id="补充使用sklearn完成逻辑回归">补充：使用sklearn完成逻辑回归</h1><p>前面讲了一大堆，实际上sklearn几行代码搞定~</p><p><del>泪目</del></p><p>还是刚才那个数据（扩大到所有特征后的），直接上代码!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math,copy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">data = pd.read_excel(<span class="string">r&#x27;.\Pumpkin_Seeds_Dataset\Pumpkin_Seeds_Dataset.xlsx&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">&#x27;Class&#x27;</span>]:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&#x27;Çerçevelik&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="string">&#x27;Ürgüp Sivrisi&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_features_bk = np.array(df)</span><br><span class="line">X_features = X_features_bk[:,:-<span class="number">1</span>]</span><br><span class="line">X_features = np.float32(X_features)</span><br><span class="line">Y_target = np.array(label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X_features,Y_target,train_size=<span class="number">0.5</span>,random_state=<span class="number">45</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn进行Z-score标准化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)  <span class="comment"># 标准化训练集X</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制训练集</span></span><br><span class="line">color = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y_train:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">        color.append(<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        color.append(<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">plt.scatter(X_train[:, <span class="number">2</span>], X_train[:, <span class="number">3</span>], color=color)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Major_Axis_Length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Minor_Axis_Length&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练逻辑回归模型</span></span><br><span class="line">lr_model = LogisticRegression()</span><br><span class="line">lr_model.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化测试集x</span></span><br><span class="line"><span class="comment"># 只有训练集才fit_transform，测试集是transform，原因上面自己写代码的时候说过了</span></span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = lr_model.predict(X_test)</span><br><span class="line"><span class="comment"># F1-Score评估</span></span><br><span class="line">f1_score = f1_score(y_test,y_pred,average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;F1分数为:<span class="subst">&#123;<span class="built_in">round</span>(f1_score,<span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>中间我们拿出之前那两列来画了下图，这个图是同样的随机种子，采用sklearn的Z-score标准化后得出的图像：</p><p><img src="https://img.issey.top/img/202209211111790.png" /></p><p>对比我们之前自己处理的数据，只能说，完全一致好吧。</p><p>然后来看看结果：</p><p><img src="https://img.issey.top/img/202209211111682.png" /></p><p>甚至比我们自己写的差了这么一丢丢。导致这个的原因是它的学习率和训练次数和我们选的不一样，不过计算速度比我们快很多，我估计它训练次数选的比较少。如果我们调整自己的参数，也可以达到同样的效果！</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记3——多项式回归</title>
      <link href="/article/31f58288ca8b/"/>
      <url>/article/31f58288ca8b/</url>
      
        <content type="html"><![CDATA[<h1 id="多项式回归">多项式回归</h1><blockquote><p>多项式回归虽然不再用直线拟合，但也是线性回归的一种，可以转化为多元线性回归，利用多元线性回归的函数解决。所以请确保熟悉多元线性回归相关知识点：<ahref="https://www.issey.top/article/92ab78771d6d/">机器学习笔记2——多元线性回归| issey的博客</a></p></blockquote><p>在学习多项式回归之前，你可能需要先了解以下内容：</p><h2 id="前置知识">前置知识</h2><p><ahref="https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%BC%8F/10660961?fr=aladdin">多项式</a></p><p><ahref="https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0/10686272?fr=aladdin">多项式函数</a></p><h3 id="numpy-c_函数">Numpy c_函数</h3><p>该函数可以实现按列拼接矩阵，具体用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">X = np.c_[x,x**<span class="number">2</span>,x**<span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;np.c_(x,x**2,x**3):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209210054285.png" /></p><h2 id="问题引入">问题引入</h2><p>现实生活中呈（狭义）线性关系的事物联系很少，绝大部分都是（狭义）非线性的，即呈曲线形式的关系。所以我们需要引入多项式回归来更好的拟合数据。</p><h2 id="多项式回归函数">多项式回归函数</h2><p>多项式回归又分为多元多项式回归和一元多项式回归：</p><p>一元多项式：<span class="math inline">\(f(x) =w_nx^n+w_{n-1}x^{n-1}+...+w_2x^2+w_1x+b\)</span></p><p>多元多项式：这里只举二元二次多项式</p><p><span class="math inline">\(f(x_1,x_2) =w_1x_1^2+w_2x^2+w_3x_1x_2+w_4x_1+w_5x_5+b\)</span></p><p>不管它是什么多项式，在处理上不同的地方都只是在数据预处理上，核心方法不变。这里为了绘图，以一元多项式进行讲解。</p><h2 id="核心思路">核心思路</h2><p>令<span class="math inline">\(x_1 = x\)</span>,<spanclass="math inline">\(x_2 = x^2\)</span>,...,<spanclass="math inline">\(x_n =x^n\)</span>,于是多项式回归转化成了多元线性回归。然后套用多元线性回归的函数求解向量<spanclass="math inline">\(\vec w\)</span>和常量b即可。</p><h2 id="示例">示例</h2><blockquote><p>注：以下函数均来自上篇文章<ahref="https://www.issey.top/article/92ab78771d6d/">机器学习笔记2——多元线性回归| issey的博客</a></p><p>请配合使用！！！</p><p>Zscore()：Z-score标准化</p><p>gradient_descent()：拟合回归函数,为了更好用，把初始化w和b的函数重新写了一下。</p></blockquote><p>gradient_descent()初始化w，b修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m, n = X.shape</span><br><span class="line"><span class="comment"># initialize parameters</span></span><br><span class="line">init_w = np.zeros(n)</span><br><span class="line">init_b = <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="数据生成">数据生成</h3><p>以下代码可以生成该示例的训练集数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_generation</span>():</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        X: 自变量训练集（矩阵）</span></span><br><span class="line"><span class="string">        Y: 应变量训练集（一维）</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>,<span class="number">40</span>,<span class="number">1</span>)</span><br><span class="line">    y = <span class="number">0.25</span>*x**<span class="number">2</span>-<span class="number">0.20</span>*x**<span class="number">3</span>+<span class="number">0.0055</span>*x**<span class="number">4</span></span><br><span class="line">    <span class="comment"># 要将X训练集转化为(m,n)的矩阵，即m个例子，n个特征，现在是一元，所以只有一列</span></span><br><span class="line">    x = x.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 生成随机数，稍微打乱y</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    y = y + np.random.random(<span class="number">40</span>)*<span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure><h3 id="转化多项式回归训练集">转化多项式回归训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">x_transform</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param x:转化前的X训练集</span></span><br><span class="line"><span class="string">    :return: 转化后的X训练集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    X_train = np.c_[x,x**<span class="number">2</span>,x**<span class="number">3</span>,x**<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_train</span><br></pre></td></tr></table></figure><h3 id="预测和绘图">预测和绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成数据并绘制训练集图</span></span><br><span class="line">    x_train,y_train = data_generation()</span><br><span class="line">    plt.scatter(x_train,y_train,marker=<span class="string">&#x27;x&#x27;</span>,c=<span class="string">&#x27;r&#x27;</span>,label = <span class="string">&#x27;Actual Value&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;y&quot;</span>)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line">    <span class="comment"># 将x训练集转化为多元线性训练集</span></span><br><span class="line">    x_train_transform = x_transform(x_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测并绘图</span></span><br><span class="line">    model_w,model_b = gradient_descent(x_train_transform,y_train,alpha=<span class="number">0.000000000003</span>,num_iters=<span class="number">100000</span>)</span><br><span class="line">    plt.plot(x_train,np.dot(x_train_transform,model_w) + model_b,label=<span class="string">&quot;Predicted Value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://img.issey.top/img/202209210101983.png" /></p><h3 id="特征缩放">特征缩放</h3><p>因为转化为多元一次回归后，会发现每个特征的范围差的特别大（比如<spanclass="math inline">\(x^4\)</span>和<spanclass="math inline">\(x\)</span>），为了照顾取值大的特征，学习率必须设置的非常小，所以梯度下降特别慢。这时，就要用到之前说过的特征缩放了。</p><p>特征缩放后的预测和绘图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将x训练集转化为多元线性训练集</span></span><br><span class="line">x_train_transform = x_transform(x_train)</span><br><span class="line">x_train_transform, mu, sigma = Zscore(x_train_transform)</span><br><span class="line"><span class="comment"># 预测并绘图</span></span><br><span class="line">model_w,model_b = gradient_descent(x_train_transform,y_train,alpha=<span class="number">0.5</span>,num_iters=<span class="number">10000</span>)</span><br><span class="line">plt.plot(x_train,np.dot(x_train_transform,model_w) + model_b,label=<span class="string">&quot;Predicted Value&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>对比学习率和迭代次数，根本不是一个级别的。而且在更短时间内模拟的更好：</p><p><img src="https://img.issey.top/img/202209210055530.png" /></p><p>所以特征缩放很重要！！</p><h2 id="补充关于多项式的次数选择">补充：关于多项式的次数选择</h2><blockquote><p>次数的选择：多项式函数有多种，一般来说，需要先观察数据的形状，再去决定选用什么形式的多项式函数来处理问题。比如，从数据的散点图观察，如果有一个“弯”，就可以考虑用二次多项式；有两个“弯”，可以考虑用三次多项式；有三个“弯”，则考虑用四次多项式，以此类推。当然，如果预先知道数据的属性，则有多少个</p><p>虽然真实的回归函数不一定是某个次数的多项式，但只要拟合的好，用适当的多项式来近似模拟真实的回归函数是可行的。</p><p>原文链接：<ahref="https://blog.csdn.net/weixin_44225602/article/details/112752565">多项式回归详解从零开始 从理论到实践</a></p></blockquote><p>稍微尝试了一下这个规律，刚才我构造的函数虽然是四次方的，但是只用三次的多项式也可以模拟出来效果较好的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.c_[x,x**<span class="number">2</span>,x**<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209210055887.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记2——多元线性回归</title>
      <link href="/article/92ab78771d6d/"/>
      <url>/article/92ab78771d6d/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ul><li><ahref="https://www.issey.top/article/c4e174e36609/">机器学习笔记1——一元线性回归| issey的博客</a></li></ul><h1 id="前言">前言</h1><p>​多元线性回归总体思路和一元线性回归相同，都是代价函数+梯度下降，所以中心思想参考一元线性回归。</p><p>​在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只用一个自变量进行预测或估计更有效，更符合实际。</p><p>​ 在开始之前，你可能需要先了解以下知识：</p><h1 id="前置知识">前置知识</h1><h2 id="向量化">向量化</h2><p>​当我们想自己实现向量的点乘时，通常会想到利用for循环来完成，例如有<spanclass="math inline">\(\vec{a}\cdot\vec{b}\)</span>，可以写为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(a.shape[<span class="number">0</span>]):</span><br><span class="line">    x = x + a[i] * b[i]</span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>​然而，Python的Numpy库提供了一个dot()函数可以帮助我们进行向量化计算，作用是加快向量的运算速度，在数据量较大时效率会明显提高。其原理是Numpy利用了计算机底层硬件单指令多数据(SIMD)管道，这在数据集非常大的机器学习中至关重要。所以，向量化是机器学习中一个非常重要和实用的方法。</p><p>​下图是使用Numpy的dot函数与自己利用for循环实现的向量点乘分别对长度各为10000000的向量a、b点乘运行时间比较：</p><p><img src="https://img.issey.top/img/202209202204205.png" /></p><h2 id="特征缩放">特征缩放</h2><h3 id="为什么要特征缩放">为什么要特征缩放</h3><p>​当有多个特征时，在有的算法中，我们需要使这些特征具有相似的尺度（无量纲化）。</p><p>​ 这里介绍特征缩放在多元线性回归中的作用。</p><p>​拿下面”问题引入“里得数据来说，各个特征的范围差距太大，我们将每个特征对价格的影响可视化，可以看出哪些因素对价格影响更大。会得到以下图像：</p><p><img src="https://img.issey.top/img/202209202204109.png" /></p><p>​由于各个特征的数量差距过大，代价函数的等高线将会是扁长的，在梯度下降时也会是曲折的，而且计算时长相对会很长（<strong>因为学习率是通用的，为了照顾尺度大的特征，学习率必须设置的很小，学习率越小，下降速度就越慢</strong>）：</p><p><img src="https://img.issey.top/img/202209202205655.png" /></p><p>​特征缩放将每个特征的范围统一，可以使梯度下降变”平滑“，并且大大提高计算速度（<strong>因为可以调大学习率</strong>）。</p><p><img src="https://img.issey.top/img/202209202205070.png" /></p><h3 id="特征缩放的方法">特征缩放的方法</h3><p>​ 特征缩放的方法有许多种，这里介绍两种：</p><h4 id="均值归一化">均值归一化</h4><p>​ 公式：<span class="math inline">\(x_i = \frac{x_i -\mu}{max-min}\)</span></p><p>​ 其中，<spanclass="math inline">\(\mu\)</span>为样本中该特征的均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均值归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MeanNormalization</span>(<span class="params">x</span>):</span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;x为列表&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> [(<span class="built_in">float</span>(i)-np.mean(x))/<span class="built_in">float</span>(<span class="built_in">max</span>(x)-<span class="built_in">min</span>(x)) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br></pre></td></tr></table></figure><h4 id="z-score标准化推荐">Z-score标准化(推荐)</h4><p>​ 公式：<span class="math inline">\(x_j^{(i)} =\frac{x_j^{(i)}-\mu_j}{\delta_j}\)</span></p><p>​ 其中，<span class="math inline">\(j\)</span>为<spanclass="math inline">\(X\)</span>矩阵中的特征（或列),<spanclass="math inline">\((i)\)</span>为样本序号。<spanclass="math inline">\(u_j\)</span>为特征<spanclass="math inline">\(j\)</span>的均值，<spanclass="math inline">\(\delta_j\)</span>为特征<spanclass="math inline">\(j\)</span>的标准差。</p><p>​ <span class="math inline">\(\mu_j =\frac{1}{m}\sum_{i=0}^{m-1}x_j^{(i)}\)</span></p><p>​ <span class="math inline">\(\delta_j^2 =\frac{1}{m}\sum_{i=0}^{m-1}(x_j^{(i)}-\mu_j)^2\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Z-score标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Zscore</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;x是(m,n)的矩阵，m为样本个数，n为特征数目&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 找每列(特征)均值</span></span><br><span class="line">    mu = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 找每列(特征)标准差</span></span><br><span class="line">    sigma = np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = (X - mu) / sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm</span><br></pre></td></tr></table></figure><p>​ 或者使用sklearn:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># X为同上矩阵</span></span><br><span class="line">X_norm = preprocessing.scale(X)</span><br></pre></td></tr></table></figure><hr /><h1 id="问题引入">问题引入</h1><p>​示例：现在你有以下数据，请利用这些值构建一个线性回归模型，并预测一栋1200平米，3间卧室，1层，年龄为40年的房屋的价格。</p><table><thead><tr class="header"><th>面积(平方)</th><th>卧室数量</th><th>楼层数</th><th>房屋年龄</th><th>价格（1000s dollars）</th></tr></thead><tbody><tr class="odd"><td>952</td><td>2</td><td>1</td><td>65</td><td>271.5</td></tr><tr class="even"><td>1244</td><td>3</td><td>2</td><td>64</td><td>300</td></tr><tr class="odd"><td>1947</td><td>3</td><td>2</td><td>17</td><td>509.8</td></tr><tr class="even"><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td></tr></tbody></table><h1 id="多元线性回归模型">多元线性回归模型</h1><h2 id="多元线性回归函数">多元线性回归函数</h2><p>​对于上面提到的数据，一共有四种特征（面积，卧室数量，楼层，房屋面积），记为<spanclass="math inline">\(x_1,x_2,x_3,x_4\)</span>,每个特征分别需要一个<spanclass="math inline">\(w\)</span>,所以对应的线性回归函数为<spanclass="math inline">\(f(x) = w_1x_1+w_2x_2+w_3x_3+w_4x_4+b\)</span>.</p><p>​ 推广到一般多元线性回归函数，即：</p><p>​ <span class="math inline">\(f(x) = w_1x_1+...+w_nx_n+b\)</span>,    其中，n为特征数量。</p><p>​ 观察<span class="math inline">\(f(x)\)</span>,我们发现可以将<spanclass="math inline">\(w\)</span>看作一列，<spanclass="math inline">\(x\)</span>看作一列。于是<spanclass="math inline">\(f(x)\)</span>又可以写为：<spanclass="math inline">\(f_{\vec{w},b}(\vec{x}) =\vec{w}\cdot\vec{x}+b\)</span>,   （注意为点乘）</p><p>​ 这样我们的目标便是利用已知数据通过梯度下降算法找到最合适的<spanclass="math inline">\(\vec{w}\)</span>和b。</p><h2 id="转化为矩阵">转化为矩阵</h2><p>​ 我们可以将训练集转化为<spanclass="math inline">\((m,n)\)</span>的矩阵，m表示示例，n表示特征，于是训练集X可以写为：</p><p><span class="math display">\[\begin{pmatrix} x_{0}^{(0)}&amp;x_{1}^{(0)}&amp;...&amp; x_{n-1}^{(0)}\\ x_{0}^{(1)}&amp;x_{1}^{(1)}&amp;...&amp; x_{n-1}^{(1)}\\...\\x_{0}^{(m-1)}&amp;x_{1}^{(m-1)}&amp;...&amp;x_{n-1}^{(m-1)}\end{pmatrix} \quad\]</span> ​ 注：<spanclass="math inline">\(\vec{x}^{(i)}\)</span>表示含有第i个示例的<strong>向量</strong>，<spanclass="math inline">\(x_j^{(i)}\)</span>表示第i个示例的第j个特征。因为每种特征对应一个<spanclass="math inline">\(w_i\)</span>,所以有向量：</p><p><span class="math display">\[\vec{w} = \begin{pmatrix}w_0\\w_1\\...\\w_{n-1}\end{pmatrix} \quad\]</span></p><h2 id="多元线性回归模型的代价函数">多元线性回归模型的代价函数</h2><p>​ 因为<span class="math inline">\(\hat{y}^{(i)} =f_{\vec{w},b}(\vec{x}^{(i)})=\vec{w}\cdot\vec{x}^{(i)}+b\)</span>,    其中<spanclass="math inline">\(\hat{y}^{(i)}\)</span>为将第i个示例的向量带入回归函数得出的预测值。那么根据一元线性回归的代价函数的定义，可得多元线性回归模型的代价函数为：</p><p>​ <span class="math inline">\(J(\vec{w},b) =\frac{1}{2m}\sum_{i=0}^{m-1}(\hat{y}^{(i)}-y^{(i)})^2=\frac{1}{2m}\sum_{i=0}^{m-1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2\)</span></p><h2 id="多元线性回归模型梯度下降函数">多元线性回归模型梯度下降函数</h2><p>​ 重复同时进行下列行为直到收敛：</p><p>​ <span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p>​ <span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><p>​ 其中，a为学习率。化简偏导得：</p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partialw_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b}= \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>​ 其中，n是特征数量，m是训练集示例数量。</p><p>​ 收敛时的<span class="math inline">\(\vec w,b\)</span>即为所求。</p><h1 id="问题解析含代码">问题解析（含代码）</h1><h2 id="导入并标准化训练集">导入并标准化训练集</h2><p>​数据就不上传了，如果想动手测验，直接搞个矩阵把上面那三行当成训练集就行。</p><p><strong>注意标准化时需要把均值和标准差也返回过去，便于对测试数据进行缩放。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Z-score标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Zscore</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;X是(m,n)的矩阵，m为样本个数，n为特征数目&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找每列(特征)均值</span></span><br><span class="line">    mu = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 找每列(特征)标准差</span></span><br><span class="line">    sigma = np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = (X - mu) / sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm,mu,sigma</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;houses.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    X_train = data[:,:<span class="number">4</span>]</span><br><span class="line">    Y_train = np.ravel(data[:,<span class="number">4</span>:<span class="number">5</span>]) <span class="comment"># 把Y从列转为一维数组</span></span><br><span class="line">    <span class="comment"># 将X训练集拿去标准化</span></span><br><span class="line">    X_train = Zscore(X_train)</span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br></pre></td></tr></table></figure><h2 id="多元线性代价函数">多元线性代价函数</h2><p>​ 这里只是把代码写出来，实际上计算回归函数时用不上。</p><p>​ 对应公式：<span class="math inline">\(J(\vec{w},b) =\frac{1}{2m}\sum_{i=0}^{m-1}(\hat{y}^{(i)}-y^{(i)})^2=\frac{1}{2m}\sum_{i=0}^{m-1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param X: 矩阵，每一行代表一个示例向量</span></span><br><span class="line"><span class="string">    :param y: 列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return: cost(标量)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_x_i = np.dot(X[i],w)</span><br><span class="line">        cost = cost + (f_x_i - y[i])**<span class="number">2</span></span><br><span class="line">    cost = cost/(<span class="number">2</span>*m)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="梯度计算函数">梯度计算函数</h2><p>​ 对应公式：</p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partialw_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b}= \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 矩阵，每一行代表一个示例向量</span></span><br><span class="line"><span class="string">    :param y: 列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        sum_dw(标量，wj对于代价函数的偏导)</span></span><br><span class="line"><span class="string">        sum_db(标量，b对于代价函数的偏导)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    sum_dw = np.zeros((n,))</span><br><span class="line">    sum_db = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        err = (np.dot(X[i],w) + b) - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sum_dw[j] = sum_dw[j] + err*X[i,j]</span><br><span class="line">        sum_db = sum_db + err</span><br><span class="line">    sum_dw = sum_dw/m</span><br><span class="line">    sum_db = sum_db/m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sum_dw,sum_db</span><br></pre></td></tr></table></figure><h2 id="梯度迭代函数">梯度迭代函数</h2><p>​ 对应公式：</p><p>     重复同时进行下列行为直到收敛。</p><p>     <span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p>     <span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w_init, b_init,alpha, num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param X: 矩阵，每一行代表一个示例向量</span></span><br><span class="line"><span class="string">    :param y: 列表</span></span><br><span class="line"><span class="string">    :param w_init: w初始值，向量</span></span><br><span class="line"><span class="string">    :param b_init: b初始值，标量</span></span><br><span class="line"><span class="string">    :param alpha: 学习率</span></span><br><span class="line"><span class="string">    :param num_iters: 迭代次数</span></span><br><span class="line"><span class="string">    :return: 训练出的w和b</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    w = copy.deepcopy(w_init)</span><br><span class="line">    b = b_init</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        dw_j, db = compute_gradient(X, y, w, b)</span><br><span class="line">        w = w - alpha * dw_j</span><br><span class="line">        b = b - alpha * db</span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h2id="设置学习率计算回归函数和预测">设置学习率，计算回归函数和预测</h2><p>​预测时必须先使用之前计算出的均值和标准差把测试数据缩放再进行预测！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;houses.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    X_train = data[:,:<span class="number">4</span>]</span><br><span class="line">    Y_train = np.ravel(data[:,<span class="number">4</span>:<span class="number">5</span>]) <span class="comment"># 把Y转为一维数组</span></span><br><span class="line">    <span class="comment"># 将X训练集拿去标准化</span></span><br><span class="line">    X_train,mu,sigma = Zscore(X_train)</span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br><span class="line">    <span class="comment"># 设置学习率等</span></span><br><span class="line">    w_init = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    b_init = <span class="number">0</span></span><br><span class="line">    alpha = <span class="number">0.5</span></span><br><span class="line">    num_iters = <span class="number">10000</span></span><br><span class="line">    w,b = gradient_descent(X_train,Y_train,w_init,b_init,alpha,num_iters)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测出的w:<span class="subst">&#123;w&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测出的b:<span class="subst">&#123;b&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    X_forecast = [<span class="number">1200</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">40</span>]</span><br><span class="line">    X_forecast = (X_forecast-mu)/sigma</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;压缩后的测试数据<span class="subst">&#123;X_forecast&#125;</span>&quot;</span>)</span><br><span class="line">    X_predice_price = np.dot(X_forecast,w)+b</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;一个1200平米，3个卧室，1层楼，存在40年的房屋 = $<span class="subst">&#123;X_predice_price*<span class="number">1000</span>:<span class="number">0.0</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>​ 结果：</p><p><img src="https://img.issey.top/img/202209202210479.png" /></p><p>​ <del>就当那个世界的房子都这么便宜吧。</del></p><hr /><p>​ 关于线性回归模型的评估：</p><p>​常用的有均方误差和均方根误差等方法，这里留个空以后出篇文章，暂时放一放。</p><hr /><h1 id="补充学习率a的评估">补充：学习率a的评估</h1><p>​ 一般可以通过两个方面来评估：</p><p>​ 1.每次计算出的梯度，如果学习率合适，那么这个值应该在不断下降。</p><p>​2.梯度下降在整个代价函数上的”跳跃“变化，如果学习率合适，它应该是不断往下跳的。</p><p>​学习率过大：如果学习率过大，每次计算出的梯度会发生类似下面左图的情况。右图是学习率过大的情况下每次梯度下降在总成本函数上的变化，注意它是从下往上跳的，最后会导致结果发散。</p><p><img src="https://img.issey.top/img/202209202211389.png" /></p><p>​ 学习率过小：它虽然可以收敛，但是过程比较慢。</p><p><img src="https://img.issey.top/img/202209202211411.png" /></p><p>​ 学习率合适：</p><p><img src="https://img.issey.top/img/202209202211432.png" /></p><h1 id="补充浅谈正规方程">补充：浅谈正规方程</h1><p>​求解线性回归不止有梯度下降一种方法，其实还有另一种方法求解，即正规方程。不过在这里不讨论它的详细公式，只拿它与梯度下降做一个对比。</p><table><colgroup><col style="width: 35%" /><col style="width: 64%" /></colgroup><thead><tr class="header"><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr class="odd"><td>需要选择学习率</td><td>不需要选择学习率</td></tr><tr class="even"><td>需要多次迭代</td><td>一次计算得出</td></tr><tr class="odd"><td>当特征数量较多时也能较好使用</td><td>时间复杂度为<spanclass="math inline">\(O(n^3)\)</span>,当特征数量较多时太慢</td></tr><tr class="even"><td>也适用于除线性回归的其他模型</td><td>只适用于线性回归模型</td></tr></tbody></table><h1id="补充利用sklearn完成线性回归预测">补充：利用sklearn完成线性回归预测</h1><blockquote><p>关于StandardScaler()函数：<ahref="https://blog.csdn.net/qq_47175528/article/details/110480918">sklearn中StandardScaler()</a></p><p>关于SGDRegressor：随机梯度线性回归，随机梯度下降是不将所有样本都进行计算后再更新参数，而是选取一个样本，计算后就更新参数。</p><p>关于LinearRegression：也是线性回归模型，这里没用，可以自己查。</p></blockquote><p>​ 绘图模块的代码可能要自己改改。其余详细说明见代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(precision=<span class="number">2</span>) <span class="comment"># 设置numpy可见小数点</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">dlblue = <span class="string">&#x27;#0096ff&#x27;</span>; dlorange = <span class="string">&#x27;#FF9300&#x27;</span>; dldarkred=<span class="string">&#x27;#C00000&#x27;</span>; dlmagenta=<span class="string">&#x27;#FF40FF&#x27;</span>; dlpurple=<span class="string">&#x27;#7030A0&#x27;</span>;</span><br><span class="line">plt.style.use(<span class="string">&#x27;../deeplearning.mplstyle&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;houses.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    X_train = data[:, :<span class="number">4</span>]</span><br><span class="line">    X_features = [<span class="string">&#x27;size(sqft)&#x27;</span>,<span class="string">&#x27;bedrooms&#x27;</span>,<span class="string">&#x27;floors&#x27;</span>,<span class="string">&#x27;age&#x27;</span>]</span><br><span class="line">    y_train = np.ravel(data[:, <span class="number">4</span>:<span class="number">5</span>])  <span class="comment"># 把Y转为一维数组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用sklearn进行Z-score标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    X_norm = scaler.fit_transform(X_train) <span class="comment"># 标准化训练集X</span></span><br><span class="line">    <span class="comment"># print(X_norm)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建回归拟合模型</span></span><br><span class="line">    sgdr = SGDRegressor(max_iter=<span class="number">1000</span>) <span class="comment"># 设置最大迭代次数</span></span><br><span class="line">    sgdr.fit(X_norm, y_train)</span><br><span class="line">    <span class="built_in">print</span>(sgdr)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;完成的迭代次数: <span class="subst">&#123;sgdr.n_iter_&#125;</span>, 权重更新数: <span class="subst">&#123;sgdr.t_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看参数</span></span><br><span class="line">    b_norm = sgdr.intercept_</span><br><span class="line">    w_norm = sgdr.coef_</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;SGDRegressor拟合参数:       w: <span class="subst">&#123;w_norm&#125;</span>, b:<span class="subst">&#123;b_norm&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;之前自己编写的线性回归拟合参数: w: [110.61 -21.47 -32.66 -37.78], b: 362.23&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用sgdr.predict()进行预测</span></span><br><span class="line">    y_pred_sgd = sgdr.predict(X_norm)</span><br><span class="line">    <span class="comment"># 使用w和b进行预测</span></span><br><span class="line">    y_pred = np.dot(X_norm, w_norm) + b_norm</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;使用 np.dot() 预测值是否与sgdr.predict预测值相同: <span class="subst">&#123;(y_pred == y_pred_sgd).<span class="built_in">all</span>()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;训练集预测:\n<span class="subst">&#123;y_pred[:<span class="number">4</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;训练集真实值\n<span class="subst">&#123;y_train[:<span class="number">4</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制训练集与预测值匹配情况</span></span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">12</span>, <span class="number">3</span>), sharey=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ax)):</span><br><span class="line">        ax[i].scatter(X_train[:, i], y_train, label=<span class="string">&#x27;target&#x27;</span>)</span><br><span class="line">        ax[i].set_xlabel(X_features[i])</span><br><span class="line">        ax[i].scatter(X_train[:, i], y_pred, color=<span class="string">&#x27;orange&#x27;</span>, label=<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">    ax[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Price&quot;</span>)</span><br><span class="line">    ax[<span class="number">0</span>].legend()</span><br><span class="line">    fig.suptitle(<span class="string">&quot;target versus prediction using z-score normalized model&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>​ 结果：</p><p><img src="机器学习笔记2%20—%20多元线性回归\sklearn.png" /></p><p>​ 训练集预测值与真实值结果对比</p><p><img src="https://img.issey.top/img/202209202211375.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记1——一元线性回归</title>
      <link href="/article/c4e174e36609/"/>
      <url>/article/c4e174e36609/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：本系列为机器学习的学习笔记，参考教程链接：</p><p><ahref="https://www.bilibili.com/video/BV1Pa411X76s?p=9&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=747540861ba5c41c17852ccf069029f5">(强推|双字)2022吴恩达机器学习Deeplearning.ai课程</a></p><p>观点不一定完全正确，欢迎指出错误的地方。</p></blockquote><h2 id="什么是线性回归模型">什么是线性回归模型？</h2><p>回归分析是研究自变量与因变量之间数量变化关系的一种分析方法，它主要是通过因变量Y与影响它的自变量<spanclass="math inline">\(X_{i}\)</span>（i=1,2,3…）之间的<ahref="https://so.csdn.net/so/search?q=%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020">回归模型</a>，衡量自变量<spanclass="math inline">\(X_{i}\)</span>对因变量Y的影响能力的，进而可以用来预测因变量Y的发展趋势。线性回归模型指因变量和自变量呈直线型关系的模型，是回归分析中最常用且最简单的方法，线性归回模型又分为一元线性回归模型和多元回归模型。</p><hr /><h2 id="一元线性回归模型">一元线性回归模型</h2><p>一元线性回归模型即自变量只有一个的线</p><h3 id="问题引入">问题引入：</h3><p><img src="https://img.issey.top/img/202209191544874.png" /></p><p>已知上图数据集，其中，X为自变量，Y为因变量，请预测当X为5000时Y的取值。</p><h3 id="问题解析">问题解析：</h3><p>因为自变量只有一个，即让你模拟一个<spanclass="math inline">\(f_{w,b}(x)=wx+b\)</span>,使该函数与上图自变量与应变量的变化趋势尽量满足，<spanclass="math inline">\(f_{w,b}(x)\)</span>即一元线性回归函数，再用计算出的回归函数去预测值即可。难点在于，这里的w和b都是未知数，我们要做的就是推断出最合适的w和b。</p><h3 id="代价函数损失函数">代价函数（损失函数）：</h3><p>如何判断w和b是否合适，我们引入了代价函数。代价函数用于判断<strong>整体来看</strong>，每个点（Y）实际值与估计值的差距大小。</p><p>这里先随便画一条线。</p><p><img src="https://img.issey.top/img/202209191550757.png" /></p><p>令模拟出来的自变量对应应变量的值为<spanclass="math inline">\(\hat{y}\)</span>,即<spanclass="math inline">\(\hat{y} = f_{w,b}(x)\)</span>,则代价函数为：</p><p><span class="math inline">\(J(w,b)=\frac{1}{2m}\sum_{i=0}^{m-1}{(\hat{y}_{i}-y_{i})^2} =\frac{1}{2m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})^2}\)</span></p><p>其中，m为训练集样例数，第一个点下标为0。这里除以2是方便后续计算。</p><h4 id="代价函数的图像">代价函数的图像</h4><p>我们先将<span class="math inline">\(f_{w,b}(x)\)</span>简化为<spanclass="math inline">\(f_{w}(x) = wx\)</span>,那么<spanclass="math inline">\(J(w) =\frac{1}{2m}\sum_{i=0}^{m-1}{(wx_{i}-y_{i})^2}\)</span></p><p>此时<span class="math inline">\(J(w)\)</span>的图像为一个凸函数</p><p><img src="https://img.issey.top/img/202209191551748.png" /></p><p>对应的<span class="math inline">\(f_w(x)\)</span>模拟情况：</p><p><img src="https://img.issey.top/img/202209191553611.png" /></p><p>当我们将<span class="math inline">\(f_{w,b}(x)\)</span>简化为<spanclass="math inline">\(f_{b}(x) = x+b\)</span>,此时<spanclass="math inline">\(J(b)\)</span>的图像也是一个凸函数，我们姑且借用<spanclass="math inline">\(J(w)\)</span>的图像，不过变量变为了b:</p><p><img src="https://img.issey.top/img/202209191552365.png" /></p><p>对应的<span class="math inline">\(f_{b}(x)\)</span>模拟情况：</p><p><img src="https://img.issey.top/img/202209191554982.png" /></p><p>现在将<span class="math inline">\(J(w)\)</span>和<spanclass="math inline">\(J(b)\)</span>合在一起，<spanclass="math inline">\(J(w,b)\)</span>便是一个三维碗装图像：</p><p>注：图中的w和b并不对应上面的例子，只是大致图像！</p><p><img src="https://img.issey.top/img/202209191555588.png" /></p><p><u>代价函数计算出的值越小，说明模拟值与实际值差距越小，则w，b越合适，回归函数模拟的越好。所以，当代价函数值最小时，w和b最合适。</u></p><p>于是问题转化为了：求w和b使得<spanclass="math inline">\(J(w,b)\)</span>能取到极小值。</p><h4 id="为什么不是最小而是极小值">为什么不是最小而是极小值？</h4><p>这与之后要用到的算法（梯度下降法）有关，梯度下降法只能求到极小值。不过梯度下降法常用于求凸函数的极小值，而凸函数只有一个极小值，所以通常求得的是最小值。这里举个非凸函数的例子，此时用梯度下降法不一定能求得最优解。</p><p><img src="https://img.issey.top/img/202209191556964.png" /></p><h3 id="梯度下降算法">梯度下降算法</h3><blockquote><p>梯度下降算法并不只用于求解线性回归问题。</p></blockquote><p>梯度算法在讲座中被描述为：假设你站在一个山坡上，你想最快下降到你四周最低的山谷。</p><p>即选择一个基点，以四周斜率绝对值最大的方向下降，直到下降到极小值点（此时斜率为0）停止。我们认为这个极小值点对应的w和b即为所求，一般我们选择<spanclass="math inline">\((0,0)\)</span>作为基点，即w和b开始为<spanclass="math inline">\((0,0)\)</span>，不过实际上基点怎么选都可以。</p><h4id="梯度下降算法公式对于一元线性回归模型">梯度下降算法公式（对于一元线性回归模型）：</h4><p>重复以下行为直到收敛：</p><p><span class="math inline">\(w = w - a\frac{\partial J(w,b)}{\partialw}\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(w,b)}{\partialb}\)</span></p><p>其中，<spanclass="math inline">\(a\)</span>被称为学习率。之后会讨论学习率<spanclass="math inline">\(a\)</span>的选择。</p><p>注意：w和b应该同时更新！（会在代码块详细说明）</p><p><span class="math inline">\(\frac{\partial J(w,b)}{\partial w} =\frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}x_{i}\)</span></p><p><span class="math inline">\(\frac{\partial J(w,b)}{\partial w} =\frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}\)</span></p><p>(之前代价函数除个2就是为了这里化简)</p><h4 id="学习率a的选择">学习率a的选择</h4><p>如果<spanclass="math inline">\(a\)</span>很小，那么每一步都走的很小，收敛过程就会很慢。</p><p><img src="https://img.issey.top/img/202209191556779.png" /></p><p>如果<span class="math inline">\(a\)</span>很大，<spanclass="math inline">\(J(w,b)\)</span>可能不会每次迭代都下降，可能错过最佳点，甚至导致发散。</p><p><img src="https://img.issey.top/img/202209191557675.png" /></p><p>关于学习率的设置有许多种方法，这里不做专门讨论<del>（其实是还没学到）</del>，姑且采用网上查到的一种简单的方法：在运行梯度下降法的时候会尝试一系列学习率的取值：...0.001,0.003，0.01, 0.03，0.1,0.3，1....尽量以三倍增长，直到找到一个合适的学习率。</p><h4 id="关于梯度下降每一步的变化">关于梯度下降每一步的变化</h4><p>梯度下降每一步并不是相等的，因为每一次迭代时，偏导数都会不断变化。在学习率选择合适的情况下，大概可以得到以下的每一步梯度变化图像。x轴为迭代次数，y轴为梯度。</p><p><img src="https://img.issey.top/img/202209191557819.png" /></p><p><img src="https://img.issey.top/img/202209191557852.png" /></p><p>可以看到最开始梯度很大，到后来慢慢接近于0。</p><h4 id="补充">补充：</h4><p>这里解释下为什么非凸函数中找到的不一定是最优解：</p><p><img src="https://img.issey.top/img/202209191558036.png" /></p><p>我们选择1和2分别作为起点，可能到达两个极小值点，我们无法判断找到的极小值点是否是全局最小值。当然凸函数只有一个极值点，所以对于凸函数，不存在这个问题。</p><h3 id="代码部分---案例实现">代码部分 - 案例实现</h3><h4 id="数据">数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2104.000000</span>,<span class="number">1600.000000</span>,<span class="number">2400.000000</span>,<span class="number">1416.000000</span>,<span class="number">3000.000000</span>,<span class="number">1985.000000</span>,<span class="number">1534.000000</span>,<span class="number">1427.000000</span>,<span class="number">1380.000000</span>,<span class="number">1494.000000</span>,<span class="number">1940.000000</span>,<span class="number">2000.000000</span>,<span class="number">1890.000000</span>,<span class="number">4478.000000</span>,<span class="number">1268.000000</span>,<span class="number">2300.000000</span>,<span class="number">1320.000000</span>,<span class="number">1236.000000</span>,<span class="number">2609.000000</span>,<span class="number">3031.000000</span>,<span class="number">1767.000000</span>,<span class="number">1888.000000</span>,<span class="number">1604.000000</span>,<span class="number">1962.000000</span>,<span class="number">3890.000000</span>,<span class="number">1100.000000</span>,<span class="number">1458.000000</span>,<span class="number">2526.000000</span>,<span class="number">2200.000000</span>,<span class="number">2637.000000</span>,<span class="number">1839.000000</span>,<span class="number">1000.000000</span>,<span class="number">2040.000000</span>,<span class="number">3137.000000</span>,<span class="number">1811.000000</span>,<span class="number">1437.000000</span>,<span class="number">1239.000000</span>,<span class="number">2132.000000</span>,<span class="number">4215.000000</span>,<span class="number">2162.000000</span>,<span class="number">1664.000000</span>,<span class="number">2238.000000</span>,<span class="number">2567.000000</span>,<span class="number">1200.000000</span>,<span class="number">852.000000</span>,<span class="number">1852.000000</span>,<span class="number">1203.000000</span></span><br><span class="line"><span class="number">399.899994</span>,<span class="number">329.899994</span>,<span class="number">369.000000</span>,<span class="number">232.000000</span>,<span class="number">539.900024</span>,<span class="number">299.899994</span>,<span class="number">314.899994</span>,<span class="number">198.998993</span>,<span class="number">212.000000</span>,<span class="number">242.500000</span>,<span class="number">239.998993</span>,<span class="number">347.000000</span>,<span class="number">329.998993</span>,<span class="number">699.900024</span>,<span class="number">259.899994</span>,<span class="number">449.899994</span>,<span class="number">299.899994</span>,<span class="number">199.899994</span>,<span class="number">499.997986</span>,<span class="number">599.000000</span>,<span class="number">252.899994</span>,<span class="number">255.000000</span>,<span class="number">242.899994</span>,<span class="number">259.899994</span>,<span class="number">573.900024</span>,<span class="number">249.899994</span>,<span class="number">464.500000</span>,<span class="number">469.000000</span>,<span class="number">475.000000</span>,<span class="number">299.899994</span>,<span class="number">349.899994</span>,<span class="number">169.899994</span>,<span class="number">314.899994</span>,<span class="number">579.900024</span>,<span class="number">285.899994</span>,<span class="number">249.899994</span>,<span class="number">229.899994</span>,<span class="number">345.000000</span>,<span class="number">549.000000</span>,<span class="number">287.000000</span>,<span class="number">368.500000</span>,<span class="number">329.899994</span>,<span class="number">314.000000</span>,<span class="number">299.000000</span>,<span class="number">179.899994</span>,<span class="number">299.899994</span>,<span class="number">239.500000</span></span><br></pre></td></tr></table></figure><h4 id="导入数据并绘制初始图">导入数据并绘制初始图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(<span class="string">&#x27;test.txt&#x27;</span>,dtype=np.float32,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">x_train = data[<span class="number">0</span>]</span><br><span class="line">y_train = data[<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_train,y_train,marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;r&#x27;</span>) <span class="comment"># marker 将样式设置为叉，c将颜色设置为红色</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="梯度产生函数">梯度产生函数</h4><p>对应公式：</p><p>sum_dw = <span class="math inline">\(\frac{\partial J(w,b)}{\partialw} = \frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}x_i\)</span></p><p>sum_db = <span class="math inline">\(\frac{\partial J(w,b)}{\partialw} = \frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 产生梯度函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient</span>(<span class="params">x,y,w,b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        x: x训练集</span></span><br><span class="line"><span class="string">        y: y训练集</span></span><br><span class="line"><span class="string">        w,b: 模型参数</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">        sum_dw: 代价函数对w的偏导数</span></span><br><span class="line"><span class="string">        sum_db: 代价函数对d的偏导数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 训练样例个数</span></span><br><span class="line">    sum_dw = <span class="number">0</span></span><br><span class="line">    sum_db = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb = w*x[i]+b</span><br><span class="line">        dw_i = (f_wb - y[i])*x[i]</span><br><span class="line">        db_i = f_wb - y[i]</span><br><span class="line">        sum_dw += dw_i</span><br><span class="line">        sum_db += db_i</span><br><span class="line"></span><br><span class="line">    sum_dw = sum_dw / m</span><br><span class="line">    sum_db = sum_db / m</span><br><span class="line">    <span class="keyword">return</span> sum_dw,sum_db</span><br></pre></td></tr></table></figure><h4 id="梯度迭代函数">梯度迭代函数</h4><p>对应公式：</p><p>重复以下行为直到收敛：</p><p><span class="math inline">\(w = w - a\frac{\partial J(w,b)}{\partialw}\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(w,b)}{\partialb}\)</span></p><p>注：代码中是让他迭代一定次数而并非以收敛为结束判断条件。这是因为当迭代次数足够大，也无限接近收敛了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度迭代函数(计算w和b)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x,y,init_w,init_b,alpha,num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数说明:</span></span><br><span class="line"><span class="string">        x: x训练集</span></span><br><span class="line"><span class="string">        y: y训练集</span></span><br><span class="line"><span class="string">        init_w: w初始值</span></span><br><span class="line"><span class="string">        init_b: b初始值</span></span><br><span class="line"><span class="string">        alpha: 学习率</span></span><br><span class="line"><span class="string">        num_iters: 迭代次数</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w,b:最终找到的w和b</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    w = init_w</span><br><span class="line">    b = init_b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># 产生梯度</span></span><br><span class="line">        sum_dw,sum_db = compute_gradient(x, y, w, b)</span><br><span class="line">        <span class="comment"># 同时更新w和b</span></span><br><span class="line">        w = w - alpha*sum_dw</span><br><span class="line">        b = b - alpha*sum_db</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h4 id="代价函数">代价函数</h4><p>对应公式：</p><p><span class="math inline">\(J(w,b)=\frac{1}{2m}\sum_{i=0}^{m-1}{(\hat{y}*{i}-y*{i})^2} =\frac{1}{2m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})^2}\)</span></p><p>这里只用于检验结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">x, y, w, b</span>):</span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb = w * x[i] + b</span><br><span class="line">        cost = cost + (f_wb - y[i]) ** <span class="number">2</span></span><br><span class="line">    total_cost = <span class="number">1</span> / (<span class="number">2</span> * m) * cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><h4 id="绘图和预测">绘图和预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;test.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    x_train = data[<span class="number">0</span>]</span><br><span class="line">    y_train = data[<span class="number">1</span>]</span><br><span class="line">    plt.scatter(x_train, y_train, marker=<span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;r&#x27;</span>)  <span class="comment"># marker 将样式设置为叉，c将颜色设置为红色</span></span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    init_m = <span class="number">0</span></span><br><span class="line">    init_b = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 一些梯度下降的设置</span></span><br><span class="line">    iterations = <span class="number">100000</span></span><br><span class="line">    tmp_alpha = <span class="number">0.000000095</span></span><br><span class="line">    w,b = gradient_descent(x_train,y_train,init_m,init_b,tmp_alpha,iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;线性回归函数为:f(x) = <span class="subst">&#123;w&#125;</span>x + <span class="subst">&#123;b&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;此时代价函数为:<span class="subst">&#123;compute_cost(x_train,y_train,w,b)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测当x = 5000是，y的值为:<span class="subst">&#123;w*<span class="number">5000</span>+b&#125;</span>&quot;</span>)</span><br><span class="line">    x = np.linspace(<span class="number">0</span>,<span class="number">5000</span>,<span class="number">100</span>)</span><br><span class="line">    y = w*x+b</span><br><span class="line">    plt.plot(x,y)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>在设置学习率alpha时，如果大了会报错，过小模拟出来的图像差距过大，这里尝试了许多次选了一个自认为比较合适的值。</p><h4 id="结果">结果</h4><p><img src="https://img.issey.top/img/202209191559138.png" /></p><p><img src="https://img.issey.top/img/202209191600322.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PageHelper返回给前端的数据示例以及参数说明</title>
      <link href="/article/da43cda93935/"/>
      <url>/article/da43cda93935/</url>
      
        <content type="html"><![CDATA[<h1id="pagehelper返回给前端的数据示例以及参数说明">PageHelper返回给前端的数据示例以及参数说明</h1><h2id="请求成功后前端拿到的response-body">请求成功后，前端拿到的Responsebody：</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;total&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span>    <span class="comment">// 所有数据条数</span></span><br><span class="line">  <span class="attr">&quot;list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>        <span class="comment">// 数据列表</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;pageNum&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>        <span class="comment">// 当前页码</span></span><br><span class="line">  <span class="attr">&quot;pageSize&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span>    <span class="comment">// 每页多少条数据</span></span><br><span class="line">  <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span>        <span class="comment">// 当前页有多少条数据，因为总数据只有7条，这是最后一页，少了一条</span></span><br><span class="line">  <span class="attr">&quot;startRow&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span>    <span class="comment">// 数据起始行，指本页面第一条数据在数据库中的行数</span></span><br><span class="line">  <span class="attr">&quot;endRow&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span>        <span class="comment">// 数据末行，指本页面最后一条数据在数据库中的行数</span></span><br><span class="line">  <span class="attr">&quot;pages&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>        <span class="comment">// 总页数</span></span><br><span class="line">  <span class="attr">&quot;prePage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span>        <span class="comment">// 前一页，第一页的prePage = 0</span></span><br><span class="line">  <span class="attr">&quot;nextPage&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span>    <span class="comment">// 后一页，最后一页的nextPage = 0</span></span><br><span class="line">  <span class="attr">&quot;isFirstPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>        <span class="comment">// 是否是第一页</span></span><br><span class="line">  <span class="attr">&quot;isLastPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span>    <span class="comment">// 是否是最后一页</span></span><br><span class="line">  <span class="attr">&quot;hasPreviousPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span>    <span class="comment">// 有前一页吗</span></span><br><span class="line">  <span class="attr">&quot;hasNextPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>        <span class="comment">// 有后一页吗</span></span><br><span class="line">  <span class="attr">&quot;navigatePages&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span>    <span class="comment">// 每页显示的页码个数</span></span><br><span class="line">  <span class="attr">&quot;navigatepageNums&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>        <span class="comment">// 显示的页码数（大概是这意思，没试过，应该是配合上一个属性用的）</span></span><br><span class="line">    <span class="number">1</span><span class="punctuation">,</span>    </span><br><span class="line">    <span class="number">2</span>    </span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;navigateFirstPage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span>    <span class="comment">// 起始页码</span></span><br><span class="line">  <span class="attr">&quot;navigateLastPage&quot;</span><span class="punctuation">:</span> <span class="number">2</span>        <span class="comment">// 尾页码</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
