<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于hive的启动和连接</title>
      <link href="/article/a54e99ba2904/"/>
      <url>/article/a54e99ba2904/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>​太久没用hive了，今天想重新熟悉一下，结果发现自己甚至忘记了怎么启动。。于是特此记录篇笔记，便于以后忘记时查阅，不会写的太细。</p><p>​顺便从这篇文章开始改变自己文章的格式，以前都是乱整，想稍微更好看一点。</p><h1 id="hive的启动与连接">hive的启动与连接</h1><h2 id="启动hadoop">启动hadoop</h2><p>​以root权限登录hadoop中心节点计算机（亲测用户登录不能启动hadoop），使用以下命令开启hadoop集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>​ 可以通过以下代码查看是否成功启动hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>​ 输出长这样：</p><p><img src="https://img.issey.top/img/202209202106612.png" /></p><p>​这时候就可以访问hadoop网页了：http://ip:9870，ip为你hadoop中心节点计算机ip。</p><p>​ 顺便提一下yarn的默认端口：8088</p><h2 id="启动hive">启动hive</h2><p>​这一步与许多教程不一样，可能是版本原因。root登录你安装hive的计算机，我的就在hadoop中心计算机上，在没有配置环境变量的情况下，进入hive安装目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/hive-3.1.2/</span><br></pre></td></tr></table></figure><p>​ 然后启动hivemetastore服务，这一步可以后台启动也可以前台启动。我现在使用前台启动，因为可以看到日志。使用前台启动之后这个命令框就不能动了。接下来开另一个命令框，如果不想再开命令框，可以选择后台启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前台启动：bin/hive --service metastore</span><br><span class="line">后台启动：<span class="built_in">nohup</span> bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>​在启动metastore服务后，同样在hive安装目录下接着启动hiveserver2服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前台启动：bin/hive --service hiveserver2</span><br><span class="line">后台启动：<span class="built_in">nohup</span> bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><p>​ 如果是前台启动，成功后会看到Hive sessionID过十几秒会变一个。到此hive服务完全启动。接下来就是hive连接了。关于hive连接没啥好注意的，和mysql连接差不多。不过注意hive默认是不需要密码的。所以账号输root，密码填空就行。测试链接，如果没问题的话hiveserver2那边的命令框会跳出"OK"。</p><h2 id="一些注意事项">一些注意事项</h2><p>​今天在启动hive服务时出现了连接不上的情况，经过分析发现是因为我前台启动然后ctrl+z（因为用的Xshell所以是ctrl+z,等同ctrl+c）后程序并没有被完全杀死。然后我又开了一个后台启动，就出错了。所以退出服务后一定要检查jps，如果程序还在一定要先kill-9。</p><p>​正常情况下，开启了metastore和hiveserver2后jps显示的只有两个Runjar。如果不是两个说明可能开多了。</p><p><img src="https://img.issey.top/img/202209202128688.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记11——高斯混合模型原理与推导</title>
      <link href="/article/3f1c8d74d0f5/"/>
      <url>/article/3f1c8d74d0f5/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ol type="1"><li><ahref="https://blog.csdn.net/qq_52466006/article/details/126786732">EM算法——直观理解与详细推导_TwilightSparkle.的博客-CSDN博客</a></li></ol><hr /><blockquote><p>    前言：在上一篇文章中，我们详细推导了EM算法的原理和过程，在最后，我们还剩下EM算法的具体应用。文章链接：<ahref="https://blog.csdn.net/qq_52466006/article/details/126786732">EM算法——直观理解与详细推导_TwilightSparkle.的博客-CSDN博客</a></p><p>    这篇文章就不对EM算法的流程和公式做过多赘述，详细请见上篇文章。在本篇文章中，将会利用EM算法处理高斯混合模型（GMM）。</p><p>    GMM学习参考链接：<ahref="https://www.bilibili.com/video/BV1aE411o7qd?p=69&amp;vd_source=747540861ba5c41c17852ccf069029f5">(系列十一)高斯混合模型4-EM求解-M-Step_哔哩哔哩_bilibili</a></p></blockquote><p>    高斯混合模型（<strong>G</strong>aussian <strong>M</strong>ixed<strong>M</strong>odel）指多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布。高斯混合模型通常用于解决同一集合下的数据包含多个不同的分布的情况，即<strong>聚类</strong>。</p><h1 id="高斯混合模型与k-mean">高斯混合模型与K-mean</h1><p>    根据K-mean聚类算法的原理，K-mean算法的缺点之一在于无法将两个聚类中心点相同的类进行聚类，比如<spanclass="math inline">\(A\sim N(\mu,\sigma_1^2),B\simN(\mu,\sigma^2_2)\)</span>,此时将无法用K-mean算法聚类出A，B。为了解决这一缺点，提出了高斯混合模型（GMM）。GMM通过选择成分最大化后验概率完成聚类，各数据点的后验概率表示属于各类的可能性，而不是判定它完全属于某个类，所以称为<strong>软聚类</strong>。其在各类尺寸不同、聚类间有相关关系的时候可能比k-means聚类更合适。</p><h1 id="gmm的概率密度函数">GMM的概率密度函数</h1><h2 id="从几何角度">从几何角度</h2><p><img src="https://img.issey.top/img/202209191338651.jpg" /></p><p>    设有随机变量<strong>X</strong>，则单高斯分布可以写为：</p><p>    <span class="math inline">\(X\sim N(\mu,\Sigma)\)</span></p><p>    像图示的这种高斯混合分布（红线），每个样本可以看作是<strong>几个高斯分布加权平均叠加</strong>而成。于是高斯混合模型的概率密度函数为：</p><p>    <span class="math inline">\(p(x)=\sum_{k=1}^K\alpha_kN(x|\mu_k,\Sigma_k)\)</span></p><ul><li><p>K：k个高斯分布</p></li><li><p><spanclass="math inline">\(\alpha_k\)</span>：权值，样本x属于第k个高斯分布的概率。对于一个样本x,有<spanclass="math inline">\(\sum_{k=1}^Ka_k = 1\)</span>。</p></li><li><p><spanclass="math inline">\(N(x|\mu_k,\Sigma_K)\)</span>：在第k个高斯分布条件下发生x的概率。其中，<spanclass="math inline">\(\mu\)</span>为均值，<spanclass="math inline">\(\Sigma\)</span>为协方差矩阵。</p></li></ul><h2 id="从混合模型角度">从混合模型角度</h2><p>    首先需要引入隐变量<spanclass="math inline">\(z\)</span>,隐变量在EM算法时已经说过，这里就不再说明隐变量的定义。只是再提一下，<strong>每个样本都有自己的隐变量</strong>。</p><p>    隐变量<span class="math inline">\(z\)</span>的概率分布：<spanclass="math inline">\(z\)</span>为离散随机变量，<spanclass="math inline">\(z\)</span>的概率分布为对应的样本x属于每一个高斯分布的概率分布。</p><table><thead><tr class="header"><th><span class="math inline">\(z\)</span></th><th>1</th><th>2</th><th>...</th><th>k</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(P(z)\)</span></td><td><span class="math inline">\(p_1\)</span></td><td><span class="math inline">\(p_2\)</span></td><td>...</td><td><span class="math inline">\(p_k\)</span></td></tr></tbody></table><p>    <span class="math inline">\(p_k\)</span>表示样本x属于第k个高斯分布的概率，有<spanclass="math inline">\(\sum_{k=1}^KP_k = 1\)</span> 。</p><p>    为了便于理解，之后也会使用<spanclass="math inline">\(p(z=c_k)\)</span> 的形式来表示<spanclass="math inline">\(p_k\)</span> 。其中，<spanclass="math inline">\(c_k\)</span>为第k类高斯分布。</p><p>    在引入隐变量<spanclass="math inline">\(z\)</span>后，x的密度函数将变为：</p><p><span class="math display">\[\begin{split}p(x)&amp; = \sum_Zp(x,z) =\sum_{k=1}^Kp(x,z=c_k)\\&amp;=\sum_{k=1}^Kp(z=c_k)*p(x|z=c_k)\end{split}\]</span></p><p>    我们令<span class="math inline">\(p_k\)</span>为<spanclass="math inline">\(\alpha_k\)</span>,因为<spanclass="math inline">\(p(z=c_k)\)</span>就是<spanclass="math inline">\(p_k\)</span>,所以<spanclass="math inline">\(p(z=c_k)\)</span> 可表示为<spanclass="math inline">\(\alpha_k\)</span> ;</p><p>    而<span class="math inline">\(p(x|z=c_k)\)</span>就是<spanclass="math inline">\(N(x|\mu_k,\Sigma_k)\)</span> 。于是：</p><p>    <span class="math inline">\(p(x) = \sum_{k=1}^K\alpha_kN(x|\mu_k,\Sigma_k)\)</span></p><p>    这与从几何角度得出的概率密度公式相同。</p><h1 id="极大似然估计">极大似然估计</h1><p>    现在我们要用已知样本估计k个高斯分布的参数，一般通过样本估计模型参数的方法为极大似然估计（MLE），MLE在EM算法中已经讲过。</p><p>    定义变量和参数：</p><ul><li><p>x：随机变量，<spanclass="math inline">\(x_i\)</span>表示第i个样本。</p></li><li><p>z：隐变量，<span class="math inline">\(z^{(i)}\)</span>表示第i个样本的隐变量。</p></li><li><p>X：可观测数据，<span class="math inline">\(X =\{x_1,x_2,...,x_n\}\)</span></p></li><li><p>Z：不可观测数据，<span class="math inline">\(Z =\{z^{(1)},z^{(2)},...,z^{(n)}\}\)</span></p></li><li><p><span class="math inline">\(\theta\)</span>：模型参数，<spanclass="math inline">\(\theta =\{\alpha_1,\alpha_2,...,\alpha_k;\mu_1,\mu_2,...\mu_k;\Sigma_1,\Sigma_2,...,\Sigma_k\}\)</span></p></li></ul><p>回顾MLE目标函数:</p><p><span class="math display">\[\begin{split}\hat\theta&amp; = argmax_\theta  logP(X|\theta) \\ &amp;=argmax_\theta  log \prod_{i=1}^np(x_i|\theta)\\ &amp;=argmax_\theta  \sum_{i=1}^n logp(x_i|\theta)\end{split}\]</span></p><p>    将高斯混合模型的概率密度函数代入，得：</p><p><span class="math display">\[\begin{split}\hat\theta &amp;= argmax_\theta \sum_{i=1}^nlogp(x_i|\theta)\\&amp;=argmax_\theta\sum_{i=1}^nlog\sum_{k=1}^K\alpha_kN(x_i|\mu_k,\Sigma_k)\end{split}\]</span></p><p>    注：<span class="math inline">\(\alpha_k\)</span> 可以写为<spanclass="math inline">\(p_k\)</span> ，具体含义在上一节讲过。</p><p>    因为引入了隐变量，导致这个式子含有<spanclass="math inline">\(log\sum\)</span>，无法再进行MLE下一个步骤。回顾EM算法，EM算法就是拿来求解此类问题的。于是接下来需要用EM迭代求近似解。</p><h1 id="em算法求近似解">EM算法求近似解</h1><h2 id="e-step-简化q函数">E-step 简化Q函数</h2><blockquote><p>关于E-step具体要计算什么需要推导完M-step再回来说明。</p></blockquote><p>    回顾EM算法，目标函数为<span class="math inline">\(\hat\theta=argmax_\theta Q(\theta,\theta^{(t)})\)</span>。</p><p>    Q函数公式：</p><p><span class="math display">\[\begin{split}Q(\theta,\theta^{(t)})&amp; =E_z[logP(X,Z|\theta)|X,\theta^{(t)}] \\&amp;=\sum_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})\end{split}\]</span></p><p>    因为高斯混合模型中的完整数据<spanclass="math inline">\((X,Z)\)</span> 独立同分布，未观测数据<spanclass="math inline">\(Z\)</span> 独立同分布，所以：</p><p><span class="math display">\[\begin{split}Q(\theta,\theta^{(t)}) &amp;=\sum_Zlog \prod_{i=1}^np(x_i,z^{(i)}|\theta)\prod _{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_Z[ \sum_{i=1}^nlog p(x_i,z^{(i)}|\theta) ]\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\end{split}\]</span></p><p>    注：$_Z $ 是<spanclass="math inline">\(\sum_{z^{(1)},z^{(2)},...,z^{(n)}}\)</span>的简写。</p><blockquote><p>关于最前面那个<span class="math inline">\(\sum_Z\)</span>的解释：</p><p>如果是是连续型函数，Q的表达式应该是：</p><p><span class="math inline">\(Q(\theta,\theta^{(t)}) =\int_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})dz\)</span></p><p>但是现在是离散型，所以积分就变成了求和。</p></blockquote><h3 id="简化q函数">简化Q函数</h3><p><strong>展开Q函数：</strong><spanclass="math inline">\(Q(\theta,\theta^{(t)}) =\sum_Z[ logp(x_1,z^{(1)}|\theta)+logp(x_2,z^{(2)}|\theta)+...+logp(x_n,z^{(n)}|\theta) ]\prod _{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\)</span></p><p><strong>只看第一项：</strong></p><p><span class="math inline">\(\sum_Zlogp(x_1,z^{(1)}|\theta)*\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\)</span></p><blockquote><p>为什么要带上<span class="math inline">\(\sum_Z\)</span>:一样先看成积分再变成离散的形式就好理解了。</p></blockquote><p>因为<span class="math inline">\(logp(x_1,z^{(1)}|\theta)\)</span>只与<span class="math inline">\(z^{(1)}\)</span> 相关，而<spanclass="math inline">\(\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\)</span>中，<spanclass="math inline">\(p(z^{(1)}|x_i,\theta^{(t)})\)</span> 与<spanclass="math inline">\(z^{(1)}\)</span>相关，所以可以将上式改写为：</p><p><span class="math display">\[\begin{split}&amp;\sum_Zlogp(x_1,z^{(1)}|\theta)*\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{z^{(1)}}logp(x_1,z^{(1)}|\theta)*p(z^{(1)}|x_1,\theta^{(t)})*[\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)})]\end{split}\]</span></p><blockquote><p>然后对于<span class="math inline">\(\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)})\)</span> ,实际上它等于1：</p><p>如同<span class="math inline">\(z^{(1)}\)</span> 一样，<spanclass="math inline">\(p(z^{(i)}|x_i,\theta^{(t)})\)</span> 只与<spanclass="math inline">\(z^{(i)}\)</span> 相关，所以上式展开将变为：</p><p><span class="math display">\[\begin{split}&amp;\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)}) \\&amp; =\sum_{z^{(2)},...z^{(n)}}p(z^{(2)}|x_2,\theta^{(t)})*p(z^{(3)}|x_3,\theta^{(t)})*...*p(z^{(n)}|x_n,\theta^{(t)})\\&amp;=\sum_{z^{(2)}}p(z^{(2)}|x_2,\theta^{(t)})*\sum_{z^{(3)}}p(z^{(3)}|x_3,\theta^{(t)})*...*\sum_{z^{(n)}}p(z^{(n)}|x_n,\theta^{(t)})\end{split}\]</span></p><p>而<span class="math inline">\(\sum_{z^{(i)}}p(z^{(i)}|x_i)=1\)</span> ,所以全部都可以约为1。</p><p>于是<span class="math inline">\(\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)}) = 1\)</span></p></blockquote><p>所以第一项：</p><p><span class="math display">\[\begin{split}&amp;\sum_Zlogp(x_1,z^{(1)}|\theta)\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)}) \\&amp;=\sum_{z^{(1)}}logp(x_1,z^{(1)}|\theta)p(z^{(1)}|x_1,\theta^{(t)})\end{split}\]</span></p><p><strong>再看整体：</strong></p><p>有了第一项的结论，推广到整体</p><p><span class="math display">\[\begin{split}&amp;\sum_Z[ logp(x_1,z^{(1)}|\theta)+logp(x_2,z^{(2)}|\theta)+...+logp(x_n,z^{(n)}|\theta) ]\prod _{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\ &amp;=\sum_{z^{(1)}}logp(x_1,z^{(1)}|\theta)p(z^{(1)}|x_1,\theta^{(t)})+...+\sum_{z^{(n)}}logp(x_n,z^{(n)}|\theta)p(z^{(n)}|x_n,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z^{(i)}}logp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\end{split}\]</span></p><p><strong>结论：</strong></p><p>通过简化后，</p><p><span class="math display">\[\begin{split}&amp;Q(\theta,\theta^{(t)}) \\&amp;= \sum_Zlog\prod_{i=1}^n p(x_i,z^{(i)}|\theta)\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z^{(i)}}logp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\end{split}\]</span></p><h3 id="做一些替换">做一些替换</h3><blockquote><p>在说明高斯混合模型的概率密度函数时，我们有</p><p><span class="math inline">\(p(x) = \sum_Zp(x,z) = \sum_{k=1}^K\alpha_kN(x|\mu_k,\Sigma_k)\)</span> ,</p><p>这里<spanclass="math inline">\(\alpha_k\)</span>代表样本x属于第k个高斯分布的概率。也可以描述为样本x的隐变量z的<spanclass="math inline">\(p_k\)</span>。</p></blockquote><p><span class="math inline">\(p(x_i,z_j^{(i)}|\theta)\)</span>说明：</p><ul><li><p><span class="math inline">\(x_i\)</span> :第i个样本</p></li><li><p><spanclass="math inline">\(z^{(i)}\)</span>:第i个样本的隐变量，前面说过每个变量都有自己的隐变量。</p></li><li><p><spanclass="math inline">\(z^{(i)}_j\)</span>:第i个样本的隐变量的第j个分类，于是<spanclass="math inline">\(P(z^{(i)}_j)\)</span>表示第i个样本属于第j个高斯分布的概率。<spanclass="math inline">\(P(z^{(i)}_j)\)</span> 又可表示为<spanclass="math inline">\(\alpha^{(i)}_j\)</span></p></li><li><p><span class="math inline">\(\theta = \{\alpha,\mu,\Sigma\}\)</span></p></li></ul><p><strong>对<spanclass="math inline">\(p(x_i,z_j^{(i)}|\theta)\)</span>展开:</strong></p><p>    <span class="math inline">\(p(x_i,z_j^{(i)}|\theta) =\alpha_j^{(i)}N(x|\mu_j,\Sigma_j)\)</span></p><p><strong>对<spanclass="math inline">\(p(z_j^{(i)}|x_i,\theta^{(t)})\)</span>展开:</strong></p><blockquote><p><span class="math inline">\(P(B|A) =\frac{P(A,B)}{P(A)}\)</span>   </p></blockquote><p><span class="math display">\[\begin{split}p(z_j^{(i)}|x_i,\theta^{(t)}) &amp;=\frac{p(x_i,z_j^{(i)}|\theta^{(t)})}{p(x_i)}\\&amp;=\frac{\alpha_j^{(i),(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}{\sum_{k=1}^K\alpha_k^{(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}\end{split}\]</span></p><p>于是：</p><p><span class="math display">\[\begin{split}&amp;Q(\theta,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z^{(i)}}logp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{z^{(i)}}\sum_{i=1}^nlogp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{k=1}^K\sum_{i=1}^nlog[\alpha_kN(x_i|\mu_k,\Sigma_k)]p(z^{(i)}=c_k|x_i,\theta^{(t)})\\&amp;=\sum_{k=1}^K\sum_{i=1}^n[log\alpha_k+logN(x_i|\mu_k,\Sigma_k)]p(z^{(i)}=c_k|x_i,\theta^{(t)})\end{split}\]</span></p><p>为了后续运算方便书写，暂时不展开后面那一项，并且我们令后验概率：</p><p><span class="math inline">\(p(z^{(i)}=c_k|x_i,\theta^{(t)}) =\gamma_t(z^{(i)}_k)\)</span></p><blockquote><p>说明：这个公式里的 <span class="math inline">\(\alpha_k\)</span>实际上是<span class="math inline">\(\alpha^{(i)}_k\)</span> ；后项<spanclass="math inline">\(p(z^{(i)}=c_k|x_i,\theta^{(t)})\)</span> 就是<spanclass="math inline">\(p(z^{(i)}_k|x_i,\theta^{(t)})\)</span>,换了个写法而已，上面已经写过了关于它的展开。</p></blockquote><h2 id="m-step">M-step</h2><p>    表示出Q函数后，M-step需要求的是：</p><p>    <span class="math inline">\(\theta^{(t+1)} = argmax_\thetaQ(\theta,\theta^{(t)})\)</span></p><p>    这里要求的<span class="math inline">\(\theta\)</span>一共有<spanclass="math inline">\(\alpha,\mu,\Sigma\)</span> ，只例举<spanclass="math inline">\(\alpha\)</span> 的求法，<spanclass="math inline">\(\mu,\Sigma\)</span> 将会直接给出结果。</p><p>    Q函数前项里与<span class="math inline">\(\alpha\)</span>相关的只有<span class="math inline">\(log\alpha\)</span>,于是仅对于<spanclass="math inline">\(\alpha\)</span>而言有<strong>约束优化问题</strong>：</p><p>    <span class="math inline">\(\theta^{(t+1)} = argmax_\theta\sum_{k=1}^K\sum_{i=1}^nlog\alpha_k\gamma_t(z^{(i)}_k)\)</span></p><p>    同时有限制条件：<span class="math inline">\(\sum_{k=1}^K\alpha_k= 1\)</span></p><h3 id="拉格朗日乘子法求解">拉格朗日乘子法求解</h3><p>    约束优化问题用拉格朗日乘子法求解，于是有拉格朗日函数：</p><p>    <span class="math inline">\(\ell(\alpha,\lambda) =\sum_{k=1}^K\sum_{i=1}^nlog\alpha_k*\gamma_t(z^{(i)}_k)+\lambda(\sum_{k=1}^K\alpha_k-1)\)</span></p><p>    对<span class="math inline">\(\alpha_k\)</span>求偏导,令其为0：</p><p>    <spanclass="math inline">\(\frac{\partial(\ell(\alpha,\lambda))}{\partial(\alpha_k)}= \sum_{i=1}^n\frac{1}{\alpha_k}\gamma_t(z^{(i)}_k)+\lambda =0\)</span></p><p>    两边同时乘分母，</p><p>    <spanclass="math inline">\(\sum_{i=1}^n\gamma_t(z^{(i)}_k)+\alpha_k\lambda =0~~~~~~~~~~~~~~(1)\)</span></p><p>    把所有<span class="math inline">\(\alpha_k\)</span>相加：</p><p>    <spanclass="math inline">\(\sum_{i=1}^n\sum_{k=1}^K\gamma_t(z^{(i)}_k)+\sum_{k=1}^K\alpha_k\lambda= 0\)</span></p><p>    又因为<spanclass="math inline">\(\gamma_t\)</span>为概率分布，<spanclass="math inline">\(\alpha\)</span> 为概率分布，有</p><p>    <span class="math inline">\(\sum_{k=1}^K\gamma_t(z^{(i)}_k) =1;\sum_{k=1}^K\alpha_k = 1\)</span></p><p>    所以化简可得：</p><p>    <span class="math inline">\(N+\lambda = 0\\\lambda =-n\)</span></p><p>    将<span class="math inline">\(\lambda =-n\)</span>代入（1）式中，可得<spanclass="math inline">\(\alpha_k\)</span>：</p><p>    <span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p>    而<span class="math inline">\(\alpha^{(t+1)} =\{a_1^{(t+1)},a_2^{(t+1)},...,a_k^{(t+1)}\}\)</span></p><h3 id="另外两个参数">另外两个参数</h3><p>    这里就不计算了，直接给结果吧。</p><p>    <span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p>    <span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><h2 id="再看e-step">再看E-step</h2><p>    其实刚才写E-step时并没有写E-step具体要求什么，现在我们推出了<spanclass="math inline">\(\theta^{(t+1)}\)</span>,发现只要求出<strong>隐变量z的后验分布</strong>的，那就都可以算出来。所以在E-step,实质上只需要求隐变量的后验概率分布<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>即可。这在上一篇EM算法推导中也说明过。精确到每一个后验概率：</p><p><span class="math display">\[\begin{split}\gamma_t(z^{(i)}_k) &amp;= p(z^{(i)}=c_k|x_i,\theta^{(t)})\\&amp;=\frac{\alpha_k^{(i),(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}\end{split}\]</span></p><h1 id="gmm总结">GMM总结</h1><h2 id="gmm聚类流程">GMM聚类流程</h2><p><strong>step1：</strong></p><p>    定义高斯分布个数K，对每个高斯分布设置初始参数值<spanclass="math inline">\(\theta^{(0)}_k = \alpha_k,\mu_k,\Sigma_k\)</span>。<strong>一般第一步不会自己设置初始值，而是通过K-mean算法计算初始值。</strong></p><p><strong>step2 E-step：</strong></p><p>    根据当前的参数<span class="math inline">\(\theta^{(t)}\)</span>,计算每一个隐变量的后验概率分布<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>。精确到每一个后验概率的计算，有</p><p>    <span class="math inline">\(\gamma_t(z_k^{(i)}) =\frac{\alpha_k^{(i),(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}\)</span></p><p><strong>step3 M-step：</strong></p><p>    根据E-step计算出的隐变量后验概率分布，进一步计算新的<spanclass="math inline">\(\theta^{(t+1)}\)</span></p><p>    <span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p>    <span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p>    <span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><strong>step4:</strong> 循环E-step和M-step直至收敛。</p><h2 id="gmm优缺点">GMM优缺点</h2><p><strong>优点：</strong></p><ul><li><p>GMM使用均值和标准差，簇可以呈现出椭圆形，优于k-means的圆形</p></li><li><p>GMM是使用概率，故一个数据点可以属于多个簇</p></li></ul><p><strong>缺点：</strong></p><p>    同EM算法缺点。</p><h2 id="gmm的实现与应用">GMM的实现与应用</h2><p>    将在下一篇文章进行GMM的具体实现和应用。</p><blockquote><p>    文章链接：待更新</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
            <tag> 聚类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo数学公式显示问题</title>
      <link href="/article/1d3b342ddcd7/"/>
      <url>/article/1d3b342ddcd7/</url>
      
        <content type="html"><![CDATA[<blockquote><p>    这周末在搬迁Hexo博客并且修复之前博客的bug，外加继续装修博客。整了一个周末终于弄好了。</p><p>    总结一下遇到的最让我无语而且耗时最长的问题吧。</p><p>    Latex数学公式+公式无法换行。</p></blockquote><p>    首先呢这是两个问题，要先让hexo支持数学公式，再来才是不能换行的问题。自己找了许许多多的教程，先后折磨了快半天，才找到了较为完美的解决方案。可能是关于这方面的教程大多都太老了，版本迭代后不太适用吧。</p><h1 id="关于hexo和主题">关于Hexo和主题</h1><p>    这个问题实际<strong>与主题无关</strong>。最开始我一直纠结于主题不同解决方法是不是不一样，最后解决了发现不是。</p><p>    我用的是hexo和butterfly截止目前的最新版本：</p><p>    hexo：6.3.0</p><p>    butterfly：4.4.0</p><h1 id="让hexo支持数学公式">让Hexo支持数学公式</h1><p>    这部分我跟着这篇文章搞的：<ahref="https://blog.csdn.net/gorray/article/details/122398901">Hexo如何显示latex公式_gorray的博客-CSDN博客_hexolatex公式</a></p><p>    其实要做的步骤很少：</p><p>    1.首先卸载hexo-math和hexo-renderer-marked。然而hexo应该是没有自带hexo-math的，所以只需要卸载第二个就行。以防万一还是可以直接执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-math</span><br><span class="line">npm un hexo-renderer-marked</span><br></pre></td></tr></table></figure><p>    2.安装hexo-renderer-pandoc渲染器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure><p>    好，到此为止，需要的包就迭代好了。</p><p>    3.然后是配置主题配置下的mathjax设置。我用的是butterfly，那么对应路径是： _config.yml</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h1 id="pandoc报错">Pandoc报错</h1><p>    理论上到这一步就可以用了对吧，一般的教程也都这样。就算再外加一步，我学习的那篇文章里提到，接下来还应该去Pandoc官网下载<strong>最新版本</strong>pandoc：<ahref="https://pandoc.org/index.html">Pandoc - About pandoc</a></p><p>    关于pandoc下载安装教程随便查一下就有，这里就不说明了。然后再在环境配置配置了pandoc路径，直到你可以在cmd输入以下命令查看它的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandoc -v</span><br></pre></td></tr></table></figure><p>    我的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pandoc 2.19.2</span><br><span class="line">Compiled with pandoc-types 1.22.2.1, texmath 0.12.5.2, skylighting 0.13,</span><br><span class="line">citeproc 0.8.0.1, ipynb 0.2, hslua 2.2.1</span><br><span class="line">Scripting engine: Lua 5.4</span><br></pre></td></tr></table></figure><p>    有的朋友可能到这一步就发现不对了，不要慌，接下来才是重点。</p><h2 id="一个莫名其妙的错误">一个莫名其妙的错误</h2><p>    先回到hexo目录，执行hexo -s,如果你没有出现这个报错：</p><p>    <span class="math inline">\(\color{red}{pandoc~exited ~with ~code~9: pandoc: Unknown~extension:~smart}\)</span></p><p>    那么恭喜你，你的这个问题并不存在，可以选择跳过。但是如果你和我一样报这个错误，可能就开始头疼了。不过我终于还是找到了解决方法。</p><p>    首先我是找到了这篇文章：<ahref="https://www.cnblogs.com/diralpo/p/12542450.html">配置hexo时遇到的问题- diralpo - 博客园 (cnblogs.com)</a></p><p>    从这篇文章得知，导致该报错的原因是<strong>pandoc版本过低</strong>，而且还不是一般原因引起的版本过低，因为前面我们已经安装了最新版本的pandoc。但是最新版本的没起作用。于是我打开了everything查找电脑上存在的pandoc。然后发现位于Anaconda，真正问题也出在这儿。</p><p>    <strong>是因为Anaconda安装的pandoc版本过低，而且hexo默认使用的是Anaconda的pandoc。</strong></p><p>    不信的话你去找找，那个pandoc居然是2017年的。在某一篇文章得知，pandoc版本应该在2.0以上，但那个pandoc好像是1.9。那接下来的就简单了，直接把新下载的pandoc.exe替换Anaconda里的pandoc.exe。</p><p><img src="https://img.issey.top/img/202209182134541.png" /></p><p>    然后你在回去hexo -s，就没问题了。</p><h1 id="换行问题">换行问题</h1><p>    其实做到上一步，换行问题也已经随之解决了。不过这里还是提一下关于这个换行。</p><p>    首先，想直接通过 ，end这种写法是做不到换行的，我最开始就是纠结于这个，然而这写法本来也不规范，例如：A  B是不能达到换行的。但是在加上规范的begin和end就可以了。其次，换行公式应该写成行间公式而非行内公式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$A \\ B$  错误写法</span><br><span class="line">\begin&#123;split&#125; A\\B \end&#123;split&#125; 写在行内错误，写在行间正确。</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{split} A\\B \end{split}\]</span></p><p>    这篇文章是我做的hexo公式测试：<ahref="https://www.issey.top/article/1365bcc580cd/">Latex公式测试 |issey的博客</a></p><p>    如果不是网速加载问题，那么显示应该是：</p><p>    <img src="https://img.issey.top/img/202209182149084.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码测试</title>
      <link href="/article/cb3b152854a1/"/>
      <url>/article/cb3b152854a1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    original_img = plt.imread(<span class="string">&#x27;color.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shape of original_img is:&quot;</span>, original_img.shape)</span><br><span class="line">    original_img /= <span class="number">255</span></span><br><span class="line">    X_img = np.reshape(original_img, (original_img.shape[<span class="number">0</span>] * original_img.shape[<span class="number">1</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    K = <span class="number">8</span></span><br><span class="line">    model = KMeans(n_clusters=K)</span><br><span class="line">    model.fit(X_img)</span><br><span class="line">    centroids = model.cluster_centers_</span><br><span class="line">    <span class="comment"># labels得到的是质心索引</span></span><br><span class="line">    labels = model.predict(X_img)</span><br><span class="line">    <span class="comment"># print(labels[:6])</span></span><br><span class="line">    <span class="comment"># 替换样本</span></span><br><span class="line">    X_recovered = centroids[labels]</span><br><span class="line">    X_recovered = np.reshape(X_recovered, original_img.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(X_recovered*<span class="number">255</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex公式测试</title>
      <link href="/article/1365bcc580cd/"/>
      <url>/article/1365bcc580cd/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[\mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix}\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\\frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp;0 \\\frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp;0 \\\end{vmatrix}\]</span></p><p>多行对齐：</p><p><span class="math display">\[\begin{gather}\begin{split}Adv^{Fed}&amp; = Pr^{Fed}\left ( A=1\mid x\in D_{T} \right ) - Pr^{Fed}\left (A=1\mid x\in D_{N} \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( P\left ( A=1\mid x \right) \right )-\underset{x\in D_{N}}{E^{Fed}}\left ( P\left ( A=1\mid x\right ) \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( 1-\frac{L\left ( \left (x,y \right ),F \right )}{A} \right )-\underset{x\in D_{N}}{E^{Fed}}\left( 1-\frac{L\left ( \left ( x,y \right ),F \right )}{A} \right )\\&amp; = \frac{1}{A}\cdot \left [ \underset{x\in D_{N}}{E^{Fed}}\left (L\left ( \left ( x,y \right ),F \right )\right )-\underset{x\inD_{T}}{E^{Fed}}\left ( L\left ( \left ( x,y \right ),F \right ) \right )\right ] \\\end{split}\end{gather}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记1——一元线性回归</title>
      <link href="/article/c4e174e36609/"/>
      <url>/article/c4e174e36609/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：本系列为机器学习的学习笔记，参考教程链接：</p><p><ahref="https://www.bilibili.com/video/BV1Pa411X76s?p=9&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=747540861ba5c41c17852ccf069029f5">(强推|双字)2022吴恩达机器学习Deeplearning.ai课程</a></p><p>观点不一定完全正确，欢迎指出错误的地方。</p></blockquote><h2 id="什么是线性回归模型">什么是线性回归模型？</h2><p>回归分析是研究自变量与因变量之间数量变化关系的一种分析方法，它主要是通过因变量Y与影响它的自变量<spanclass="math inline">\(X_{i}\)</span>（i=1,2,3…）之间的<ahref="https://so.csdn.net/so/search?q=%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020">回归模型</a>，衡量自变量<spanclass="math inline">\(X_{i}\)</span>对因变量Y的影响能力的，进而可以用来预测因变量Y的发展趋势。线性回归模型指因变量和自变量呈直线型关系的模型，是回归分析中最常用且最简单的方法，线性归回模型又分为一元线性回归模型和多元回归模型。</p><hr /><h2 id="一元线性回归模型">一元线性回归模型</h2><p>一元线性回归模型即自变量只有一个的线</p><h3 id="问题引入">问题引入：</h3><p><img src="https://img.issey.top/img/202209191544874.png" /></p><p>已知上图数据集，其中，X为自变量，Y为因变量，请预测当X为5000时Y的取值。</p><h3 id="问题解析">问题解析：</h3><p>因为自变量只有一个，即让你模拟一个<spanclass="math inline">\(f_{w,b}(x)=wx+b\)</span>,使该函数与上图自变量与应变量的变化趋势尽量满足，<spanclass="math inline">\(f_{w,b}(x)\)</span>即一元线性回归函数，再用计算出的回归函数去预测值即可。难点在于，这里的w和b都是未知数，我们要做的就是推断出最合适的w和b。</p><h3 id="代价函数损失函数">代价函数（损失函数）：</h3><p>如何判断w和b是否合适，我们引入了代价函数。代价函数用于判断<strong>整体来看</strong>，每个点（Y）实际值与估计值的差距大小。</p><p>这里先随便画一条线。</p><p><img src="https://img.issey.top/img/202209191550757.png" /></p><p>令模拟出来的自变量对应应变量的值为<spanclass="math inline">\(\hat{y}\)</span>,即<spanclass="math inline">\(\hat{y} = f_{w,b}(x)\)</span>,则代价函数为：</p><p><span class="math inline">\(J(w,b)=\frac{1}{2m}\sum_{i=0}^{m-1}{(\hat{y}_{i}-y_{i})^2} =\frac{1}{2m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})^2}\)</span></p><p>其中，m为训练集样例数，第一个点下标为0。这里除以2是方便后续计算。</p><h4 id="代价函数的图像">代价函数的图像</h4><p>我们先将<span class="math inline">\(f_{w,b}(x)\)</span>简化为<spanclass="math inline">\(f_{w}(x) = wx\)</span>,那么<spanclass="math inline">\(J(w) =\frac{1}{2m}\sum_{i=0}^{m-1}{(wx_{i}-y_{i})^2}\)</span></p><p>此时<span class="math inline">\(J(w)\)</span>的图像为一个凸函数</p><p><img src="https://img.issey.top/img/202209191551748.png" /></p><p>对应的<span class="math inline">\(f_w(x)\)</span>模拟情况：</p><p><img src="https://img.issey.top/img/202209191553611.png" /></p><p>当我们将<span class="math inline">\(f_{w,b}(x)\)</span>简化为<spanclass="math inline">\(f_{b}(x) = x+b\)</span>,此时<spanclass="math inline">\(J(b)\)</span>的图像也是一个凸函数，我们姑且借用<spanclass="math inline">\(J(w)\)</span>的图像，不过变量变为了b:</p><p><img src="https://img.issey.top/img/202209191552365.png" /></p><p>对应的<span class="math inline">\(f_{b}(x)\)</span>模拟情况：</p><p><img src="https://img.issey.top/img/202209191554982.png" /></p><p>现在将<span class="math inline">\(J(w)\)</span>和<spanclass="math inline">\(J(b)\)</span>合在一起，<spanclass="math inline">\(J(w,b)\)</span>便是一个三维碗装图像：</p><p>注：图中的w和b并不对应上面的例子，只是大致图像！</p><p><img src="https://img.issey.top/img/202209191555588.png" /></p><p><u>代价函数计算出的值越小，说明模拟值与实际值差距越小，则w，b越合适，回归函数模拟的越好。所以，当代价函数值最小时，w和b最合适。</u></p><p>于是问题转化为了：求w和b使得<spanclass="math inline">\(J(w,b)\)</span>能取到极小值。</p><h4 id="为什么不是最小而是极小值">为什么不是最小而是极小值？</h4><p>这与之后要用到的算法（梯度下降法）有关，梯度下降法只能求到极小值。不过梯度下降法常用于求凸函数的极小值，而凸函数只有一个极小值，所以通常求得的是最小值。这里举个非凸函数的例子，此时用梯度下降法不一定能求得最优解。</p><p><img src="https://img.issey.top/img/202209191556964.png" /></p><h3 id="梯度下降算法">梯度下降算法</h3><blockquote><p>梯度下降算法并不只用于求解线性回归问题。</p></blockquote><p>梯度算法在讲座中被描述为：假设你站在一个山坡上，你想最快下降到你四周最低的山谷。</p><p>即选择一个基点，以四周斜率绝对值最大的方向下降，直到下降到极小值点（此时斜率为0）停止。我们认为这个极小值点对应的w和b即为所求，一般我们选择<spanclass="math inline">\((0,0)\)</span>作为基点，即w和b开始为<spanclass="math inline">\((0,0)\)</span>，不过实际上基点怎么选都可以。</p><h4id="梯度下降算法公式对于一元线性回归模型">梯度下降算法公式（对于一元线性回归模型）：</h4><p>重复以下行为直到收敛：</p><p><span class="math inline">\(w = w - a\frac{\partial J(w,b)}{\partialw}\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(w,b)}{\partialb}\)</span></p><p>其中，<spanclass="math inline">\(a\)</span>被称为学习率。之后会讨论学习率<spanclass="math inline">\(a\)</span>的选择。</p><p>注意：w和b应该同时更新！（会在代码块详细说明）</p><p><span class="math inline">\(\frac{\partial J(w,b)}{\partial w} =\frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}x_{i}\)</span></p><p><span class="math inline">\(\frac{\partial J(w,b)}{\partial w} =\frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}\)</span></p><p>(之前代价函数除个2就是为了这里化简)</p><h4 id="学习率a的选择">学习率a的选择</h4><p>如果<spanclass="math inline">\(a\)</span>很小，那么每一步都走的很小，收敛过程就会很慢。</p><p><img src="https://img.issey.top/img/202209191556779.png" /></p><p>如果<span class="math inline">\(a\)</span>很大，<spanclass="math inline">\(J(w,b)\)</span>可能不会每次迭代都下降，可能错过最佳点，甚至导致发散。</p><p><img src="https://img.issey.top/img/202209191557675.png" /></p><p>关于学习率的设置有许多种方法，这里不做专门讨论<del>（其实是还没学到）</del>，姑且采用网上查到的一种简单的方法：在运行梯度下降法的时候会尝试一系列学习率的取值：...0.001,0.003，0.01, 0.03，0.1,0.3，1....尽量以三倍增长，直到找到一个合适的学习率。</p><h4 id="关于梯度下降每一步的变化">关于梯度下降每一步的变化</h4><p>梯度下降每一步并不是相等的，因为每一次迭代时，偏导数都会不断变化。在学习率选择合适的情况下，大概可以得到以下的每一步梯度变化图像。x轴为迭代次数，y轴为梯度。</p><p><img src="https://img.issey.top/img/202209191557819.png" /></p><p><img src="https://img.issey.top/img/202209191557852.png" /></p><p>可以看到最开始梯度很大，到后来慢慢接近于0。</p><h4 id="补充">补充：</h4><p>这里解释下为什么非凸函数中找到的不一定是最优解：</p><p><img src="https://img.issey.top/img/202209191558036.png" /></p><p>我们选择1和2分别作为起点，可能到达两个极小值点，我们无法判断找到的极小值点是否是全局最小值。当然凸函数只有一个极值点，所以对于凸函数，不存在这个问题。</p><h3 id="代码部分---案例实现">代码部分 - 案例实现</h3><h4 id="数据">数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2104.000000</span>,<span class="number">1600.000000</span>,<span class="number">2400.000000</span>,<span class="number">1416.000000</span>,<span class="number">3000.000000</span>,<span class="number">1985.000000</span>,<span class="number">1534.000000</span>,<span class="number">1427.000000</span>,<span class="number">1380.000000</span>,<span class="number">1494.000000</span>,<span class="number">1940.000000</span>,<span class="number">2000.000000</span>,<span class="number">1890.000000</span>,<span class="number">4478.000000</span>,<span class="number">1268.000000</span>,<span class="number">2300.000000</span>,<span class="number">1320.000000</span>,<span class="number">1236.000000</span>,<span class="number">2609.000000</span>,<span class="number">3031.000000</span>,<span class="number">1767.000000</span>,<span class="number">1888.000000</span>,<span class="number">1604.000000</span>,<span class="number">1962.000000</span>,<span class="number">3890.000000</span>,<span class="number">1100.000000</span>,<span class="number">1458.000000</span>,<span class="number">2526.000000</span>,<span class="number">2200.000000</span>,<span class="number">2637.000000</span>,<span class="number">1839.000000</span>,<span class="number">1000.000000</span>,<span class="number">2040.000000</span>,<span class="number">3137.000000</span>,<span class="number">1811.000000</span>,<span class="number">1437.000000</span>,<span class="number">1239.000000</span>,<span class="number">2132.000000</span>,<span class="number">4215.000000</span>,<span class="number">2162.000000</span>,<span class="number">1664.000000</span>,<span class="number">2238.000000</span>,<span class="number">2567.000000</span>,<span class="number">1200.000000</span>,<span class="number">852.000000</span>,<span class="number">1852.000000</span>,<span class="number">1203.000000</span></span><br><span class="line"><span class="number">399.899994</span>,<span class="number">329.899994</span>,<span class="number">369.000000</span>,<span class="number">232.000000</span>,<span class="number">539.900024</span>,<span class="number">299.899994</span>,<span class="number">314.899994</span>,<span class="number">198.998993</span>,<span class="number">212.000000</span>,<span class="number">242.500000</span>,<span class="number">239.998993</span>,<span class="number">347.000000</span>,<span class="number">329.998993</span>,<span class="number">699.900024</span>,<span class="number">259.899994</span>,<span class="number">449.899994</span>,<span class="number">299.899994</span>,<span class="number">199.899994</span>,<span class="number">499.997986</span>,<span class="number">599.000000</span>,<span class="number">252.899994</span>,<span class="number">255.000000</span>,<span class="number">242.899994</span>,<span class="number">259.899994</span>,<span class="number">573.900024</span>,<span class="number">249.899994</span>,<span class="number">464.500000</span>,<span class="number">469.000000</span>,<span class="number">475.000000</span>,<span class="number">299.899994</span>,<span class="number">349.899994</span>,<span class="number">169.899994</span>,<span class="number">314.899994</span>,<span class="number">579.900024</span>,<span class="number">285.899994</span>,<span class="number">249.899994</span>,<span class="number">229.899994</span>,<span class="number">345.000000</span>,<span class="number">549.000000</span>,<span class="number">287.000000</span>,<span class="number">368.500000</span>,<span class="number">329.899994</span>,<span class="number">314.000000</span>,<span class="number">299.000000</span>,<span class="number">179.899994</span>,<span class="number">299.899994</span>,<span class="number">239.500000</span></span><br></pre></td></tr></table></figure><h4 id="导入数据并绘制初始图">导入数据并绘制初始图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(<span class="string">&#x27;test.txt&#x27;</span>,dtype=np.float32,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">x_train = data[<span class="number">0</span>]</span><br><span class="line">y_train = data[<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_train,y_train,marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;r&#x27;</span>) <span class="comment"># marker 将样式设置为叉，c将颜色设置为红色</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="梯度产生函数">梯度产生函数</h4><p>对应公式：</p><p>sum_dw = <span class="math inline">\(\frac{\partial J(w,b)}{\partialw} = \frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}x_i\)</span></p><p>sum_db = <span class="math inline">\(\frac{\partial J(w,b)}{\partialw} = \frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 产生梯度函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient</span>(<span class="params">x,y,w,b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        x: x训练集</span></span><br><span class="line"><span class="string">        y: y训练集</span></span><br><span class="line"><span class="string">        w,b: 模型参数</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">        sum_dw: 代价函数对w的偏导数</span></span><br><span class="line"><span class="string">        sum_db: 代价函数对d的偏导数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 训练样例个数</span></span><br><span class="line">    sum_dw = <span class="number">0</span></span><br><span class="line">    sum_db = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb = w*x[i]+b</span><br><span class="line">        dw_i = (f_wb - y[i])*x[i]</span><br><span class="line">        db_i = f_wb - y[i]</span><br><span class="line">        sum_dw += dw_i</span><br><span class="line">        sum_db += db_i</span><br><span class="line"></span><br><span class="line">    sum_dw = sum_dw / m</span><br><span class="line">    sum_db = sum_db / m</span><br><span class="line">    <span class="keyword">return</span> sum_dw,sum_db</span><br></pre></td></tr></table></figure><h4 id="梯度迭代函数">梯度迭代函数</h4><p>对应公式：</p><p>重复以下行为直到收敛：</p><p><span class="math inline">\(w = w - a\frac{\partial J(w,b)}{\partialw}\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(w,b)}{\partialb}\)</span></p><p>注：代码中是让他迭代一定次数而并非以收敛为结束判断条件。这是因为当迭代次数足够大，也无限接近收敛了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度迭代函数(计算w和b)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x,y,init_w,init_b,alpha,num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数说明:</span></span><br><span class="line"><span class="string">        x: x训练集</span></span><br><span class="line"><span class="string">        y: y训练集</span></span><br><span class="line"><span class="string">        init_w: w初始值</span></span><br><span class="line"><span class="string">        init_b: b初始值</span></span><br><span class="line"><span class="string">        alpha: 学习率</span></span><br><span class="line"><span class="string">        num_iters: 迭代次数</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w,b:最终找到的w和b</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    w = init_w</span><br><span class="line">    b = init_b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># 产生梯度</span></span><br><span class="line">        sum_dw,sum_db = compute_gradient(x, y, w, b)</span><br><span class="line">        <span class="comment"># 同时更新w和b</span></span><br><span class="line">        w = w - alpha*sum_dw</span><br><span class="line">        b = b - alpha*sum_db</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h4 id="代价函数">代价函数</h4><p>对应公式：</p><p><span class="math inline">\(J(w,b)=\frac{1}{2m}\sum_{i=0}^{m-1}{(\hat{y}*{i}-y*{i})^2} =\frac{1}{2m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})^2}\)</span></p><p>这里只用于检验结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">x, y, w, b</span>):</span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb = w * x[i] + b</span><br><span class="line">        cost = cost + (f_wb - y[i]) ** <span class="number">2</span></span><br><span class="line">    total_cost = <span class="number">1</span> / (<span class="number">2</span> * m) * cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><h4 id="绘图和预测">绘图和预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;test.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    x_train = data[<span class="number">0</span>]</span><br><span class="line">    y_train = data[<span class="number">1</span>]</span><br><span class="line">    plt.scatter(x_train, y_train, marker=<span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;r&#x27;</span>)  <span class="comment"># marker 将样式设置为叉，c将颜色设置为红色</span></span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    init_m = <span class="number">0</span></span><br><span class="line">    init_b = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 一些梯度下降的设置</span></span><br><span class="line">    iterations = <span class="number">100000</span></span><br><span class="line">    tmp_alpha = <span class="number">0.000000095</span></span><br><span class="line">    w,b = gradient_descent(x_train,y_train,init_m,init_b,tmp_alpha,iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;线性回归函数为:f(x) = <span class="subst">&#123;w&#125;</span>x + <span class="subst">&#123;b&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;此时代价函数为:<span class="subst">&#123;compute_cost(x_train,y_train,w,b)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测当x = 5000是，y的值为:<span class="subst">&#123;w*<span class="number">5000</span>+b&#125;</span>&quot;</span>)</span><br><span class="line">    x = np.linspace(<span class="number">0</span>,<span class="number">5000</span>,<span class="number">100</span>)</span><br><span class="line">    y = w*x+b</span><br><span class="line">    plt.plot(x,y)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>在设置学习率alpha时，如果大了会报错，过小模拟出来的图像差距过大，这里尝试了许多次选了一个自认为比较合适的值。</p><h4 id="结果">结果</h4><p><img src="https://img.issey.top/img/202209191559138.png" /></p><p><img src="https://img.issey.top/img/202209191600322.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PageHelper返回给前端的数据示例以及参数说明</title>
      <link href="/article/da43cda93935/"/>
      <url>/article/da43cda93935/</url>
      
        <content type="html"><![CDATA[<h1id="pagehelper返回给前端的数据示例以及参数说明">PageHelper返回给前端的数据示例以及参数说明</h1><h2id="请求成功后前端拿到的response-body">请求成功后，前端拿到的Responsebody：</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;total&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span>    <span class="comment">// 所有数据条数</span></span><br><span class="line">  <span class="attr">&quot;list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>        <span class="comment">// 数据列表</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;pageNum&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>        <span class="comment">// 当前页码</span></span><br><span class="line">  <span class="attr">&quot;pageSize&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span>    <span class="comment">// 每页多少条数据</span></span><br><span class="line">  <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span>        <span class="comment">// 当前页有多少条数据，因为总数据只有7条，这是最后一页，少了一条</span></span><br><span class="line">  <span class="attr">&quot;startRow&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span>    <span class="comment">// 数据起始行，指本页面第一条数据在数据库中的行数</span></span><br><span class="line">  <span class="attr">&quot;endRow&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span>        <span class="comment">// 数据末行，指本页面最后一条数据在数据库中的行数</span></span><br><span class="line">  <span class="attr">&quot;pages&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>        <span class="comment">// 总页数</span></span><br><span class="line">  <span class="attr">&quot;prePage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span>        <span class="comment">// 前一页，第一页的prePage = 0</span></span><br><span class="line">  <span class="attr">&quot;nextPage&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span>    <span class="comment">// 后一页，最后一页的nextPage = 0</span></span><br><span class="line">  <span class="attr">&quot;isFirstPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>        <span class="comment">// 是否是第一页</span></span><br><span class="line">  <span class="attr">&quot;isLastPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span>    <span class="comment">// 是否是最后一页</span></span><br><span class="line">  <span class="attr">&quot;hasPreviousPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span>    <span class="comment">// 有前一页吗</span></span><br><span class="line">  <span class="attr">&quot;hasNextPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>        <span class="comment">// 有后一页吗</span></span><br><span class="line">  <span class="attr">&quot;navigatePages&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span>    <span class="comment">// 每页显示的页码个数</span></span><br><span class="line">  <span class="attr">&quot;navigatepageNums&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>        <span class="comment">// 显示的页码数（大概是这意思，没试过，应该是配合上一个属性用的）</span></span><br><span class="line">    <span class="number">1</span><span class="punctuation">,</span>    </span><br><span class="line">    <span class="number">2</span>    </span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;navigateFirstPage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span>    <span class="comment">// 起始页码</span></span><br><span class="line">  <span class="attr">&quot;navigateLastPage&quot;</span><span class="punctuation">:</span> <span class="number">2</span>        <span class="comment">// 尾页码</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
