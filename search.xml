<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>代码测试</title>
      <link href="/%E6%B5%8B%E8%AF%95/%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95/"/>
      <url>/%E6%B5%8B%E8%AF%95/%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    original_img = plt.imread(<span class="string">&#x27;color.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shape of original_img is:&quot;</span>, original_img.shape)</span><br><span class="line">    original_img /= <span class="number">255</span></span><br><span class="line">    X_img = np.reshape(original_img, (original_img.shape[<span class="number">0</span>] * original_img.shape[<span class="number">1</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    K = <span class="number">8</span></span><br><span class="line">    model = KMeans(n_clusters=K)</span><br><span class="line">    model.fit(X_img)</span><br><span class="line">    centroids = model.cluster_centers_</span><br><span class="line">    <span class="comment"># labels得到的是质心索引</span></span><br><span class="line">    labels = model.predict(X_img)</span><br><span class="line">    <span class="comment"># print(labels[:6])</span></span><br><span class="line">    <span class="comment"># 替换样本</span></span><br><span class="line">    X_recovered = centroids[labels]</span><br><span class="line">    X_recovered = np.reshape(X_recovered, original_img.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(X_recovered*<span class="number">255</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex公式测试</title>
      <link href="/%E6%B5%8B%E8%AF%95/Latex%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/"/>
      <url>/%E6%B5%8B%E8%AF%95/Latex%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[\mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix}\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\\frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp;0 \\\frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp;0 \\\end{vmatrix}\]</span></p><p>多行对齐：</p><p><span class="math display">\[\begin{gather}\begin{split}Adv^{Fed}&amp; = Pr^{Fed}\left ( A=1\mid x\in D_{T} \right ) - Pr^{Fed}\left (A=1\mid x\in D_{N} \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( P\left ( A=1\mid x \right) \right )-\underset{x\in D_{N}}{E^{Fed}}\left ( P\left ( A=1\mid x\right ) \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( 1-\frac{L\left ( \left (x,y \right ),F \right )}{A} \right )-\underset{x\in D_{N}}{E^{Fed}}\left( 1-\frac{L\left ( \left ( x,y \right ),F \right )}{A} \right )\\&amp; = \frac{1}{A}\cdot \left [ \underset{x\in D_{N}}{E^{Fed}}\left (L\left ( \left ( x,y \right ),F \right )\right )-\underset{x\inD_{T}}{E^{Fed}}\left ( L\left ( \left ( x,y \right ),F \right ) \right )\right ] \\\end{split}\end{gather}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记10——EM算法原理与详细推导</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<blockquote><p>    本文章为EM算法笔记，本人学习和参考的文章：<ahref="https://blog.csdn.net/v_JULY_v/article/details/81708386">如何通俗理解EM算法_v_JULY_v的博客-CSDN博客_em算法</a></p><p>    在开始之前，我们需要先复习一些在概率统计中学过的东西。</p></blockquote><h1 id="似然函数">似然函数</h1><blockquote><p>    统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：</p><p>    <strong>L(θ|x)=P(X=x|θ)。</strong></p><p>    注：L(θ|x)也可以写为L(θ:x)</p></blockquote><p>    用例说明可能会更好理解：</p><p>    考虑一个抛硬币实验，每次抛硬币相互独立，我们先假设正面朝上的概率<spanclass="math inline">\(P(H) = 0.5\)</span>,那么两次正面朝上的概率就是：</p><p>    <span class="math inline">\(P(H,H) = 0.5*0.5 = 0.25\)</span></p><p>    现在换一种角度，已知“两次抛硬币都是正面朝上”，则<strong>硬币正面朝上的概率为0.5的似然</strong>就是：</p><p>    <span class="math inline">\(L(p(H) = 0.5;HH) = P(HH|P(H) = 0.5) =0.25\)</span></p><p>    同理，硬币正面朝上的概率为0.6的似然就是0.36。</p><h1id="极大似然估计maximum-likelihood-estimatemle">极大似然估计（MaximumLikelihood Estimate，MLE）</h1><p>    <strong>极大似然估计也称为最大似然估计。</strong></p><p>    比如现在有一个袋子，里面装有白球和红球，你在里面随机抓了10个球，发现9个都是红球，那么你可以猜测从这个袋子抓出红球的概率为0.9，而抓出白球的概率为0.1。也就是说，我们<strong>根据样本来推测“为什么抓出来的样本会是这样”最可能的原因</strong>。</p><p>    这种根据样本推断<strong>最可能的</strong>模型未知参数的方法叫做极大似然估计。</p><p>    回到上面那个抛硬币的例子：如果两次抛硬币都是正面朝上。我们已经计算了当<spanclass="math inline">\(P(H) = 0.5\)</span> 时，似然函数等于0.25；当<spanclass="math inline">\(P(H) = 0.6\)</span>时，似然函数等于0.36。计算<span class="math inline">\(P(H) =1\)</span>时，似然函数将会等于1，此时是最大的。</p><p>    那么“两次抛硬币都正面朝上”的实验推出的极大似然估计就是“硬币正面朝上的概率为1”。</p><h2 id="极大似然估计应用">极大似然估计应用</h2><p>    现在我们使用抽样统计的方法从一个学校里随机抽取了100名男生和100名女生，并且统计他们各自的身高。</p><p>    假设男生的身高服从高斯分布<span class="math inline">\(X \simN(\mu_1,\sigma_1^2)\)</span> ,女生身高服从另一个高斯分布<spanclass="math inline">\(X \sim N(\mu_2,\sigma_2^2)\)</span>。不过我们只知道他们服从高斯分布模型，并不知道这两个模型的各个参数，即<spanclass="math inline">\(\mu,\sigma\)</span></p><p>是未知参数。</p><p>    现在，设<span class="math inline">\(\theta =[\mu,\sigma]^T\)</span>。我们需要利用极大似然估计，通过100个男生样本和100个女生样本分别估计各自的<spanclass="math inline">\(\theta\)</span>。因为求男生和求女生的过程一样，只是样本不同。所以接下来仅以求解男生身高对应的极大似然估计为例。</p><p>    令<spanclass="math inline">\(x_i\)</span>为来自男生样本集X的第i个样本。现在套入之前提到的似然函数<spanclass="math inline">\(L(\theta)\)</span>：</p><p>    $L() = P(X=x|θ)= P(x_1,x_2,...,x_n|) = _{i=1}^nP(x_i|) $</p><p>    现在要求<span class="math inline">\(\hat\theta\)</span>使得似然函数<span class="math inline">\(L(\theta)\)</span>最大，求得的<span class="math inline">\(\hat\theta\)</span> 即<spanclass="math inline">\(\theta\)</span>的极大似然估计。用公式写出来就是：</p><p>    <span class="math inline">\(\hat \theta =argmaxL(\theta)\)</span></p><h3 id="求解极大似然估计">求解极大似然估计</h3><p>    将似然函数化为<strong>对数似然函数：</strong></p><p>    <span class="math inline">\(\ell(\theta) = lnL(\theta) =ln\prod_{i=1}^nP(x_i|\theta) = \sum_{i=1}^nlnP(x_i|\theta)\)</span></p><p>    转化成对数似然函数后，再求导（或偏导），令导数为0，解得的参数就是极大似然估计。</p><p>    求解极大似然估计的一般步骤：</p><ol type="1"><li><p>写出似然函数</p></li><li><p>化位对数似然函数</p></li><li><p>求导数，令导数为0，得到似然方程</p></li><li><p>求解似然方程 </p></li></ol><blockquote><p>这里顺带提一下最大似然估计的适用条件：</p><p><strong>1.样本独立同分布</strong></p><p>2.已知样本服从某种分布模型，只是参数未知</p></blockquote><p>    以上都是概率统计的内容复习，及如何通过已知<strong>服从单个概率模型</strong>的样本集求解该模型的未知参数。</p><hr /><h1 id="初识em算法">初识EM算法</h1><h2 id="问题引入">问题引入</h2><p>    刚才提到，要想使用极大似然估计，必须确保样本都服从同一个分布。比如，我们通过100个男生身高样本使用极大似然估计可以算出男生身高服从的高斯模型。</p><p>    现在，考虑这样一种情况：男女生的样本混合在一起了，你不知道这200个样本哪个是男生，哪个是女生。现在要想计算男女生各自服从的高斯模型的未知参数，就需要用到EM算法。</p><h2 id="隐变量">隐变量</h2><p>    像上述问题中这种“不知道样本属于男生还是女生”在EM算法中被称为<strong>隐变量</strong>。隐变量记作<strong>Z</strong>，样本i的隐变量记作<spanclass="math inline">\(z_i\)</span>。</p><p>    隐变量指<strong>不可观测数据</strong>，比如聚类中，训练样本集给出了特征却没给出类别。这时候样本的类别就是隐变量。</p><p>    一般用<strong>Y</strong>表示可观测到的随机变量的数据，<strong>Z</strong>表示不可观测的随机变量的数据。Y与Z合起来被称为<strong>完全数据</strong>，只有Y则被称作<strong>不完全数据</strong>。</p><h2 id="直观理解em算法">直观理解EM算法</h2><p>    假设有5枚硬币，这些硬币来自A,B两类，但我们不知道它们各自属于哪类硬币。现在把这5枚硬币各抛10次，请根据样本数据推测A,B两类硬币各自的“正面朝上的极大似然估计”，并且将这5枚硬币分类。</p><p>    那么在上述例子中，“硬币种类”就是我们的隐变量，设为向量<strong>Z</strong>，<spanclass="math inline">\(Z = (z_1,z_2,z_3,z_4,z_5)\)</span></p><p>    （注：H代表正面朝上，T代表反面朝上）</p><table><thead><tr class="header"><th>硬币序号</th><th>硬币类别</th><th>结果（X）</th><th>统计</th></tr></thead><tbody><tr class="odd"><td>1</td><td><span class="math inline">\(z_1\)</span>,未知</td><td>H T T T T H T T T T</td><td>2H,8T</td></tr><tr class="even"><td>2</td><td><span class="math inline">\(z_2\)</span>,未知</td><td>H H H H T H H H H H</td><td>9H,1T</td></tr><tr class="odd"><td>3</td><td><span class="math inline">\(z_3\)</span>,未知</td><td>H T H T H H T T H H</td><td>6H,4T</td></tr><tr class="even"><td>4</td><td><span class="math inline">\(z_4\)</span>,未知</td><td>T H T T H T T T H T</td><td>3H,7T</td></tr><tr class="odd"><td>5</td><td><span class="math inline">\(z_5\)</span>,未知</td><td>T H H H T H H H T H</td><td>7H,3T</td></tr></tbody></table><p>现在使用EM算法求解：</p><p><strong>step1:随机初始化模型参数<spanclass="math inline">\(\theta^{(0)}\)</span> .</strong></p><p>    <span class="math inline">\(P(H_A)\)</span>、<spanclass="math inline">\(P(H_B)\)</span>分别是A类硬币、B类硬币正面朝上的概率，即<strong>模型未知参数</strong><spanclass="math inline">\(\theta\)</span>。</p><p>    EM算法需要设置一个初始化模型参数<spanclass="math inline">\(\theta^{(0)}\)</span> ,进而推出在<spanclass="math inline">\(\theta^{(0)}\)</span>条件下的隐变量<strong>Z</strong>。</p><p>    设<span class="math inline">\(P(H_A) = 0.2\)</span> , <spanclass="math inline">\(P(H_B) = 0.7\)</span> 。</p><p><strong>step2:计算隐变量</strong></p><p>    计算各硬币的隐变量：</p><blockquote><p>    还记得最开始提到的似然函数吗？这里就需要求似然，即<spanclass="math inline">\(L(θ|x)=P(X=x|θ)\)</span>。</p></blockquote><p>    例：</p><p>    对于1号硬币，属于A类的似然：</p><p>    $ L(<em>A^{(0)}|X = x_1)= P(X = x_1|<em>A^{(0)}) \= </em>{j =1}^{10}P(x</em>{1,j}|_A^{(0)})\= 0.2<sup>2</sup>8 = 67.1089^{-4}$</p><p>    对于1号硬币，属于B类的似然：</p><p>    <span class="math inline">\(L(\theta_B^{(0)}|X = x_1) = P(X =x_1|\theta_B^{(0)}) \\= \prod_{j = 1}^{10}P(x_{1,j}|\theta_B^{(0)})\\=0.7^2\times0.3^8 = 0.3215\times10^{-4}\)</span></p><p>    比较属于A类和属于B类的似然，发现A的似然更大，则把1号硬币归为A类，即令隐变量<spanclass="math inline">\(z_1 = A\)</span>。</p><p>   各枚硬币的隐变量计算：</p><table><colgroup><col style="width: 6%" /><col style="width: 10%" /><col style="width: 32%" /><col style="width: 32%" /><col style="width: 18%" /></colgroup><thead><tr class="header"><th>序号</th><th>统计</th><th>A类的似然(<span class="math inline">\(10^{-4}\)</span>)</th><th>B类的似然(<span class="math inline">\(10^{-4}\)</span>)</th><th>类别（隐变量）</th></tr></thead><tbody><tr class="odd"><td>1</td><td>2H,8T</td><td>67.11</td><td>0.32</td><td><span class="math inline">\(z_1\)</span> = A</td></tr><tr class="even"><td>2</td><td>9H,1T</td><td>0.004</td><td>121.06</td><td><span class="math inline">\(z_2\)</span> = B</td></tr><tr class="odd"><td>3</td><td>6H,4T</td><td>0.26</td><td>9.53</td><td><span class="math inline">\(z_3\)</span> = B</td></tr><tr class="even"><td>4</td><td>3H,7T</td><td>16.78</td><td>0.75</td><td><span class="math inline">\(z_4\)</span> = A</td></tr><tr class="odd"><td>5</td><td>7H,3T</td><td>0.06</td><td>22.24</td><td><span class="math inline">\(z_5\)</span> = B</td></tr></tbody></table><p>    我们通过假设模型未知参数，“猜出了”硬币各自属于哪一类，使得隐函数变为已知值，这时就满足了应用<strong>极大似然估计</strong>的条件，于是可以用极大似然估计求解未知参数<spanclass="math inline">\(\theta^{(1)}\)</span>。</p><p><strong>step3：极大似然估计求解模型参数<spanclass="math inline">\(\theta^{(1)}\)</span> .</strong></p><p>    根据<span class="math inline">\(\theta^{(0)}\)</span>,我们算出了1号、4号属于A类，2、3、5属于B类。于是跟新<spanclass="math inline">\(P(H_A)\)</span>和<spanclass="math inline">\(P(H_B)\)</span>:</p><p>    <span class="math inline">\(P(H_A) = \frac{2+3}{20} =0.25\)</span></p><p>    <span class="math inline">\(P(H_B) = \frac{9+6+7}{30} =0.73\)</span></p><p>    可以看出<span class="math inline">\(\theta^{(1)}\)</span>与<spanclass="math inline">\(\theta^{(0)}\)</span>并不相同。聪明的你可能想到了，接下来就是迭代的过程，重复step2和step3得到<spanclass="math inline">\(\theta^{(2)}\)</span>、<spanclass="math inline">\(\theta^{(3)}\)</span> ...</p><p>    可以证明，<strong>模型参数会随着迭代越来越接近真实值</strong>，并且一定会收敛到局部最优值。<strong>但不一定会收敛到全局最优值</strong>。</p><h2id="隐变量的期望隐变量的概率分布">隐变量的期望（隐变量的概率分布）</h2><p>    通过初步认识EM算法，你已经大致知道EM算法在干嘛了，但是EM算法在处理隐变量时，并不是通过似然函数判断某个样本的隐变量属于哪一个特定值，而是求<strong>隐变量的概率分布</strong>。再通过隐变量的概率分布进行后续运算。所以在正式开始推导EM算法前，我们还需要知道<strong>隐变量的概率分布</strong>。</p><h3 id="隐变量的概率分布">隐变量的概率分布</h3><p>    我们使用期望来简化计算，于是<strong>求隐变量的概率分布就是隐变量的期望</strong>。因为上面那个抛硬币的数据算出来的期望比较极端，所以这里重新举一个例子：现在有五枚硬币，分别来自A,B两类,但分类未知。</p><table><thead><tr class="header"><th>硬币序号</th><th>结果</th><th>统计</th></tr></thead><tbody><tr class="odd"><td>1</td><td>H H T H T</td><td>3H,2T</td></tr><tr class="even"><td>2</td><td>T T H H T</td><td>2H,3T</td></tr><tr class="odd"><td>3</td><td>H T T T T</td><td>1H,4T</td></tr><tr class="even"><td>4</td><td>H T T H H</td><td>3H,2T</td></tr><tr class="odd"><td>5</td><td>T H H T T</td><td>2H,3T</td></tr></tbody></table><p>    因为前面的过程都一样，所以我们直接来到计算出极大似然这一步。现在已经在随机固定模型参数<spanclass="math inline">\(\theta^{(0)}\)</span>的情况下计算出了每一枚硬币对应的似然：</p><table><thead><tr class="header"><th>硬币序号</th><th>A类似然</th><th>B类似然</th></tr></thead><tbody><tr class="odd"><td>1</td><td>0.00512</td><td>0.03087</td></tr><tr class="even"><td>2</td><td>0.02048</td><td>0.01323</td></tr><tr class="odd"><td>3</td><td>0.08192</td><td>0.00567</td></tr><tr class="even"><td>4</td><td>0.00512</td><td>0.03087</td></tr><tr class="odd"><td>5</td><td>0.02048</td><td>0.01323</td></tr></tbody></table><p>    在上一节中，接下来我们直接确认了隐变量的值。现在，我们用<strong>隐变量的期望</strong>来计算隐变量的概率分布<spanclass="math inline">\(Q(z)\)</span> 。</p><blockquote><p>因为每一个样本都有自己的隐变量概率分布，所以精确到样本的隐变量概率分布应该写为：<spanclass="math inline">\(Q_i(z^{(i)})\)</span>。<spanclass="math inline">\(z^{(i)}\)</span> 表示这是第<spanclass="math inline">\(i\)</span> 个样本的隐变量。</p></blockquote><p>    例如硬币1的隐变量概率分布：</p><p>$    P(z_1=A) = = 0.14$</p><p>    <span class="math inline">\(P(z_1=B) = 1-0.14 = 0.86\)</span></p><p>    其他同理，隐变量的期望如下：</p><table><thead><tr class="header"><th>硬币序号</th><th>P(<span class="math inline">\(z_i\)</span> = A)</th><th><span class="math inline">\(P(z_i=B)\)</span></th></tr></thead><tbody><tr class="odd"><td>1</td><td>0.14</td><td>0.86</td></tr><tr class="even"><td>2</td><td>0.61</td><td>0.39</td></tr><tr class="odd"><td>3</td><td>0.94</td><td>0.06</td></tr><tr class="even"><td>4</td><td>0.14</td><td>0.86</td></tr><tr class="odd"><td>5</td><td>0.61</td><td>0.39</td></tr></tbody></table><p>     再将隐变量的分布函数代入最开始的表以计算极大似然估计来计算下一轮的模型参数<spanclass="math inline">\(\theta^{(1)}\)</span> ,即新的<spanclass="math inline">\(P(H_A)\)</span>、<spanclass="math inline">\(P(H_B)\)</span>。</p><p>    计算1号各权值：</p><p>    1号是A类时正面朝上的权值： $w(A_1 = H) = 3*0.14 = 0.42 $</p><p>    1号是A类时反面朝上的权值：<span class="math inline">\(w(A_1 = T)= 2*0.14 = 0.28\)</span> </p><p>    1号是B类时正面朝上的权值：<span class="math inline">\(w(B_1 = H)= 3*0.86= 2.58\)</span></p><p>    1号是B类时反面朝上的权值：<span class="math inline">\(w(B_1 = T)= 2*0.86 = 1.72\)</span></p><table><colgroup><col style="width: 5%" /><col style="width: 8%" /><col style="width: 21%" /><col style="width: 21%" /><col style="width: 21%" /><col style="width: 21%" /></colgroup><thead><tr class="header"><th>序号</th><th>统计</th><th><span class="math inline">\(w(A_i = H)\)</span></th><th><span class="math inline">\(w(A_i = T)\)</span></th><th><span class="math inline">\(w(B_i=H)\)</span></th><th><span class="math inline">\(w(B_i = T)\)</span></th></tr></thead><tbody><tr class="odd"><td>1</td><td>3H,2T</td><td>3*0.14 = 0.42</td><td>2*0.14 = 0.28</td><td>3*0.86 = 2.58</td><td>2*0.86 = 1.72</td></tr><tr class="even"><td>2</td><td>2H,3T</td><td>2*0.61 = 1.22</td><td>3*0.61 = 1.83</td><td>2*0.39 = 0.78</td><td>3*0.39 = 1.17</td></tr><tr class="odd"><td>3</td><td>1H,4T</td><td>1*0.94 = 0.94</td><td>4*0.94 = 3.76</td><td>1*0.06 = 0.06</td><td>4*0.06 = 0.24</td></tr><tr class="even"><td>4</td><td>3H,2T</td><td>3*0.14 = 0.42</td><td>2*0.14 = 0.28</td><td>3*0.86 = 2.58</td><td>2*0.86 = 1.72</td></tr><tr class="odd"><td>5</td><td>2H,3T</td><td>2*0.61 = 1.22</td><td>3*0.61 = 1.83</td><td>2*0.39 = 0.78</td><td>3*0.39 = 1.17</td></tr><tr class="even"><td>总权值</td><td></td><td>4.22</td><td>7.98</td><td>6.78</td><td>6.02</td></tr></tbody></table><p><span class="math inline">\(\theta^{(1)}:\)</span></p><p>    <span class="math inline">\(P(H_A) = \frac{4.22}{4.22+7.98} =0.35\)</span></p><p>    <span class="math inline">\(P(H_B) = \frac{6.78}{6.78+6.02} =0.52\)</span></p><p>    这就是使用隐变量概率分布求极大既然估计的过程。</p><p>    现在推广到一般情况：</p><h1 id="em算法公式详细推导">EM算法公式详细推导</h1><h2 id="含隐变量的对数似然函数">含隐变量的对数似然函数</h2><p>   之前我们提到，极大似然估计是为了求下面这个似然函数的极大值： $    L()= P(X=x|θ)= P(x_1,x_2,...,x_n|) = _{i=1}^nP(x_i|)$</p><p>    其中，X服从独立同分布，<spanclass="math inline">\(\theta\)</span>为概率分布模型参数。</p><p>    根据极大似然方程的步骤，要先将似然函数化为对数似然函数：$    () =logL() = log<em>{i=1}^nP(x_i|) \    = </em>{i=1}^nlogP(x_i|)$</p><p>    问题在于现在X并不是同分布的样本集，同分布样本需要由X和隐变量Z共同确定。于是引入隐变量Z的对数似然变为：</p><p>    $() =<em>{i=1}^nlogP(x_i|) \=</em>{i=1}^nlog_{z}<sup>{Z</sup>{(i)}}P(x_i,z|) $</p><blockquote><p><strong>这里解释一下<span class="math inline">\(P(x_i|\theta) =\sum_z^{Z^{(i)}}P(x_i,z|\theta)\)</span>：</strong></p><p>此处为全概率公式，描述为：<strong>对每个样例的每种可能类别求联合分布概率和。</strong></p></blockquote><p>按照求极大似然的步骤，接下来对<spanclass="math inline">\(\ell(\theta)\)</span>应该求偏导，然后令偏导为0，求解似然方程。但是这里多了个随机变量z，像<spanclass="math inline">\(log\sum\)</span>的形式求导太过于复杂，所以我们需要转化方程：</p><h2 id="利用jensen不等式转化方程">利用jensen不等式转化方程</h2><p>    待会儿解释jensen不等式是什么，先看转化结果。</p><p>    为了使用jensen不等式，我们需要先把分子分母都乘隐变量<spanclass="math inline">\(z_i\)</span>的概率分布函数<spanclass="math inline">\(Q_i(z^{(i)})\)</span>。</p><p>    然后通过jensen不等式，可以将其变为：</p><p>    <span class="math inline">\(\ell(\theta) =\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}P(x_i,z|\theta)     （1）\\ =\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}       （2）\\\geq\sum_{i=1}^n\sum_{z}^{Z^{(i)}}Q_i(z)log\frac{P(x_i,z|\theta)}{Q_i(z)}        （3）\)</span></p><blockquote><p>（注：因为<span class="math inline">\(z\)</span>来自于<spanclass="math inline">\(Z^{(i)}\)</span>，所以其实<spanclass="math inline">\(z\)</span> 就是<spanclass="math inline">\(z^{(i)}\)</span> 的简写。）</p></blockquote><p>    可以看出对于（3）式就很好求导了。</p><p>    接下来说明是如何从(2)转化为(3)的。</p><h2 id="jeasen不等式转化详解">jeasen不等式转化详解</h2><p>    <strong>（国际定义）凸函数</strong>：设<spanclass="math inline">\(f(x)\)</span>为定义域为实数的函数。</p><ol type="1"><li><p>对于所有实数x，若<span class="math inline">\(f&#39;&#39;(x)\geq0\)</span> ,则<spanclass="math inline">\(f(x)\)</span>为凸函数。<strong>即下凹为凸，上凸为凹。</strong></p></li><li><p>当<strong>x</strong>为向量，如果hessian矩阵H是半正定的（<spanclass="math inline">\(H\geq0\)</span>）,那么<spanclass="math inline">\(f(\vec x)\)</span> 为凸函数。</p></li><li><p>若<span class="math inline">\(f&#39;&#39;(x)&gt;0\)</span>或<spanclass="math inline">\(H&gt;0\)</span> ，则<spanclass="math inline">\(f(x)\)</span>为严格凸函数。</p></li></ol><p>    Jeasen不等式：</p><blockquote><p>如果f是凸函数，X是随机变量，那么：<spanclass="math inline">\(E[f(X)]&gt;=f(E[X])\)</span>，通俗的说法是函数的期望大于等于期望的函数。而凹函数反之。</p><p>特别地，如果f是严格凸函数，当且仅当<span class="math inline">\(P(X =EX) = 1\)</span>。即X是常量时，有<span class="math inline">\(E[f(X)] =f(EX)\)</span>。</p></blockquote><p>    图就不画了，大概了解上述的用法即可。所以要用jeason不等式的关键在于，<strong>函数要为凸或凹函数，且需要表示出期望</strong>。</p><hr /><h3 id="如何表示期望">如何表示期望</h3><p>    通过将公式变成含有<spanclass="math inline">\(Q_i(z^{(i)})\)</span>的式子。我们可以表示出期望：  </p><p>    如果你还记得期望公式的懒人定理：</p><blockquote><p>    设Y是关于随机变量X的函数,<span class="math inline">\(Y =g(X)\)</span> ,g为连续函数，那么：</p><p>    若X是离散型随机变量，X分布律为<span class="math inline">\(P(X =x_k) = P_k,k = 1,2,3...,\)</span>若<spanclass="math inline">\(\sum_{k=1}^\infty g(x_k)p_k\)</span>绝对收敛则有：</p><p>    <span class="math inline">\(E(Y) = E[g(X)] = \sum_{k=1}^\inftyg(x_k)p_k\)</span></p><p>    若X是连续型随机变量，X的概率密度为<spanclass="math inline">\(f(x)\)</span>，若<spanclass="math inline">\(\int_{-\infty}^\infty g(x)f(x)dx\)</span>绝对收敛，则有：</p><p>    <span class="math inline">\(E(Y) = E[g(X)] =\int_{-\infty}^\infty g(x)f(x)dx\)</span></p></blockquote><p>    根据懒人定理，（2）中的<spanclass="math inline">\(\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\)</span>就是<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})}\)</span>的期望。原因：<span class="math inline">\(Q_i(z^{(i)})\)</span> 是<spanclass="math inline">\(z^{(i)}\)</span> 的分布函数，而<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})}\)</span>是关于随机变量<span class="math inline">\(z^{(i)}\)</span>的函数。</p><p>    于是，令<span class="math inline">\(g(z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})}\)</span>,</p><p>    则<span class="math inline">\(E(g(z^{(i)})) =\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\)</span> 。</p><hr /><h3 id="为什么是凹函数">为什么是凹函数</h3><p>    在（1）式中，将<spanclass="math inline">\(log\sum_{z}^{Z^{(i)}}P(x_i,z|\theta)\)</span>看作一个整体，即<span class="math inline">\(log(x)\)</span> 。由于<spanclass="math inline">\(f&#39;&#39;(log(x))&lt;0\)</span>，所以是凹函数。</p><hr /><h3 id="转化对数似然方程式为不等式">转化对数似然方程式为不等式</h3><p>    由于是凹函数，所以不等式反向：<spanclass="math inline">\(f(E[X])\geq E[f(x)]\)</span>,</p><p>   令<span class="math inline">\(f(x) = logx\)</span>,<spanclass="math inline">\(X = g(z^{(i)})\)</span>则：</p><p>    <span class="math inline">\(f(E[X]) = f(E[g(z^{(i)})]) =log\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\)</span></p><p>    $E[f(X)] = E[f(g(z^{(i)}))] = _{z}<sup>{Z</sup>{(i)}}Q_i(z)log $;</p><p>   于是：</p><p>   <span class="math inline">\(\ell(\theta) =\sum_{i=1}^nlog\sum_{z}^{Z^{(i)}}Q_i(z)\frac{P(x_i,z|\theta)}{Q_i(z)}\\       \geq\sum_{i=1}^n\sum_{z}^{Z^{(i)}}Q_i(z)log\frac{P(x_i,z|\theta)}{Q_i(z)} =J(z,Q)\)</span></p><p>    至此，我们得到了一个好求偏导的公式。</p><h2 id="拔高下界">拔高下界</h2><p>    回顾我们的目标：使<spanclass="math inline">\(\ell(\theta)\)</span>最大化，直至目前，我们通过jensen不等式，获得了一个可以求导的公式<spanclass="math inline">\(J(z,Q)\)</span>。现在的目的是通过<spanclass="math inline">\(J(z,Q)\)</span>获得<spanclass="math inline">\(\ell(\theta)\)</span> 最大值。</p><p>    我们可以通过不断最大化<spanclass="math inline">\(J(z,Q)\)</span>的下界来让<spanclass="math inline">\(\ell(\theta)\)</span>不断提高。为了顺利理清接下来的过程，在这里回顾一下各函数和变量的说明：</p><ul><li><p><span class="math inline">\(\theta\)</span> :模型参数（未知），我们可以假设它已知，并在不断迭代中更新<spanclass="math inline">\(\theta\)</span> 。</p></li><li><p><spanclass="math inline">\(\ell(\theta)\)</span>:对数似然方程，我们需要调整<spanclass="math inline">\(\theta\)</span>使得<spanclass="math inline">\(\ell(\theta)\)</span> 取极大值，此时$= argmax ()$</p></li><li><p><spanclass="math inline">\(Q(z)\)</span>:隐变量z的概率分布，其实就是隐变量的期望。</p></li><li><p><span class="math inline">\(J(z,Q)\)</span>:不等式右边的方程，我们可以不断调整它的最大值来使得<spanclass="math inline">\(\ell(\theta)\)</span> 最大。</p></li></ul><p>    <img src="提高下界.png" alt="loading-ag-919" /></p><p><strong>step1.首先固定<span class="math inline">\(\theta\)</span>,调整<span class="math inline">\(Q(z)\)</span>使下界<spanclass="math inline">\(J(z,Q)\)</span> 上升到与<spanclass="math inline">\(\ell(\theta)\)</span> 在点<spanclass="math inline">\(\theta\)</span> 处相等。</strong></p><p><strong>step2.固定<spanclass="math inline">\(Q(z)\)</span>,并使用极大似然估计法使<spanclass="math inline">\(\theta^{(t)}\)</span>达到最大值<spanclass="math inline">\(\theta^{(t+1)}\)</span> 。</strong></p><p><strong>step3.再将<spanclass="math inline">\(\theta^{(t+1)}\)</span>固定为模型参数，重复以上过程，直至收敛达到<spanclass="math inline">\(\ell(\theta)\)</span>最大值。此时对应的 <spanclass="math inline">\(\hat\theta\)</span>即为所求。</strong></p><p>    关于EM算法一定收敛的证明过程就暂时不写了。</p><blockquote><p>    step1想解决的问题可以阐述为：在模型参数已知的情况下，隐变量的最可能概率分布是什么？答案：使得<spanclass="math inline">\(J(z,Q)\)</span>与<spanclass="math inline">\(\ell(\theta)\)</span>在<spanclass="math inline">\(\theta\)</span>处相等的<spanclass="math inline">\(Q(z)\)</span> 。</p><p>    于是引出了新的问题：什么时候<spanclass="math inline">\(J(z,Q)\)</span>才与<spanclass="math inline">\(\ell(\theta)\)</span>相等？   </p></blockquote><h3 id="什么时候jzq-elltheta">什么时候<span class="math inline">\(J(z,Q)= \ell(\theta)\)</span></h3><p>    回顾Jensen不等式，当X是常量时，<spanclass="math inline">\(E[f(X)] = f(EX)\)</span>。所以我们为了让<spanclass="math inline">\(J(z,Q) = \ell(\theta)\)</span> ,需要让<spanclass="math inline">\(X\)</span>为常数。</p><p>    于是<span class="math inline">\(X = g(z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})} = c\)</span>    ,其中<spanclass="math inline">\(c\)</span>为常数。</p><p>    在由（1）变化（2）时，曾经提到过<spanclass="math inline">\(\sum_z^{Z^{(i)}}Q_i(z^{(i)}) =1\)</span>。因为<spanclass="math inline">\(Q(z)\)</span>是隐变量z的概率分布，概率之和为1。</p><p>    现在我们可以变换公式：（先同乘分母，再化简）</p><p>    因为 ：<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})} =c\)</span>，</p><p>    所以：<spanclass="math inline">\(\sum_z^{Z^{(i)}}P(x_i,z^{(i)}|\theta) =\sum_z^{Z^{(i)}}Q_i(z^{(i)})c = c\)</span> ，即<spanclass="math inline">\(\sum_z^{Z^{(i)}}P(x_i,z^{(i)}|\theta) =c\)</span>    (1)</p><p>    又因为由<spanclass="math inline">\(\frac{P(x_i,z^{(i)}|\theta)}{Q_i(z^{(i)})} =c\)</span> 可得：</p><p>    <span class="math inline">\(Q_i(Z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{c}\)</span>     (2)</p><p>    于是当<span class="math inline">\(J(z,Q) =\ell(\theta)\)</span>时,将（1）代入（2）可得：</p><p>    <span class="math inline">\(Q_i(Z^{(i)}) =\frac{P(x_i,z^{(i)}|\theta)}{c}\\              =\frac{P(x_i,z^{(i)}|\theta)}{\sum_z^{Z^{(i)}}P(x_i,z^{(i)}|\theta)}\\              = P(z^{(i)}|x_i;\theta)\)</span></p><p> <strong>结论：</strong></p><p>    当<span class="math inline">\(J(z,Q) =\ell(\theta)\)</span>时，<spanclass="math inline">\(Q_i(z^{(i)})\)</span>为隐变量的条件概率分布，即隐变量的期望。</p><p>    现在我们只需要利用似然函数求隐变量的期望即可(前面有讲过怎么求)。</p><hr /><h1 id="em算法总结">EM算法总结</h1><h2 id="em算法应用场景">EM算法应用场景</h2><p>    EM算法是一种迭代算法，主要用于含有<strong>隐变量</strong>的概率模型参数的极大似然估计或极大后验估计。广泛应用于缺损数据、截尾数据、成群数据、带有讨厌参数的数据等所谓不完全数据的统计推断问题。</p><h2 id="em算法步骤">EM算法步骤</h2><p>    其实在EM算法直观理解和详细推导时已经说了两次它的步骤了，不过在这里还是再总结一下，如果你和我一样没看懂，待会儿会给出解释：</p><blockquote><p>    输入：观测变量数据Y;隐变量数据Z;联合分布<spanclass="math inline">\(P(Y,Z|\theta)\)</span>,即完全数据的概率分布；条件分布<spanclass="math inline">\(P(Z|Y,\theta)\)</span>,即未观测数据Z的条件概率分布。</p><p>    输出：模型参数<span class="math inline">\(\theta\)</span> 。</p></blockquote><p>step1：初始化模型参数<spanclass="math inline">\(\theta^{(0)}\)</span> ，进入迭代。</p><p>step2（E-step）：记<span class="math inline">\(\theta^{(i)}\)</span>为第i次迭代参数<span class="math inline">\(\theta\)</span>估计值，第<spanclass="math inline">\(i+1\)</span>次迭代的E-step将计算</p><p>        $Q(,^{(i)})=E_z[logP(Y,Z|)|Y,^{(i)}]\               =_ZlogP(Y,Z|)P(Z|Y,^{i}) $</p><p>step3（M-step）：利用极大似然估计，计算使<spanclass="math inline">\(Q(\theta,\theta^{(i)})\)</span> 极大化的<spanclass="math inline">\(\hat \theta\)</span>，作为第<spanclass="math inline">\(i+1\)</span> 次迭代的参数估计值<spanclass="math inline">\(\theta^{(i+1)}\)</span>。</p><p>        <span class="math inline">\(\theta^{(i+1)} = argmax_\thetaQ(\theta,\theta^{(i+1)})\)</span></p><p>step4:重复step2、step3直到收敛。收敛时的<spanclass="math inline">\(\theta\)</span> 即为所求。</p><h3 id="关于步骤的解释">关于步骤的解释</h3><p>    <spanclass="math inline">\(Q(\theta,\theta^{(i)})\)</span>：完全数据的对数似然函数<spanclass="math inline">\(P(Y,Z|\theta)\)</span>关于在给定观测数据Y和当前参数<spanclass="math inline">\(\theta^{(i)}\)</span>下对未观测数据Z的条件概率分布<spanclass="math inline">\(P(Z|Y,\theta^{(i)})\)</span> 的期望称为Q函数。</p><p>    所以这个Q函数其实就是推导过程中的<spanclass="math inline">\(Q_i(z^{(i)})\)</span>,在拔高下界时详细说明了为什么求的是它。</p><p>    也就是说，<strong>E-step求的是在固定<spanclass="math inline">\(\theta\)</span>的条件下求隐变量z的期望</strong>，等同于拔高下界中的step1。</p><p>    而M-step则是固定<span class="math inline">\(Q_i(z^{(i)})\)</span>后极大似然估计求<span class="math inline">\(\theta^{(i+1)}\)</span>的过程。等同于拔高下界中的step2。</p><h2 id="关于em算法的重要说明">关于EM算法的重要说明</h2><ol type="1"><li><p>初始化时可以随机选择<spanclass="math inline">\(\theta^{(0)}\)</span>，但是EM算法对初始值很敏感，一旦选择不好会造成很大的计算损失。 对于初始值的选择也有相关的做法，这里暂时不研究。</p></li><li><p>停止迭代的条件一般是对于较小的正数<spanclass="math inline">\(A\)</span> ,若满足<spanclass="math inline">\(|\theta^{(i+1)}-\theta^{(i)}|&lt; A\)</span>，停止迭代。</p></li><li><p>EM算法的推进还有广义EM算法和EM算法变种等等。</p></li></ol><h2 id="算法优缺点">算法优缺点</h2><p><strong>优点：</strong></p><ol type="1"><li><p>通常计算起来比较简单</p></li><li><p>收敛稳定，不需要设置超参数</p></li></ol><p><strong>缺点：</strong></p><ol type="1"><li><p>对大规模数据和多维高斯分布，计算量大，迭代速度慢</p></li><li><p>如果初始值设置不当，收敛过程的计算代价会非常大。</p></li><li><p>EM算法求得的是<strong>局部最优解</strong>而不一定是全局最优解。</p></li></ol><h1 id="em算法的应用">EM算法的应用</h1><p>    关于EM算法的应用，例如处理混合高斯模型（GMM）和聚类等等，将会在下一篇EM文章中详细介绍。</p><p>    文章链接：（待更新）    </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类算法 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
