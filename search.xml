<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于hive的启动和连接</title>
      <link href="/article/a54e99ba2904/"/>
      <url>/article/a54e99ba2904/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>​太久没用hive了，今天想重新熟悉一下，结果发现自己甚至忘记了怎么启动。。于是特此记录篇笔记，便于以后忘记时查阅，不会写的太细。</p><p>​顺便从这篇文章开始改变自己文章的格式，以前都是乱整，想稍微更好看一点。</p><h1 id="hive的启动与连接">hive的启动与连接</h1><h2 id="启动hadoop">启动hadoop</h2><p>​以root权限登录hadoop中心节点计算机（亲测用户登录不能启动hadoop），使用以下命令开启hadoop集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>​ 可以通过以下代码查看是否成功启动hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>​ 输出长这样：</p><p><img src="https://img.issey.top/img/202209202106612.png" /></p><p>​这时候就可以访问hadoop网页了：http://ip:9870，ip为你hadoop中心节点计算机ip。</p><p>​ 顺便提一下yarn的默认端口：8088</p><h2 id="启动hive">启动hive</h2><p>​这一步与许多教程不一样，可能是版本原因。root登录你安装hive的计算机，我的就在hadoop中心计算机上，在没有配置环境变量的情况下，进入hive安装目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/hive-3.1.2/</span><br></pre></td></tr></table></figure><p>​ 然后启动hivemetastore服务，这一步可以后台启动也可以前台启动。我现在使用前台启动，因为可以看到日志。使用前台启动之后这个命令框就不能动了。接下来开另一个命令框，如果不想再开命令框，可以选择后台启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前台启动：bin/hive --service metastore</span><br><span class="line">后台启动：<span class="built_in">nohup</span> bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>​在启动metastore服务后，同样在hive安装目录下接着启动hiveserver2服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前台启动：bin/hive --service hiveserver2</span><br><span class="line">后台启动：<span class="built_in">nohup</span> bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><p>​ 如果是前台启动，成功后会看到Hive sessionID过十几秒会变一个。到此hive服务完全启动。接下来就是hive连接了。关于hive连接没啥好注意的，和mysql连接差不多。不过注意hive默认是不需要密码的。所以账号输root，密码填空就行。测试链接，如果没问题的话hiveserver2那边的命令框会跳出"OK"。</p><h2 id="一些注意事项">一些注意事项</h2><p>​今天在启动hive服务时出现了连接不上的情况，经过分析发现是因为我前台启动然后ctrl+z（因为用的Xshell所以是ctrl+z,等同ctrl+c）后程序并没有被完全杀死。然后我又开了一个后台启动，就出错了。所以退出服务后一定要检查jps，如果程序还在一定要先kill-9。</p><p>​正常情况下，开启了metastore和hiveserver2后jps显示的只有两个Runjar。如果不是两个说明可能开多了。</p><p><img src="https://img.issey.top/img/202209202128688.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记11——高斯混合模型原理与推导</title>
      <link href="/article/3f1c8d74d0f5/"/>
      <url>/article/3f1c8d74d0f5/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ol type="1"><li><ahref="https://blog.csdn.net/qq_52466006/article/details/126786732">EM算法——直观理解与详细推导_TwilightSparkle.的博客-CSDN博客</a></li></ol><hr /><blockquote><p>    前言：在上一篇文章中，我们详细推导了EM算法的原理和过程，在最后，我们还剩下EM算法的具体应用。文章链接：<ahref="https://blog.csdn.net/qq_52466006/article/details/126786732">EM算法——直观理解与详细推导_TwilightSparkle.的博客-CSDN博客</a></p><p>    这篇文章就不对EM算法的流程和公式做过多赘述，详细请见上篇文章。在本篇文章中，将会利用EM算法处理高斯混合模型（GMM）。</p><p>    GMM学习参考链接：<ahref="https://www.bilibili.com/video/BV1aE411o7qd?p=69&amp;vd_source=747540861ba5c41c17852ccf069029f5">(系列十一)高斯混合模型4-EM求解-M-Step_哔哩哔哩_bilibili</a></p></blockquote><p>    高斯混合模型（<strong>G</strong>aussian <strong>M</strong>ixed<strong>M</strong>odel）指多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布。高斯混合模型通常用于解决同一集合下的数据包含多个不同的分布的情况，即<strong>聚类</strong>。</p><h1 id="高斯混合模型与k-mean">高斯混合模型与K-mean</h1><p>    根据K-mean聚类算法的原理，K-mean算法的缺点之一在于无法将两个聚类中心点相同的类进行聚类，比如<spanclass="math inline">\(A\sim N(\mu,\sigma_1^2),B\simN(\mu,\sigma^2_2)\)</span>,此时将无法用K-mean算法聚类出A，B。为了解决这一缺点，提出了高斯混合模型（GMM）。GMM通过选择成分最大化后验概率完成聚类，各数据点的后验概率表示属于各类的可能性，而不是判定它完全属于某个类，所以称为<strong>软聚类</strong>。其在各类尺寸不同、聚类间有相关关系的时候可能比k-means聚类更合适。</p><h1 id="gmm的概率密度函数">GMM的概率密度函数</h1><h2 id="从几何角度">从几何角度</h2><p><img src="https://img.issey.top/img/202209191338651.jpg" /></p><p>    设有随机变量<strong>X</strong>，则单高斯分布可以写为：</p><p>    <span class="math inline">\(X\sim N(\mu,\Sigma)\)</span></p><p>    像图示的这种高斯混合分布（红线），每个样本可以看作是<strong>几个高斯分布加权平均叠加</strong>而成。于是高斯混合模型的概率密度函数为：</p><p>    <span class="math inline">\(p(x)=\sum_{k=1}^K\alpha_kN(x|\mu_k,\Sigma_k)\)</span></p><ul><li><p>K：k个高斯分布</p></li><li><p><spanclass="math inline">\(\alpha_k\)</span>：权值，样本x属于第k个高斯分布的概率。对于一个样本x,有<spanclass="math inline">\(\sum_{k=1}^Ka_k = 1\)</span>。</p></li><li><p><spanclass="math inline">\(N(x|\mu_k,\Sigma_K)\)</span>：在第k个高斯分布条件下发生x的概率。其中，<spanclass="math inline">\(\mu\)</span>为均值，<spanclass="math inline">\(\Sigma\)</span>为协方差矩阵。</p></li></ul><h2 id="从混合模型角度">从混合模型角度</h2><p>    首先需要引入隐变量<spanclass="math inline">\(z\)</span>,隐变量在EM算法时已经说过，这里就不再说明隐变量的定义。只是再提一下，<strong>每个样本都有自己的隐变量</strong>。</p><p>    隐变量<span class="math inline">\(z\)</span>的概率分布：<spanclass="math inline">\(z\)</span>为离散随机变量，<spanclass="math inline">\(z\)</span>的概率分布为对应的样本x属于每一个高斯分布的概率分布。</p><table><thead><tr class="header"><th><span class="math inline">\(z\)</span></th><th>1</th><th>2</th><th>...</th><th>k</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(P(z)\)</span></td><td><span class="math inline">\(p_1\)</span></td><td><span class="math inline">\(p_2\)</span></td><td>...</td><td><span class="math inline">\(p_k\)</span></td></tr></tbody></table><p>    <span class="math inline">\(p_k\)</span>表示样本x属于第k个高斯分布的概率，有<spanclass="math inline">\(\sum_{k=1}^KP_k = 1\)</span> 。</p><p>    为了便于理解，之后也会使用<spanclass="math inline">\(p(z=c_k)\)</span> 的形式来表示<spanclass="math inline">\(p_k\)</span> 。其中，<spanclass="math inline">\(c_k\)</span>为第k类高斯分布。</p><p>    在引入隐变量<spanclass="math inline">\(z\)</span>后，x的密度函数将变为：</p><p><span class="math display">\[\begin{split}p(x)&amp; = \sum_Zp(x,z) =\sum_{k=1}^Kp(x,z=c_k)\\&amp;=\sum_{k=1}^Kp(z=c_k)*p(x|z=c_k)\end{split}\]</span></p><p>    我们令<span class="math inline">\(p_k\)</span>为<spanclass="math inline">\(\alpha_k\)</span>,因为<spanclass="math inline">\(p(z=c_k)\)</span>就是<spanclass="math inline">\(p_k\)</span>,所以<spanclass="math inline">\(p(z=c_k)\)</span> 可表示为<spanclass="math inline">\(\alpha_k\)</span> ;</p><p>    而<span class="math inline">\(p(x|z=c_k)\)</span>就是<spanclass="math inline">\(N(x|\mu_k,\Sigma_k)\)</span> 。于是：</p><p>    <span class="math inline">\(p(x) = \sum_{k=1}^K\alpha_kN(x|\mu_k,\Sigma_k)\)</span></p><p>    这与从几何角度得出的概率密度公式相同。</p><h1 id="极大似然估计">极大似然估计</h1><p>    现在我们要用已知样本估计k个高斯分布的参数，一般通过样本估计模型参数的方法为极大似然估计（MLE），MLE在EM算法中已经讲过。</p><p>    定义变量和参数：</p><ul><li><p>x：随机变量，<spanclass="math inline">\(x_i\)</span>表示第i个样本。</p></li><li><p>z：隐变量，<span class="math inline">\(z^{(i)}\)</span>表示第i个样本的隐变量。</p></li><li><p>X：可观测数据，<span class="math inline">\(X =\{x_1,x_2,...,x_n\}\)</span></p></li><li><p>Z：不可观测数据，<span class="math inline">\(Z =\{z^{(1)},z^{(2)},...,z^{(n)}\}\)</span></p></li><li><p><span class="math inline">\(\theta\)</span>：模型参数，<spanclass="math inline">\(\theta =\{\alpha_1,\alpha_2,...,\alpha_k;\mu_1,\mu_2,...\mu_k;\Sigma_1,\Sigma_2,...,\Sigma_k\}\)</span></p></li></ul><p>回顾MLE目标函数:</p><p><span class="math display">\[\begin{split}\hat\theta&amp; = argmax_\theta  logP(X|\theta) \\ &amp;=argmax_\theta  log \prod_{i=1}^np(x_i|\theta)\\ &amp;=argmax_\theta  \sum_{i=1}^n logp(x_i|\theta)\end{split}\]</span></p><p>    将高斯混合模型的概率密度函数代入，得：</p><p><span class="math display">\[\begin{split}\hat\theta &amp;= argmax_\theta \sum_{i=1}^nlogp(x_i|\theta)\\&amp;=argmax_\theta\sum_{i=1}^nlog\sum_{k=1}^K\alpha_kN(x_i|\mu_k,\Sigma_k)\end{split}\]</span></p><p>    注：<span class="math inline">\(\alpha_k\)</span> 可以写为<spanclass="math inline">\(p_k\)</span> ，具体含义在上一节讲过。</p><p>    因为引入了隐变量，导致这个式子含有<spanclass="math inline">\(log\sum\)</span>，无法再进行MLE下一个步骤。回顾EM算法，EM算法就是拿来求解此类问题的。于是接下来需要用EM迭代求近似解。</p><h1 id="em算法求近似解">EM算法求近似解</h1><h2 id="e-step-简化q函数">E-step 简化Q函数</h2><blockquote><p>关于E-step具体要计算什么需要推导完M-step再回来说明。</p></blockquote><p>    回顾EM算法，目标函数为<span class="math inline">\(\hat\theta=argmax_\theta Q(\theta,\theta^{(t)})\)</span>。</p><p>    Q函数公式：</p><p><span class="math display">\[\begin{split}Q(\theta,\theta^{(t)})&amp; =E_z[logP(X,Z|\theta)|X,\theta^{(t)}] \\&amp;=\sum_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})\end{split}\]</span></p><p>    因为高斯混合模型中的完整数据<spanclass="math inline">\((X,Z)\)</span> 独立同分布，未观测数据<spanclass="math inline">\(Z\)</span> 独立同分布，所以：</p><p><span class="math display">\[\begin{split}Q(\theta,\theta^{(t)}) &amp;=\sum_Zlog \prod_{i=1}^np(x_i,z^{(i)}|\theta)\prod _{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_Z[ \sum_{i=1}^nlog p(x_i,z^{(i)}|\theta) ]\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\end{split}\]</span></p><p>    注：$_Z $ 是<spanclass="math inline">\(\sum_{z^{(1)},z^{(2)},...,z^{(n)}}\)</span>的简写。</p><blockquote><p>关于最前面那个<span class="math inline">\(\sum_Z\)</span>的解释：</p><p>如果是是连续型函数，Q的表达式应该是：</p><p><span class="math inline">\(Q(\theta,\theta^{(t)}) =\int_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})dz\)</span></p><p>但是现在是离散型，所以积分就变成了求和。</p></blockquote><h3 id="简化q函数">简化Q函数</h3><p><strong>展开Q函数：</strong><spanclass="math inline">\(Q(\theta,\theta^{(t)}) =\sum_Z[ logp(x_1,z^{(1)}|\theta)+logp(x_2,z^{(2)}|\theta)+...+logp(x_n,z^{(n)}|\theta) ]\prod _{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\)</span></p><p><strong>只看第一项：</strong></p><p><span class="math inline">\(\sum_Zlogp(x_1,z^{(1)}|\theta)*\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\)</span></p><blockquote><p>为什么要带上<span class="math inline">\(\sum_Z\)</span>:一样先看成积分再变成离散的形式就好理解了。</p></blockquote><p>因为<span class="math inline">\(logp(x_1,z^{(1)}|\theta)\)</span>只与<span class="math inline">\(z^{(1)}\)</span> 相关，而<spanclass="math inline">\(\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\)</span>中，<spanclass="math inline">\(p(z^{(1)}|x_i,\theta^{(t)})\)</span> 与<spanclass="math inline">\(z^{(1)}\)</span>相关，所以可以将上式改写为：</p><p><span class="math display">\[\begin{split}&amp;\sum_Zlogp(x_1,z^{(1)}|\theta)*\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{z^{(1)}}logp(x_1,z^{(1)}|\theta)*p(z^{(1)}|x_1,\theta^{(t)})*[\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)})]\end{split}\]</span></p><blockquote><p>然后对于<span class="math inline">\(\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)})\)</span> ,实际上它等于1：</p><p>如同<span class="math inline">\(z^{(1)}\)</span> 一样，<spanclass="math inline">\(p(z^{(i)}|x_i,\theta^{(t)})\)</span> 只与<spanclass="math inline">\(z^{(i)}\)</span> 相关，所以上式展开将变为：</p><p><span class="math display">\[\begin{split}&amp;\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)}) \\&amp; =\sum_{z^{(2)},...z^{(n)}}p(z^{(2)}|x_2,\theta^{(t)})*p(z^{(3)}|x_3,\theta^{(t)})*...*p(z^{(n)}|x_n,\theta^{(t)})\\&amp;=\sum_{z^{(2)}}p(z^{(2)}|x_2,\theta^{(t)})*\sum_{z^{(3)}}p(z^{(3)}|x_3,\theta^{(t)})*...*\sum_{z^{(n)}}p(z^{(n)}|x_n,\theta^{(t)})\end{split}\]</span></p><p>而<span class="math inline">\(\sum_{z^{(i)}}p(z^{(i)}|x_i)=1\)</span> ,所以全部都可以约为1。</p><p>于是<span class="math inline">\(\sum_{z^{(2)},...z^{(n)}}\prod_{i=2}^np(z^{(i)}|x_i,\theta^{(t)}) = 1\)</span></p></blockquote><p>所以第一项：</p><p><span class="math display">\[\begin{split}&amp;\sum_Zlogp(x_1,z^{(1)}|\theta)\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)}) \\&amp;=\sum_{z^{(1)}}logp(x_1,z^{(1)}|\theta)p(z^{(1)}|x_1,\theta^{(t)})\end{split}\]</span></p><p><strong>再看整体：</strong></p><p>有了第一项的结论，推广到整体</p><p><span class="math display">\[\begin{split}&amp;\sum_Z[ logp(x_1,z^{(1)}|\theta)+logp(x_2,z^{(2)}|\theta)+...+logp(x_n,z^{(n)}|\theta) ]\prod _{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\ &amp;=\sum_{z^{(1)}}logp(x_1,z^{(1)}|\theta)p(z^{(1)}|x_1,\theta^{(t)})+...+\sum_{z^{(n)}}logp(x_n,z^{(n)}|\theta)p(z^{(n)}|x_n,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z^{(i)}}logp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\end{split}\]</span></p><p><strong>结论：</strong></p><p>通过简化后，</p><p><span class="math display">\[\begin{split}&amp;Q(\theta,\theta^{(t)}) \\&amp;= \sum_Zlog\prod_{i=1}^n p(x_i,z^{(i)}|\theta)\prod_{i=1}^np(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z^{(i)}}logp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\end{split}\]</span></p><h3 id="做一些替换">做一些替换</h3><blockquote><p>在说明高斯混合模型的概率密度函数时，我们有</p><p><span class="math inline">\(p(x) = \sum_Zp(x,z) = \sum_{k=1}^K\alpha_kN(x|\mu_k,\Sigma_k)\)</span> ,</p><p>这里<spanclass="math inline">\(\alpha_k\)</span>代表样本x属于第k个高斯分布的概率。也可以描述为样本x的隐变量z的<spanclass="math inline">\(p_k\)</span>。</p></blockquote><p><span class="math inline">\(p(x_i,z_j^{(i)}|\theta)\)</span>说明：</p><ul><li><p><span class="math inline">\(x_i\)</span> :第i个样本</p></li><li><p><spanclass="math inline">\(z^{(i)}\)</span>:第i个样本的隐变量，前面说过每个变量都有自己的隐变量。</p></li><li><p><spanclass="math inline">\(z^{(i)}_j\)</span>:第i个样本的隐变量的第j个分类，于是<spanclass="math inline">\(P(z^{(i)}_j)\)</span>表示第i个样本属于第j个高斯分布的概率。<spanclass="math inline">\(P(z^{(i)}_j)\)</span> 又可表示为<spanclass="math inline">\(\alpha^{(i)}_j\)</span></p></li><li><p><span class="math inline">\(\theta = \{\alpha,\mu,\Sigma\}\)</span></p></li></ul><p><strong>对<spanclass="math inline">\(p(x_i,z_j^{(i)}|\theta)\)</span>展开:</strong></p><p>    <span class="math inline">\(p(x_i,z_j^{(i)}|\theta) =\alpha_j^{(i)}N(x|\mu_j,\Sigma_j)\)</span></p><p><strong>对<spanclass="math inline">\(p(z_j^{(i)}|x_i,\theta^{(t)})\)</span>展开:</strong></p><blockquote><p><span class="math inline">\(P(B|A) =\frac{P(A,B)}{P(A)}\)</span>   </p></blockquote><p><span class="math display">\[\begin{split}p(z_j^{(i)}|x_i,\theta^{(t)}) &amp;=\frac{p(x_i,z_j^{(i)}|\theta^{(t)})}{p(x_i)}\\&amp;=\frac{\alpha_j^{(i),(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}{\sum_{k=1}^K\alpha_k^{(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}\end{split}\]</span></p><p>于是：</p><p><span class="math display">\[\begin{split}&amp;Q(\theta,\theta^{(t)})\\&amp;=\sum_{i=1}^n\sum_{z^{(i)}}logp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{z^{(i)}}\sum_{i=1}^nlogp(x_i,z^{(i)}|\theta)p(z^{(i)}|x_i,\theta^{(t)})\\&amp;=\sum_{k=1}^K\sum_{i=1}^nlog[\alpha_kN(x_i|\mu_k,\Sigma_k)]p(z^{(i)}=c_k|x_i,\theta^{(t)})\\&amp;=\sum_{k=1}^K\sum_{i=1}^n[log\alpha_k+logN(x_i|\mu_k,\Sigma_k)]p(z^{(i)}=c_k|x_i,\theta^{(t)})\end{split}\]</span></p><p>为了后续运算方便书写，暂时不展开后面那一项，并且我们令后验概率：</p><p><span class="math inline">\(p(z^{(i)}=c_k|x_i,\theta^{(t)}) =\gamma_t(z^{(i)}_k)\)</span></p><blockquote><p>说明：这个公式里的 <span class="math inline">\(\alpha_k\)</span>实际上是<span class="math inline">\(\alpha^{(i)}_k\)</span> ；后项<spanclass="math inline">\(p(z^{(i)}=c_k|x_i,\theta^{(t)})\)</span> 就是<spanclass="math inline">\(p(z^{(i)}_k|x_i,\theta^{(t)})\)</span>,换了个写法而已，上面已经写过了关于它的展开。</p></blockquote><h2 id="m-step">M-step</h2><p>    表示出Q函数后，M-step需要求的是：</p><p>    <span class="math inline">\(\theta^{(t+1)} = argmax_\thetaQ(\theta,\theta^{(t)})\)</span></p><p>    这里要求的<span class="math inline">\(\theta\)</span>一共有<spanclass="math inline">\(\alpha,\mu,\Sigma\)</span> ，只例举<spanclass="math inline">\(\alpha\)</span> 的求法，<spanclass="math inline">\(\mu,\Sigma\)</span> 将会直接给出结果。</p><p>    Q函数前项里与<span class="math inline">\(\alpha\)</span>相关的只有<span class="math inline">\(log\alpha\)</span>,于是仅对于<spanclass="math inline">\(\alpha\)</span>而言有<strong>约束优化问题</strong>：</p><p>    <span class="math inline">\(\theta^{(t+1)} = argmax_\theta\sum_{k=1}^K\sum_{i=1}^nlog\alpha_k\gamma_t(z^{(i)}_k)\)</span></p><p>    同时有限制条件：<span class="math inline">\(\sum_{k=1}^K\alpha_k= 1\)</span></p><h3 id="拉格朗日乘子法求解">拉格朗日乘子法求解</h3><p>    约束优化问题用拉格朗日乘子法求解，于是有拉格朗日函数：</p><p>    <span class="math inline">\(\ell(\alpha,\lambda) =\sum_{k=1}^K\sum_{i=1}^nlog\alpha_k*\gamma_t(z^{(i)}_k)+\lambda(\sum_{k=1}^K\alpha_k-1)\)</span></p><p>    对<span class="math inline">\(\alpha_k\)</span>求偏导,令其为0：</p><p>    <spanclass="math inline">\(\frac{\partial(\ell(\alpha,\lambda))}{\partial(\alpha_k)}= \sum_{i=1}^n\frac{1}{\alpha_k}\gamma_t(z^{(i)}_k)+\lambda =0\)</span></p><p>    两边同时乘分母，</p><p>    <spanclass="math inline">\(\sum_{i=1}^n\gamma_t(z^{(i)}_k)+\alpha_k\lambda =0~~~~~~~~~~~~~~(1)\)</span></p><p>    把所有<span class="math inline">\(\alpha_k\)</span>相加：</p><p>    <spanclass="math inline">\(\sum_{i=1}^n\sum_{k=1}^K\gamma_t(z^{(i)}_k)+\sum_{k=1}^K\alpha_k\lambda= 0\)</span></p><p>    又因为<spanclass="math inline">\(\gamma_t\)</span>为概率分布，<spanclass="math inline">\(\alpha\)</span> 为概率分布，有</p><p>    <span class="math inline">\(\sum_{k=1}^K\gamma_t(z^{(i)}_k) =1;\sum_{k=1}^K\alpha_k = 1\)</span></p><p>    所以化简可得：</p><p>    <span class="math inline">\(N+\lambda = 0\\\lambda =-n\)</span></p><p>    将<span class="math inline">\(\lambda =-n\)</span>代入（1）式中，可得<spanclass="math inline">\(\alpha_k\)</span>：</p><p>    <span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p>    而<span class="math inline">\(\alpha^{(t+1)} =\{a_1^{(t+1)},a_2^{(t+1)},...,a_k^{(t+1)}\}\)</span></p><h3 id="另外两个参数">另外两个参数</h3><p>    这里就不计算了，直接给结果吧。</p><p>    <span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p>    <span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><h2 id="再看e-step">再看E-step</h2><p>    其实刚才写E-step时并没有写E-step具体要求什么，现在我们推出了<spanclass="math inline">\(\theta^{(t+1)}\)</span>,发现只要求出<strong>隐变量z的后验分布</strong>的，那就都可以算出来。所以在E-step,实质上只需要求隐变量的后验概率分布<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>即可。这在上一篇EM算法推导中也说明过。精确到每一个后验概率：</p><p><span class="math display">\[\begin{split}\gamma_t(z^{(i)}_k) &amp;= p(z^{(i)}=c_k|x_i,\theta^{(t)})\\&amp;=\frac{\alpha_k^{(i),(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}\end{split}\]</span></p><h1 id="gmm总结">GMM总结</h1><h2 id="gmm聚类流程">GMM聚类流程</h2><p><strong>step1：</strong></p><p>    定义高斯分布个数K，对每个高斯分布设置初始参数值<spanclass="math inline">\(\theta^{(0)}_k = \alpha_k,\mu_k,\Sigma_k\)</span>。<strong>一般第一步不会自己设置初始值，而是通过K-mean算法计算初始值。</strong></p><p><strong>step2 E-step：</strong></p><p>    根据当前的参数<span class="math inline">\(\theta^{(t)}\)</span>,计算每一个隐变量的后验概率分布<spanclass="math inline">\(\gamma_t(z^{(i)})\)</span>。精确到每一个后验概率的计算，有</p><p>    <span class="math inline">\(\gamma_t(z_k^{(i)}) =\frac{\alpha_k^{(i),(t)}N(x|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}N(x|\mu_j^{(t)},\Sigma_j^{(t)})}\)</span></p><p><strong>step3 M-step：</strong></p><p>    根据E-step计算出的隐变量后验概率分布，进一步计算新的<spanclass="math inline">\(\theta^{(t+1)}\)</span></p><p>    <span class="math inline">\(\alpha_k^{(t+1)} =\frac{1}{N}\sum_{i=1}^n\gamma_t(z^{(i)}_k)\)</span></p><p>    <span class="math inline">\(\mu_k^{(t+1)} =\frac{\sum_{i=1}^n[\gamma_t(z^{(i)}_k)x_i]}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p>    <span class="math inline">\(\Sigma_k^{(t+1)} =\frac{\sum_{i=1}^n\gamma_t(z^{(i)}_k)(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{i=1}^n\gamma_t(z^{(i)}_k)}\)</span></p><p><strong>step4:</strong> 循环E-step和M-step直至收敛。</p><h2 id="gmm优缺点">GMM优缺点</h2><p><strong>优点：</strong></p><ul><li><p>GMM使用均值和标准差，簇可以呈现出椭圆形，优于k-means的圆形</p></li><li><p>GMM是使用概率，故一个数据点可以属于多个簇</p></li></ul><p><strong>缺点：</strong></p><p>    同EM算法缺点。</p><h2 id="gmm的实现与应用">GMM的实现与应用</h2><p>    将在下一篇文章进行GMM的具体实现和应用。</p><blockquote><p>    文章链接：待更新</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
            <tag> 聚类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo数学公式显示问题</title>
      <link href="/article/1d3b342ddcd7/"/>
      <url>/article/1d3b342ddcd7/</url>
      
        <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>周末在搬迁Hexo博客并且修复之前博客的bug，外加继续装修博客。整了一个周末终于弄好了。    </p><p>总结一下遇到的最让我无语而且耗时最长的问题吧。</p><p>Latex数学公式+公式无法换行。首先呢这是两个问题，要先让hexo支持数学公式，再来才是不能换行的问题。自己找了许许多多的教程，先后折磨了快半天，才找到了较为完美的解决方案。可能是关于这方面的教程大多都太老了，版本迭代后不太适用吧。</p><h1 id="关于hexo和主题">关于Hexo和主题</h1><p>这个问题实际<strong>与主题无关</strong>。最开始我一直纠结于主题不同解决方法是不是不一样，最后解决了发现不是。</p><p>我用的是hexo和butterfly截止目前的最新版本：</p><p>hexo：6.3.0</p><p>butterfly：4.4.0</p><h1 id="让hexo支持数学公式">让Hexo支持数学公式</h1><p>这部分我跟着这篇文章搞的：<ahref="https://blog.csdn.net/gorray/article/details/122398901">Hexo如何显示latex公式_gorray的博客-CSDN博客_hexolatex公式</a></p><p>其实要做的步骤很少：</p><p>1.首先卸载hexo-math和hexo-renderer-marked。然而hexo应该是没有自带hexo-math的，所以只需要卸载第二个就行。以防万一还是可以直接执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-math</span><br><span class="line">npm un hexo-renderer-marked</span><br></pre></td></tr></table></figure><p>2.安装hexo-renderer-pandoc渲染器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure><p>好，到此为止，需要的包就迭代好了。</p><p>3.然后是配置主题配置下的mathjax设置。我用的是butterfly，那么对应路径是： _config.yml</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h1 id="pandoc报错">Pandoc报错</h1><p>理论上到这一步就可以用了对吧，一般的教程也都这样。就算再外加一步，我学习的那篇文章里提到，接下来还应该去Pandoc官网下载<strong>最新版本</strong>pandoc：<ahref="https://pandoc.org/index.html">Pandoc - About pandoc</a></p><p>关于pandoc下载安装教程随便查一下就有，这里就不说明了。然后再在环境配置配置了pandoc路径，直到你可以在cmd输入以下命令查看它的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandoc -v</span><br></pre></td></tr></table></figure><p>我的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pandoc 2.19.2</span><br><span class="line">Compiled with pandoc-types 1.22.2.1, texmath 0.12.5.2, skylighting 0.13,</span><br><span class="line">citeproc 0.8.0.1, ipynb 0.2, hslua 2.2.1</span><br><span class="line">Scripting engine: Lua 5.4</span><br></pre></td></tr></table></figure><p>有的朋友可能到这一步就发现不对了，不要慌，接下来才是重点。</p><h2 id="一个莫名其妙的错误">一个莫名其妙的错误</h2><p>先回到hexo目录，执行hexo -s,如果你没有出现这个报错：</p><p><span class="math inline">\(\color{red}{pandoc~exited ~with ~code ~9:pandoc: Unknown~extension:~smart}\)</span></p><p>那么恭喜你，你的这个问题并不存在，可以选择跳过。但是如果你和我一样报这个错误，可能就开始头疼了。不过我终于还是找到了解决方法。</p><p>首先我是找到了这篇文章：<ahref="https://www.cnblogs.com/diralpo/p/12542450.html">配置hexo时遇到的问题- diralpo - 博客园 (cnblogs.com)</a></p><p>从这篇文章得知，导致该报错的原因是<strong>pandoc版本过低</strong>，而且还不是一般原因引起的版本过低，因为前面我们已经安装了最新版本的pandoc。但是最新版本的没起作用。于是我打开了everything查找电脑上存在的pandoc。然后发现位于Anaconda，真正问题也出在这儿。</p><p><strong>是因为Anaconda安装的pandoc版本过低，而且hexo默认使用的是Anaconda的pandoc。</strong></p><p>不信的话你去找找，那个pandoc居然是2017年的。在某一篇文章得知，pandoc版本应该在2.0以上，但那个pandoc好像是1.9。那接下来的就简单了，直接把新下载的pandoc.exe替换Anaconda里的pandoc.exe。</p><p><img src="https://img.issey.top/img/202209182134541.png" /></p><p>然后你在回去hexo -s，就没问题了。</p><h1 id="换行问题">换行问题</h1><p>其实做到上一步，换行问题也已经随之解决了。不过这里还是提一下关于这个换行。</p><p>首先，想直接通过 ，end这种写法是做不到换行的，我最开始就是纠结于这个，然而这写法本来也不规范，例如：A  B是不能达到换行的。但是在加上规范的begin和end就可以了。其次，换行公式应该写成行间公式而非行内公式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$A \\ B$  错误写法</span><br><span class="line">\begin&#123;split&#125; A\\B \end&#123;split&#125; 写在行内错误，写在行间正确。</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{split} A\\B \end{split}\]</span></p><p>这篇文章是我做的hexo公式测试：<ahref="https://www.issey.top/article/1365bcc580cd/">Latex公式测试 |issey的博客</a></p><p>如果不是网速加载问题，那么显示应该是：</p><p>    <img src="https://img.issey.top/img/202209182149084.png" /></p>]]></content>
      
      
      <categories>
          
          <category> hexo相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码测试</title>
      <link href="/article/cb3b152854a1/"/>
      <url>/article/cb3b152854a1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    original_img = plt.imread(<span class="string">&#x27;color.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shape of original_img is:&quot;</span>, original_img.shape)</span><br><span class="line">    original_img /= <span class="number">255</span></span><br><span class="line">    X_img = np.reshape(original_img, (original_img.shape[<span class="number">0</span>] * original_img.shape[<span class="number">1</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    K = <span class="number">8</span></span><br><span class="line">    model = KMeans(n_clusters=K)</span><br><span class="line">    model.fit(X_img)</span><br><span class="line">    centroids = model.cluster_centers_</span><br><span class="line">    <span class="comment"># labels得到的是质心索引</span></span><br><span class="line">    labels = model.predict(X_img)</span><br><span class="line">    <span class="comment"># print(labels[:6])</span></span><br><span class="line">    <span class="comment"># 替换样本</span></span><br><span class="line">    X_recovered = centroids[labels]</span><br><span class="line">    X_recovered = np.reshape(X_recovered, original_img.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(X_recovered*<span class="number">255</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hexo相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex公式测试</title>
      <link href="/article/1365bcc580cd/"/>
      <url>/article/1365bcc580cd/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[\mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix}\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\\frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp;0 \\\frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp;0 \\\end{vmatrix}\]</span></p><p>多行对齐：</p><p><span class="math display">\[\begin{gather}\begin{split}Adv^{Fed}&amp; = Pr^{Fed}\left ( A=1\mid x\in D_{T} \right ) - Pr^{Fed}\left (A=1\mid x\in D_{N} \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( P\left ( A=1\mid x \right) \right )-\underset{x\in D_{N}}{E^{Fed}}\left ( P\left ( A=1\mid x\right ) \right ) \\&amp; = \underset{x\in D_{T}}{E^{Fed}}\left ( 1-\frac{L\left ( \left (x,y \right ),F \right )}{A} \right )-\underset{x\in D_{N}}{E^{Fed}}\left( 1-\frac{L\left ( \left ( x,y \right ),F \right )}{A} \right )\\&amp; = \frac{1}{A}\cdot \left [ \underset{x\in D_{N}}{E^{Fed}}\left (L\left ( \left ( x,y \right ),F \right )\right )-\underset{x\inD_{T}}{E^{Fed}}\left ( L\left ( \left ( x,y \right ),F \right ) \right )\right ] \\\end{split}\end{gather}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> hexo相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记7——决策树原理与应用</title>
      <link href="/article/2f3e74c9632f/"/>
      <url>/article/2f3e74c9632f/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：关于决策树的含义这里就不详细解释了，简单来说决策树就是根据数据集的最优特征构造一棵树，再用这棵树来预测新的数据属于哪一类。决策树是一种基本的分类算法。（实际上也可以用于回归，这里不做讨论）</p><p>参考文章：<ahref="https://blog.csdn.net/jiaoyangwm/article/details/79525237">机器学习实战（三）——决策树_呆呆的猫的博客-CSDN博客_决策树</a></p><p><ahref="https://blog.csdn.net/fuqiuai/article/details/79456971?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166202030616782425158024%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=166202030616782425158024&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79456971-null-null.142%5Ev44%5Econtrol&amp;utm_term=C4.5&amp;spm=1018.2226.3001.4187">数据挖掘领域十大经典算法之—C4.5算法（超详细附代码）_fuqiuai的博客-CSDN博客_c4.5</a></p></blockquote><h1 id="构造决策树的算法">构造决策树的算法</h1><p>一般构建决策树的算法有三种：ID3，C4.5，CART。此三种算法的区别在于：</p><ul><li><p>ID3：特征划分基于信息增益。ID3 仅仅适用于分类问题；ID3仅仅能够处理离散属性。</p></li><li><p>C4.5：特征划分基于信息增益比。可以处理连续值。</p></li><li><p>CART：特征划分基于基尼系数。并且CART算法只能构造出二叉树。CART既可以用于分类问题，也可以用于回归问题。</p></li></ul><h1 id="信息增益与信息增益比">信息增益与信息增益比</h1><h2 id="熵">熵</h2><p>简单来说，在这里熵是用来衡量数据集纯净度的。熵的值越小，代表数据集纯净度越高。反之，熵越大代表数据集越混乱。</p><p>熵的计算公式为：</p><p><span class="math inline">\(H =-\sum_{i=1}^np(x_i)log_2p(x_i)\)</span></p><p><span class="math inline">\(x_i\)</span>为第i个分类，<spanclass="math inline">\(p(x_i)\)</span>为选择该分类的概率。</p><h2 id="经验熵">经验熵</h2><p>当上诉的<spanclass="math inline">\(p(x_i)\)</span>（即选则某一类的概率）是由数据估计（特别是最大似然估计）得到时，此时的熵被称为经验熵。举个例子：</p><table><thead><tr class="header"><th style="text-align: center;">类别</th><th style="text-align: center;">个数</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">A</td><td style="text-align: center;">4</td></tr><tr class="even"><td style="text-align: center;">B</td><td style="text-align: center;">6</td></tr></tbody></table><p>如果<span class="math inline">\(p(A) = \frac 4 {10},P(B) = \frac 6{10}\)</span>,那么此时算出的熵就是经验熵。</p><p>经验熵的公式可以写为：</p><p><span class="math inline">\(H(D) = -\sum_{k=1}^n \frac{|c_k|}{|D|}log_2\frac {|c_k|}{|D|}\)</span></p><p>使用上式可算出例中的经验熵<spanclass="math inline">\(H(D)\)</span>为：</p><p>$H(D) = -log_2-log_2 = 0.971 $</p><h2 id="条件熵">条件熵</h2><p>条件熵<spanclass="math inline">\(H(Y|X)\)</span>表示在已知随机变量<spanclass="math inline">\(X\)</span>的条件下，随机变量<spanclass="math inline">\(Y\)</span>的不确定性。</p><p>公式：</p><p><span class="math display">\[\begin{split} H(Y|X) &amp;= \sum_{i=1}^nP(x_i)H(Y|X = x_i)\\&amp;=-\sum_{i=1}^nP(x_i)\sum_{j=1}^mP(y_i|x_i)log_2P(y_i|x_i)\\&amp;=-\sum_{i=1}^n\sum_{j=1}^mP(x_i,y_j)log_2P(y_j|x_i)   \end{split}\]</span></p><p>这个条件熵，可能最开始不太好理解，建议参考这篇文章，里面有详细的说明和例题，这里就不举例了。<ahref="https://blog.csdn.net/xwd18280820053/article/details/70739368">通俗理解条件熵_AI_盲的博客-CSDN博客_条件熵</a></p><p>经验熵对应的条件熵为<strong>经验条件熵</strong>，一般我们计算的都是经验条件熵。<strong>注：经验条件熵必须会自己手算</strong>。</p><p>特别的，令<span class="math inline">\(0log_20 = 0\)</span>。</p><h2 id="信息增益">信息增益</h2><p>信息增益指知道了某个条件后，事件的不确定性的下降程度。特征A对训练集D的信息增益<spanclass="math inline">\(g(D,A)\)</span>被定义为集合D的经验熵<spanclass="math inline">\(H(D)\)</span>与给定特征A的条件下的经验条件熵<spanclass="math inline">\(H(D|A)\)</span>之差：</p><p><span class="math inline">\(G(D,A)=H(D)-H(D|A)\)</span></p><p>如果一个特征的信息增益越大，说明使用这个特征划分后的样本集可以拥有更好的纯净度。所以在ID3算法中，每一轮我们会计算每个未使用特征的信息增益，然后选择信息增益最大的那个特征作为划分节点。但是这样会有个问题，假设每个属性中每种类别都只有一个样本，那这样属性信息熵就等于零，根据信息增益就无法选择出有效分类特征。</p><p>所以提出了信息增益率作为标准的C4.5算法。</p><h2 id="信息增益率">信息增益率</h2><p>信息增益率<spanclass="math inline">\(GainRatio(D,A)\)</span>由信息增益<spanclass="math inline">\(G(D,A)\)</span>和分裂信息度量<spanclass="math inline">\(SplitInformation(D,A)\)</span>共同定义，公式如下：</p><p><span class="math inline">\(GainRatio(D,A) =\frac{G(D,A)}{SplitInformation(D,A)}\)</span></p><p><span class="math inline">\(SplitInformation(D,A) =-\sum_{i=1}^n\frac{|S_i|}{|S|}log_2\frac{|S_i|}{S}\)</span></p><p>也有人把信息增益率的公式写为：</p><p><span class="math inline">\(GainRatio(D,A) =\frac{G(D,A)}{H(D)}\)</span>，<spanclass="math inline">\(H(D)\)</span>为训练集D的经验熵。</p><p>需要注意的是，增益率准则对可取值数目较少的属性所有偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p><h2 id="关于cart算法和基尼系数">关于CART算法和基尼系数</h2><p>不在这里进行说明，之后可能会单独开一篇文章。</p><h1 id="决策树的构建">决策树的构建</h1><p>上面已经介绍了构建决策树需要用到的子模块，即经验熵的计算和最优特征的选择。接下来介绍决策树的整体构建步骤。</p><p>构建决策树的核心都是递归算法，结束递归的条件又多种，将在之后的决策树剪枝中介绍。</p><h2 id="id3算法">ID3算法</h2><p>核心是在各个节点上对应信息增益准则选择特征，递归构建决策树。</p><p>具体方法：</p><ol type="1"><li><p>从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。</p></li><li><p>由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到达到预剪枝条件或没有特征可以选择为止。</p></li><li><p>得到一个决策树。</p></li></ol><h2 id="c4.5算法">C4.5算法</h2><p>与ID3算法的区别在于将信息增益率作为选择特征的标准。</p><h1 id="决策树的剪枝">决策树的剪枝</h1><p>如果不进行剪枝，那么决策树生成算法递归地产生决策树，直到没有特征选择为止。这样生成的决策树往往会导致<strong>过拟合</strong>。过拟合在前面的文章已经解释过，这里不再进行说明。为了防止过拟合，我们需要对决策树进行剪枝。</p><p>决策树的剪枝分为预剪枝和后剪枝两大类。</p><h2 id="预剪枝">预剪枝</h2><p>预剪枝指在生成决策树的过程中提前停止树的生长。常用的预剪枝有以下几种：</p><ol type="1"><li><p>当树到达一定深度的时候，停止树的生长。</p></li><li><p>当信息增益，增益率和基尼指数增益小于某个阈值的时候不在生长。</p></li><li><p>达到当前节点的样本数量小于某个阈值的时候。</p></li><li><p>计算每次分裂对测试集的准确性提升，当小于某个阈值，或不再提升甚至有所下降时，停止生长。</p></li></ol><p>优缺点：</p><p>优点：思想简单，算法高效，采用了贪心的思想，适合大规模问题。<br />缺点：提前停止生长，有可能存在欠拟合的风险。</p><h2 id="后剪枝">后剪枝</h2><p>后剪枝是先从训练集生成一颗完整的决策树，然后自底向上的对决策树进行剪枝。后剪枝一般不常用，这里暂时不拓展。贴一个连接：<ahref="https://blog.csdn.net/zr1213159840/article/details/112334211">西瓜书决策树预剪枝后剪枝过程详解_铁锤2号的博客-CSDN博客</a></p><p>缺点：耗时太长。</p><h1 id="利用sklearn实现决策树">利用sklearn实现决策树</h1><blockquote><p>关于决策树的手动实现：<ahref="https://blog.csdn.net/jiaoyangwm/article/details/79525237">机器学习实战（三）——决策树_呆呆的猫的博客-CSDN博客_决策树</a></p><p>这篇文章写的十分完整，甚至包括决策树的储存与绘图，这里就不贴手动实现的代码了，因为感觉贴了也和他的长得几乎一样？并且文章中还提到了sklearn决策树的各种参数说明，可以说是十分详尽了。不过他代码中注释写错了一些，自己跟着走一遍就看得懂了。</p></blockquote><h2 id="导入并分割数据集">导入并分割数据集</h2><p>数据集下载：<ahref="https://www.kaggle.com/datasets/brynja/wineuci">Classifying winevarieties | Kaggle</a></p><p>这个数据集sklearn其实已经内置了，只需要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line">wine = load_wine()</span><br><span class="line"><span class="built_in">print</span>(wine.data) </span><br><span class="line"><span class="built_in">print</span>(wine.target) <span class="comment"># 分类</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.423e+01</span> <span class="number">1.710e+00</span> <span class="number">2.430e+00</span> ... <span class="number">1.040e+00</span> <span class="number">3.920e+00</span> <span class="number">1.065e+03</span>]</span><br><span class="line"> [<span class="number">1.320e+01</span> <span class="number">1.780e+00</span> <span class="number">2.140e+00</span> ... <span class="number">1.050e+00</span> <span class="number">3.400e+00</span> <span class="number">1.050e+03</span>]</span><br><span class="line"> [<span class="number">1.316e+01</span> <span class="number">2.360e+00</span> <span class="number">2.670e+00</span> ... <span class="number">1.030e+00</span> <span class="number">3.170e+00</span> <span class="number">1.185e+03</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">1.327e+01</span> <span class="number">4.280e+00</span> <span class="number">2.260e+00</span> ... <span class="number">5.900e-01</span> <span class="number">1.560e+00</span> <span class="number">8.350e+02</span>]</span><br><span class="line"> [<span class="number">1.317e+01</span> <span class="number">2.590e+00</span> <span class="number">2.370e+00</span> ... <span class="number">6.000e-01</span> <span class="number">1.620e+00</span> <span class="number">8.400e+02</span>]</span><br><span class="line"> [<span class="number">1.413e+01</span> <span class="number">4.100e+00</span> <span class="number">2.740e+00</span> ... <span class="number">6.100e-01</span> <span class="number">1.600e+00</span> <span class="number">5.600e+02</span>]]</span><br><span class="line">[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>可以更直观的展示数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更直观的展示</span></span><br><span class="line"><span class="built_in">print</span>(pd.concat([pd.DataFrame(wine.data), pd.DataFrame(wine.target)], axis=<span class="number">1</span>)) <span class="comment"># 按列拼接</span></span><br><span class="line"><span class="built_in">print</span>(wine.feature_names) <span class="comment"># 特征名称</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">        <span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>      <span class="number">4</span>     <span class="number">5</span>   ...    <span class="number">8</span>      <span class="number">9</span>     <span class="number">10</span>    <span class="number">11</span>      <span class="number">12</span>  <span class="number">0</span> </span><br><span class="line"><span class="number">0</span>    <span class="number">14.23</span>  <span class="number">1.71</span>  <span class="number">2.43</span>  <span class="number">15.6</span>  <span class="number">127.0</span>  <span class="number">2.80</span>  ...  <span class="number">2.29</span>   <span class="number">5.64</span>  <span class="number">1.04</span>  <span class="number">3.92</span>  <span class="number">1065.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">13.20</span>  <span class="number">1.78</span>  <span class="number">2.14</span>  <span class="number">11.2</span>  <span class="number">100.0</span>  <span class="number">2.65</span>  ...  <span class="number">1.28</span>   <span class="number">4.38</span>  <span class="number">1.05</span>  <span class="number">3.40</span>  <span class="number">1050.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">13.16</span>  <span class="number">2.36</span>  <span class="number">2.67</span>  <span class="number">18.6</span>  <span class="number">101.0</span>  <span class="number">2.80</span>  ...  <span class="number">2.81</span>   <span class="number">5.68</span>  <span class="number">1.03</span>  <span class="number">3.17</span>  <span class="number">1185.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">14.37</span>  <span class="number">1.95</span>  <span class="number">2.50</span>  <span class="number">16.8</span>  <span class="number">113.0</span>  <span class="number">3.85</span>  ...  <span class="number">2.18</span>   <span class="number">7.80</span>  <span class="number">0.86</span>  <span class="number">3.45</span>  <span class="number">1480.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">4</span>    <span class="number">13.24</span>  <span class="number">2.59</span>  <span class="number">2.87</span>  <span class="number">21.0</span>  <span class="number">118.0</span>  <span class="number">2.80</span>  ...  <span class="number">1.82</span>   <span class="number">4.32</span>  <span class="number">1.04</span>  <span class="number">2.93</span>   <span class="number">735.0</span>   <span class="number">0</span></span><br><span class="line">..     ...   ...   ...   ...    ...   ...  ...   ...    ...   ...   ...     ...  ..</span><br><span class="line"><span class="number">173</span>  <span class="number">13.71</span>  <span class="number">5.65</span>  <span class="number">2.45</span>  <span class="number">20.5</span>   <span class="number">95.0</span>  <span class="number">1.68</span>  ...  <span class="number">1.06</span>   <span class="number">7.70</span>  <span class="number">0.64</span>  <span class="number">1.74</span>   <span class="number">740.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">174</span>  <span class="number">13.40</span>  <span class="number">3.91</span>  <span class="number">2.48</span>  <span class="number">23.0</span>  <span class="number">102.0</span>  <span class="number">1.80</span>  ...  <span class="number">1.41</span>   <span class="number">7.30</span>  <span class="number">0.70</span>  <span class="number">1.56</span>   <span class="number">750.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">175</span>  <span class="number">13.27</span>  <span class="number">4.28</span>  <span class="number">2.26</span>  <span class="number">20.0</span>  <span class="number">120.0</span>  <span class="number">1.59</span>  ...  <span class="number">1.35</span>  <span class="number">10.20</span>  <span class="number">0.59</span>  <span class="number">1.56</span>   <span class="number">835.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">176</span>  <span class="number">13.17</span>  <span class="number">2.59</span>  <span class="number">2.37</span>  <span class="number">20.0</span>  <span class="number">120.0</span>  <span class="number">1.65</span>  ...  <span class="number">1.46</span>   <span class="number">9.30</span>  <span class="number">0.60</span>  <span class="number">1.62</span>   <span class="number">840.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">177</span>  <span class="number">14.13</span>  <span class="number">4.10</span>  <span class="number">2.74</span>  <span class="number">24.5</span>   <span class="number">96.0</span>  <span class="number">2.05</span>  ...  <span class="number">1.35</span>   <span class="number">9.20</span>  <span class="number">0.61</span>  <span class="number">1.60</span>   <span class="number">560.0</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">[<span class="number">178</span> rows x <span class="number">14</span> columns]</span><br><span class="line">[<span class="string">&#x27;alcohol&#x27;</span>, <span class="string">&#x27;malic_acid&#x27;</span>, <span class="string">&#x27;ash&#x27;</span>, <span class="string">&#x27;alcalinity_of_ash&#x27;</span>, <span class="string">&#x27;magnesium&#x27;</span>, <span class="string">&#x27;total_phenols&#x27;</span>, <span class="string">&#x27;flavanoids&#x27;</span>, <span class="string">&#x27;nonflavanoid_phenols&#x27;</span>, <span class="string">&#x27;proanthocyanins&#x27;</span>, <span class="string">&#x27;color_intensity&#x27;</span>, <span class="string">&#x27;hue&#x27;</span>, <span class="string">&#x27;od280/od315_of_diluted_wines&#x27;</span>, <span class="string">&#x27;proline&#x27;</span>]</span><br></pre></td></tr></table></figure><p>分割数据集为训练集和测试集（7：3）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test,y_train,y_test = train_test_split(wine.data,wine.target,train_size=<span class="number">0.7</span>)</span><br><span class="line"><span class="built_in">print</span>(pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">        <span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>      <span class="number">4</span>     <span class="number">5</span>   ...    <span class="number">8</span>     <span class="number">9</span>     <span class="number">10</span>    <span class="number">11</span>      <span class="number">12</span>  <span class="number">0</span> </span><br><span class="line"><span class="number">0</span>    <span class="number">12.08</span>  <span class="number">1.83</span>  <span class="number">2.32</span>  <span class="number">18.5</span>   <span class="number">81.0</span>  <span class="number">1.60</span>  ...  <span class="number">1.64</span>  <span class="number">2.40</span>  <span class="number">1.08</span>  <span class="number">2.27</span>   <span class="number">480.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">1</span>    <span class="number">11.82</span>  <span class="number">1.72</span>  <span class="number">1.88</span>  <span class="number">19.5</span>   <span class="number">86.0</span>  <span class="number">2.50</span>  ...  <span class="number">1.42</span>  <span class="number">2.06</span>  <span class="number">0.94</span>  <span class="number">2.44</span>   <span class="number">415.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">2</span>    <span class="number">13.72</span>  <span class="number">1.43</span>  <span class="number">2.50</span>  <span class="number">16.7</span>  <span class="number">108.0</span>  <span class="number">3.40</span>  ...  <span class="number">2.04</span>  <span class="number">6.80</span>  <span class="number">0.89</span>  <span class="number">2.87</span>  <span class="number">1285.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">13.49</span>  <span class="number">3.59</span>  <span class="number">2.19</span>  <span class="number">19.5</span>   <span class="number">88.0</span>  <span class="number">1.62</span>  ...  <span class="number">0.88</span>  <span class="number">5.70</span>  <span class="number">0.81</span>  <span class="number">1.82</span>   <span class="number">580.0</span>   <span class="number">2</span></span><br><span class="line"><span class="number">4</span>    <span class="number">13.05</span>  <span class="number">2.05</span>  <span class="number">3.22</span>  <span class="number">25.0</span>  <span class="number">124.0</span>  <span class="number">2.63</span>  ...  <span class="number">1.92</span>  <span class="number">3.58</span>  <span class="number">1.13</span>  <span class="number">3.20</span>   <span class="number">830.0</span>   <span class="number">0</span></span><br><span class="line">..     ...   ...   ...   ...    ...   ...  ...   ...   ...   ...   ...     ...  ..</span><br><span class="line"><span class="number">119</span>  <span class="number">12.42</span>  <span class="number">4.43</span>  <span class="number">2.73</span>  <span class="number">26.5</span>  <span class="number">102.0</span>  <span class="number">2.20</span>  ...  <span class="number">1.71</span>  <span class="number">2.08</span>  <span class="number">0.92</span>  <span class="number">3.12</span>   <span class="number">365.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">120</span>  <span class="number">14.22</span>  <span class="number">1.70</span>  <span class="number">2.30</span>  <span class="number">16.3</span>  <span class="number">118.0</span>  <span class="number">3.20</span>  ...  <span class="number">2.03</span>  <span class="number">6.38</span>  <span class="number">0.94</span>  <span class="number">3.31</span>   <span class="number">970.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">121</span>  <span class="number">13.16</span>  <span class="number">2.36</span>  <span class="number">2.67</span>  <span class="number">18.6</span>  <span class="number">101.0</span>  <span class="number">2.80</span>  ...  <span class="number">2.81</span>  <span class="number">5.68</span>  <span class="number">1.03</span>  <span class="number">3.17</span>  <span class="number">1185.0</span>   <span class="number">0</span></span><br><span class="line"><span class="number">122</span>  <span class="number">11.84</span>  <span class="number">2.89</span>  <span class="number">2.23</span>  <span class="number">18.0</span>  <span class="number">112.0</span>  <span class="number">1.72</span>  ...  <span class="number">0.95</span>  <span class="number">2.65</span>  <span class="number">0.96</span>  <span class="number">2.52</span>   <span class="number">500.0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">123</span>  <span class="number">12.86</span>  <span class="number">1.35</span>  <span class="number">2.32</span>  <span class="number">18.0</span>  <span class="number">122.0</span>  <span class="number">1.51</span>  ...  <span class="number">0.94</span>  <span class="number">4.10</span>  <span class="number">0.76</span>  <span class="number">1.29</span>   <span class="number">630.0</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">[<span class="number">124</span> rows x <span class="number">14</span> columns]</span><br></pre></td></tr></table></figure><p>决策树算法一般不需要标准化，所以这里省去了标准化。</p><h2 id="建立并训练决策树">建立并训练决策树</h2><p>sklearn并没有实现C4.5算法，所以这里用的ID3,所以要用C4.5算法还是需要手动实现。sklearn默认使用的是CART算法（基尼系数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dec_tree = tree.DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line"><span class="comment"># 如果criterion不填则默认使用gini系数。</span></span><br><span class="line">dec_tree = dec_tree.fit(X_train,y_train)</span><br></pre></td></tr></table></figure><h2 id="测试决策树">测试决策树</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_predict = dec_tree.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y predict value is:<span class="subst">&#123;y_predict&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y ture value is:<span class="subst">&#123;y_test&#125;</span>&quot;</span>)</span><br><span class="line">f1_score = f1_score(y_test, y_predict, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;F1-Score is <span class="subst">&#123;f1_score&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y predict value <span class="keyword">is</span>:[<span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">y ture value <span class="keyword">is</span>:[<span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">F1-Score <span class="keyword">is</span> <span class="number">0.9266835016835017</span></span><br></pre></td></tr></table></figure><h2 id="使用graphviz绘制决策树">使用Graphviz绘制决策树</h2><p>下载graphviz：<a href="https://graphviz.org/download/">Download |Graphviz</a></p><p>配置环境变量：</p><p><img src="https://img.issey.top/img/202209212337604.png" /></p><p>pip安装graphviz：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install graphviz -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>然后重启Pycharm，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">feature_name = [<span class="string">&#x27;酒精&#x27;</span>, <span class="string">&#x27;苹果酸&#x27;</span>, <span class="string">&#x27;灰&#x27;</span>, <span class="string">&#x27;灰的碱性&#x27;</span>, <span class="string">&#x27;镁&#x27;</span>, <span class="string">&#x27;总酚&#x27;</span>, <span class="string">&#x27;类黄酮&#x27;</span>, <span class="string">&#x27;非黄烷类酚类&#x27;</span>, <span class="string">&#x27;花青素&#x27;</span>, <span class="string">&#x27;颜色强度&#x27;</span>, <span class="string">&#x27;色调&#x27;</span>, <span class="string">&#x27;稀释葡萄酒&#x27;</span>, <span class="string">&#x27;脯氨酸&#x27;</span>]</span><br><span class="line">dot_data = tree.export_graphviz(dec_tree</span><br><span class="line">                                , feature_names=feature_name</span><br><span class="line">                                , class_names=[<span class="string">&#x27;琴酒&#x27;</span>, <span class="string">&#x27;雪莉&#x27;</span>, <span class="string">&#x27;贝尔摩德&#x27;</span>]</span><br><span class="line">                                , filled=<span class="literal">True</span></span><br><span class="line">                                , rounded=<span class="literal">True</span></span><br><span class="line">                                , fontname=<span class="string">&quot;Microsoft YaHei&quot;</span>)  <span class="comment"># 圆角</span></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.view()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212336007.png" /></p><h2 id="保存和读取决策树">保存和读取决策树</h2><p>决策树的训练过程往往很浪费时间，所以我们可以把训练好的决策树保存起来方便直接调用。这里介绍模型保存方法，不仅仅适用于决策树，而适用于所有模型。</p><h3 id="使用pickle保存模型">使用pickle保存模型</h3><p>存储模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;储存模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;train_model.pkl&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(model,f)</span><br></pre></td></tr></table></figure><p>新开一个py读取模型并查看模型:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;读取模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_model</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        model = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">dec_tree = read_model(<span class="string">&#x27;train_model.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">feature_name = [<span class="string">&#x27;酒精&#x27;</span>, <span class="string">&#x27;苹果酸&#x27;</span>, <span class="string">&#x27;灰&#x27;</span>, <span class="string">&#x27;灰的碱性&#x27;</span>, <span class="string">&#x27;镁&#x27;</span>, <span class="string">&#x27;总酚&#x27;</span>, <span class="string">&#x27;类黄酮&#x27;</span>, <span class="string">&#x27;非黄烷类酚类&#x27;</span>, <span class="string">&#x27;花青素&#x27;</span>, <span class="string">&#x27;颜色强度&#x27;</span>, <span class="string">&#x27;色调&#x27;</span>, <span class="string">&#x27;稀释葡萄酒&#x27;</span>, <span class="string">&#x27;脯氨酸&#x27;</span>]</span><br><span class="line">dot_data = tree.export_graphviz(dec_tree</span><br><span class="line">                                , feature_names=feature_name</span><br><span class="line">                                , class_names=[<span class="string">&#x27;琴酒&#x27;</span>, <span class="string">&#x27;雪莉&#x27;</span>, <span class="string">&#x27;贝尔摩德&#x27;</span>]</span><br><span class="line">                                , filled=<span class="literal">True</span></span><br><span class="line">                                , rounded=<span class="literal">True</span></span><br><span class="line">                                , fontname=<span class="string">&quot;Microsoft YaHei&quot;</span>)  <span class="comment"># 圆角</span></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.view()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209212337291.png" /></p><h1 id="决策树的优缺点">决策树的优缺点</h1><p>优点：</p><ul><li><p>可以可视化，易于理解</p></li><li><p>决策树几乎不需要预处理</p></li></ul><p>缺点：</p><ul><li><p>决策树可能会创建一棵过于复杂的树，容易过拟合，需要选择合适的剪枝策略</p></li><li><p>决策树对训练集十分敏感，哪怕训练集稍微有一点变化，也可能会产生一棵完全不同的决策树。（可以在分割训练集时把随机种子去掉，会发现每次的决策树都不一样）</p></li></ul><p>最后还是贴一个C4.5的实现文章连接吧：<ahref="https://blog.csdn.net/weixin_38273255/article/details/88981925?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%9E%E7%8E%B0C4.5&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-88981925.142%5Ev44%5Econtrol&amp;spm=1018.2226.3001.4187">python实现C4.5_张##的博客-CSDN博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记6——【支持向量机1】支持向量机的原理与推导</title>
      <link href="/article/1c4151e0792a/"/>
      <url>/article/1c4151e0792a/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：以下为支持向量机学习笔记，参考教程：</p><p><ahref="https://www.bilibili.com/video/BV1qf4y1x7kB?p=1&amp;vd_source=747540861ba5c41c17852ccf069029f5">(强推)浙江大学-机器学习_哔哩哔哩_bilibili</a></p><p><ahref="https://blog.csdn.net/v_JULY_v/article/details/7624837">支持向量机通俗导论（理解SVM的三层境界）</a></p><p><ahref="https://www.bilibili.com/video/BV1Ca411M7KA?p=1&amp;vd_source=747540861ba5c41c17852ccf069029f5">【太...完整了！】上海交大和腾讯强强联合的机器学习与深度学习课程分享！-人工智能/AI/神经网络_哔哩哔哩_bilibili</a></p></blockquote><h1 id="线性可分与线性不可分">线性可分与线性不可分</h1><p>    简单来说，线性可分就是可以用线性函数将两类样本分开。在二维中，表现为一条直线，在三维中为一个平面，而更高维中为超平面。如果不存在这样的线性函数，则为线性不可分。</p><p><img src="https://img.issey.top/img/202209211133192.png" /></p><p>    事实：如果一个数据集是线性可分的，那一定存在无数多个超平面将各个类别分开。</p><h1id="支持向量机support-vector-machine">支持向量机（Support Vector Machine）</h1><p>    支持向量机（简称SVM）最初是一种解决二分类的有监督学习算法，SVM的目的为：在给定两类样本的数据集的前提下，寻找一个将两类样本分隔开的超平面，并且使得两类样本之间的边界间隔(margin)最大化。最终得到的超平面被称为决策边界(decisionboundary)。</p><p>    示例（二维）：</p><p><img src="https://img.issey.top/img/202209211134546.png" /></p><p>    上面的三个超平面都可以将不同类别的样本分开，但是哪个是最好的呢？</p><p>    如果这里判断超平面“好坏”的标准为哪条直线对样本误差的容忍程度最高，那么直线2显然是最好的。支持向量机就是基于最优化理论，来寻找线2的算法。</p><p>    注意：在支持向量机中，样本输出值都是-1或1。</p><h1 id="最大间隔分离超平面">最大间隔分离超平面</h1><p><img src="https://img.issey.top/img/202209211134871.png" /></p><p>    <strong>上面那个图可能不太标准，红圈圈住的样本应该刚好在虚线上才对！</strong></p><p>    我们假定两类数据中间有一个超平面，将这个超平面向两边平移，直到刚好擦过样本为止（图中两条虚线），我们定义这两个超平面刚好经过的训练样本为这个数据集的<strong>支持向量</strong>(图中红圈所示样本)，把这两个超平面中间的距离叫做<strong>间隔（margin）</strong>。支持向量机要找的是使间隔最大的那个超平面。并且，求得的超平面只能有一个，所以这个超平面应该处于上线两超平面的中间，即到支持向量距离相等。</p><p>    于是，支持向量机寻找的最优分类超平面应该满足：</p><ol type="1"><li><p>该超平面分开了两类</p></li><li><p>该超平面最大化了间隔</p></li><li><p>该超平面处于间隔的中间，到所有支持向量距离相等。</p></li></ol><h1 id="线性可分支持向量机硬间隔">线性可分支持向量机（硬间隔）</h1><h2 id="线性可分的数学定义">线性可分的数学定义</h2><p>    一个训练样本集<span class="math inline">\({(\vec x_i,y_i)},i = 1\sim n\)</span> 线性可分，是指存在<span class="math inline">\((\vecw,b)\)</span> 使得：</p><p>    <span class="math inline">\(当 y_i = +1时，\vec w \cdot \vecx_i+b&gt;0,\)</span></p><p>    <span class="math inline">\(当y_i = -1时，\vec w \cdot \vec x_i+b&lt;0\)</span>.</p><p>    （注：有时会看到<spanclass="math inline">\(W^Tx_i+b\)</span>的形式，意思都一样的。）</p><h2 id="svm目标函数">SVM目标函数</h2><p>    假定训练样本集线性可分，那么支持向量机寻找的最大化间隔超平面为：</p><p>    已知训练样本集<span class="math inline">\({(\vec x_i,y_i)},i =1\sim n,y_i = -1  or 1\)</span>；</p><p>    求解<span class="math inline">\((\vec w,b)\)</span>使得：</p><p>    <span class="math inline">\(最小化(Minimize):\frac 1 2||\vecw||^2\)</span></p><p>    <span class="math inline">\(限制条件：y_i(\vec w\cdot \vec x_i+b) \geq1,(i = 1 \sim n)\)</span></p><p>    其中<span class="math inline">\(||\vec w||^2\)</span>(向量w模的平方)为，<span class="math inline">\(||\vec w||^2 =w_1^2+w_2^2+...+w_m^2 = \sum_{i=1}^m w_i^2\)</span></p><p>    我们可以看出，需要求的目标函数其实是凸优化（ConvexOptimization）中的二次规划问题。关于目标函数求解，用的是<strong>拉格朗日乘子法</strong>以及<strong>拉格朗日对偶问题</strong>求解。拉格朗日乘子法和对偶问题暂时不叙述。这里直接使用凸优化求解包进行求解。</p><h3 id="目标函数推导过程">目标函数推导过程</h3><blockquote><p>事实1：</p><p><span class="math inline">\(\vec w \cdot \vec x + b =0\)</span>与<span class="math inline">\(a(\vec w\cdot \vec x+b) = 0，(a\neq 0)\)</span> 表示同一个超平面。</p><p>事实2：一个点<span class="math inline">\(X_0\)</span>到超平面<spanclass="math inline">\(\vec w\cdot \vec x+b = 0\)</span>的距离为：</p><p><span class="math inline">\(d = \frac{|\vec w\cdot \vecx_0+b|}{||\vec w||}\)</span></p></blockquote><p>    假设我们已知最终要求的超平面为：<span class="math inline">\(\vecw\cdot \vec x+b\)</span>，因为这个超平面在间隔最大的两个平行的超平面正中间，而且上下两个超平面都经过支持向量，<strong>所以支持向量到所求超平面的距离应该都是相同的。</strong></p><p>    于是，我们可以根据事实1，将<span class="math inline">\((\vecw,b)\)</span> 放缩为<span class="math inline">\((a\vec w,ab)\)</span>,最终使得：</p><p>    在支持向量<span class="math inline">\(X_0\)</span>上有<spanclass="math inline">\(|\vec w\cdot \vec x_0+b| = 1\)</span>；</p><p>    那么显而易见在非支持向量上有<span class="math inline">\(|\vecw\cdot \vec x_0+b|&gt;1\)</span>。</p><p>   （有点难懂，我的理解是a对于每一个支持向量都是一个不同的值，使其满足上述条件，也就是说a并不是一个定值。但是无论a怎么变，都表示的同一个超平面，所以对后续没有影响。至于非支持向量上为什么绝对值都大于1，是因为事实2，因为支持向量离超平面距离是最近的，所以分母相同的情况下，非支持向量作为分子自然就更大。）</p><p>    变换后，根据事实2，支持向量<spanclass="math inline">\(X_0\)</span>到超平面的距离将会变为：<spanclass="math inline">\(d = \frac{|\vec w\cdot \vec x_0+b|}{||\vec w||} =\frac {1}{||\vec w||}\)</span></p><p>    我们的目标是使支持向量到超平面的距离最大，也就是<spanclass="math inline">\(maximize(\frac 1 {||\vec w||})\)</span></p><p>    又因<span class="math inline">\(maximize(\frac 1 {||\vec w||}) =minimize(||\vec w||)\)</span></p><p>    于是我们将问题优化的目标函数定为：<spanclass="math inline">\(最小化(Minimize):\frac 1 2||\vecw||^2\)</span></p><p>    而 最小化<span class="math inline">\(\frac 1 2||\vecw||^2\)</span> 与 最小化$||w|| $是完全等价的。之所以写成这种形式，是为了后续求导更加方便。</p><p>    同时，因为我们将<span class="math inline">\((\vecw,b)\)</span>放缩为<span class="math inline">\((a\vec w,ab)\)</span>,我们可以得到限制条件：</p><p>    <span class="math inline">\(y_i(\vec w\cdot \vec x_i +b) \geq1,(i= 1 \sim n)\)</span></p><p>    其中，<span class="math inline">\(y_i = -1  or  1\)</span></p><h2 id="凸优化中的二次规划">凸优化中的二次规划</h2><ol type="1"><li><p>目标函数为二次项</p></li><li><p>限制条件是一次项</p></li></ol><p>    因为我们的目标函数<span class="math inline">\(\frac 1 2||\vecw||^2 = \frac 1 2(w_1^2+w_2^2+...+w_m^2)\)</span>为二次项,限制条件<spanclass="math inline">\(y_i(\vec w\cdot \vec x_i +b) \geq1,(i = 1 \simn)\)</span>为一次项，所以满足二次规划。</p><p>   凸优化的二次规划问题要么无解，要么只有唯一最小值解。于是，我们就可以用梯度下降算法求解啦！另外，只要一个优化问题是凸的，我们总能找到高效快速的算法去解决它。<strong>线性可分条件下的支持向量机是凸优化问题</strong>，因此能迅速找到高效的算法解决。不过我们不会详细探讨求解凸优化问题，关于凸优化求解是一门专门的课程，有兴趣可以学习《凸优化理论》这门课程。</p><h1 id="线性支持向量机软间隔">线性支持向量机（软间隔）</h1><h2 id="硬间隔与软间隔">硬间隔与软间隔</h2><p>硬间隔：间隔内不存在样本。训练集完全分类正确，损失函数不存在，损失值为0。也就是说，找到的超平面完全分离两类。上述都是硬间隔。硬间隔容易受到极端值影响，泛化能力不强，于是我们提出了软间隔。</p><p>软间隔：间隔内允许样本存在。允许一定量的样本分类错误，不过这些错误样本范围不会超过间隔区间。软间隔是硬间隔SVM的拓展版本。</p><h2 id="松弛因子">松弛因子</h2><blockquote><p>注：因为线性支持向量机模拟出的直线允许误差存在，所以根据线性可分的定义，线性支持向量机其实属于线性不可分。</p></blockquote><p>若数据线性不可分，则增加松弛因子<span class="math inline">\(\zeta_i\geq0\)</span> ,使函数间隔加上松弛变量大于等于1。于是，</p><p>约束条件变为:<span class="math inline">\(y_i(\vec w\cdot \vec x_i+b)\geq 1-\zeta_i\)</span></p><p>目标函数变为：<span class="math inline">\(minimize_{w,b}(\frac 12||\vec w||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>其中，C为惩罚因子，是为了防止松弛因子过大加入的一个代价。当C等于无穷大，只有当<spanclass="math inline">\(\zeta_i =0\)</span>时才有最小值，因此，当C为无穷大时，退化为线性可分支持向量机。</p><p>目标函数求解依然是代入拉格朗日乘子，转化为对偶问题并求解。</p><h1 id="合页损失函数hinge-loss-function">合页损失函数（hinge lossfunction）</h1><p>公式：<span class="math inline">\(L(y(\vec w\cdot \vec x+b)) =[1-y(\vec w\cdot \vec x+b)]_+\)</span></p><p>下标“+”表示以下情况取正值：</p><p><span class="math inline">\([z]_+ =\left\{\begin{aligned} z, z&gt;0\\ 0,z\leq0\end{aligned} \right.\)</span></p><p>当函数间隔<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&gt;1\)</span>时，即当分类正确并在（软）间隔之外时，损失为0。否则损失为<spanclass="math inline">\(1-y_i(\vec w\cdot \vec x+b)\)</span></p><p><img src="https://img.issey.top/img/202209211135657.jpg" /></p><p>当样本正确分类，<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&gt;0\)</span>,反之小于0。</p><p><span class="math inline">\(|y_i(\vec w\cdot \vec x+b)|\)</span>表示样本与决策边界的距离。绝对值越大，距离决策边界越远。</p><p>于是：</p><p>当<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&gt;0\)</span>,即分类正确情况下，距离决策边界越远区分程度越好。</p><p>当<span class="math inline">\(y_i(\vec w\cdot \vecx+b)&lt;0\)</span>,即分类错误情况下，距离决策边界越远区分程度越差。</p><h2 id="svm的损失函数">SVM的损失函数</h2><p>SVM有另一种解释，即最小化以下目标函数：</p><p><span class="math inline">\(\sum_{i=1}^N[1-y_i(\vec w\cdot \vecx_i+b)]_++\lambda||\vec w||^2\)</span></p><p>这里不提供相关证明，详情见文章：<ahref="https://blog.csdn.net/lynn_001/article/details/84198007">线性支持向量机-合页损失函数(HingeLoss)_搏击俱乐部_的博客-CSDN博客_支持向量机损失函数</a></p><p>也就是说，SVM目标函数实际上就是合页损失函数加上<spanclass="math inline">\(\lambda||\vec w||^2\)</span>。</p><h1 id="非线性支持向量机">非线性支持向量机</h1><h2 id="将特征空间从低位映射到高维">将特征空间从低位映射到高维</h2><p>当遇到如下图所示的非线性数据时，支持向量机的处理是将该训练集的特征从低维映射到高维，在高维仍然采用线性超平面对数据进行分类。</p><p><img src="https://img.issey.top/img/202209211135086.png" /></p><p>现有以下假设：</p><p>假设1：在一个M维空间上随机取N个训练样本，随机的对每个训练样本赋予标签+1或-1，设这些训练样本线性可分的概率为<spanclass="math inline">\(P(M)\)</span>。那么当M趋于无穷大时，<spanclass="math inline">\(P(M)=1\)</span>。</p><p>这里略去该假设的证明。</p><p>也就是说，一个训练集在低维上不可分，但它到高维的映射将会是可分的。于是，支持向量机将训练样本由低维映射到高维以增加线性可分的概率。</p><p>我们设<spanclass="math inline">\(\phi(x)\)</span>为x在高维上的映射，那么假定<spanclass="math inline">\(\phi(x)\)</span>形式已知的条件下，引入松弛变量的目标函数将会变为：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vecw||^2+C\sum_{i=1}^N\zeta_i),    \zeta_i\geq0,(i=1\sim n)\)</span></p><p>限制条件：</p><p><span class="math inline">\(y_i[\vec w\cdot\phi(\vec x_i)+b]\geq1-\zeta_i,(i=1 \sim n)\)</span></p><p>注意：这里的<span class="math inline">\(\vec w\)</span>是与高维的<span class="math inline">\(\phi(\vec x)\)</span> 对应的。</p><p>我们可以看到，转化为高维后同样可以采用凸优化的二次规划求解。</p><h2 id="核函数kernel-function">核函数（Kernel Function）</h2><p>注意：接下来向量将会用<span class="math inline">\(X\)</span>表示，向量点乘则变为矩阵乘法，例如：</p><p><span class="math inline">\(\vec x_1\cdot\vec x_2\)</span>将变为<spanclass="math inline">\(X_1^TX_2\)</span> 。</p><p>根据低维映射到高维的规则，重点在于如何找到<spanclass="math inline">\(\phi(X)\)</span>使得线性不可分训练集在高维线性可分。实际上，我们可以不用知道 <spanclass="math inline">\(\phi(X)\)</span>的具体形式。取而代之，如果对于任意两个向量<spanclass="math inline">\(X_1,X_2\)</span>,有<spanclass="math inline">\(K(X_1,X_2) = \phi(X_1)^T\phi(X_2)\)</span>,那么我们仍然可以通过一些技巧完成测试样本的预测。</p><p>我们定义<span class="math inline">\(K(X_1,X_2)\)</span>为核函数。易得，核函数是一个实数。</p><p>可以证明，<span class="math inline">\(K(X_1,X_2)\)</span>与<spanclass="math inline">\(\phi(X_1),\phi(X_2)\)</span>是一一对应的关系，证明略。另外，核函数必须满足以下条件才能写成两个向量内积的形式：</p><p><span class="math inline">\(K(X_1,X_2)\)</span>能写成<spanclass="math inline">\(\phi(X_1)^T\phi(X_2)\)</span> 的充要条件：</p><ol type="1"><li><p><span class="math inline">\(K(X_1,X_2) =K(X_2,X_1)\)</span>,即交换性</p></li><li><p>$C_i(i=1N),N有<em>{i=1}^N</em>{j=1}^NC_iC_jK(X_iX_j) $,即半正定性</p></li></ol><p>接下来，我们将研究如何在已知<spanclass="math inline">\(K(X_1,X_2)\)</span>而不知道<spanclass="math inline">\(\phi(X)\)</span>的条件下求解支持向量机的目标函数。</p><h2 id="对偶问题与kkt条件">对偶问题与KKT条件</h2><h3 id="原问题与对偶问题">原问题与对偶问题</h3><p>原问题（Prime problem）定义：</p><p>    最小化（Minimize）：<span class="math inline">\(f(w)\)</span></p><p>    限制条件（Subject to）： <span class="math display">\[\begin{split}&amp;g_i(w) \leq0 ,i=1\sim K\\&amp;h_i(w) = 0,i=1\simm\end{split}\]</span> 注：自变量为<spanclass="math inline">\(w\)</span>,目标函数是<spanclass="math inline">\(f(w)\)</span>，限制条件：有K个不等式，分别用<spanclass="math inline">\(g_i(w)\)</span> 来表示，等式有m个，分别用<spanclass="math inline">\(h_i(m)\)</span> 表示 。</p><p>为了定义对偶问题，我们先定义一个函数：</p><p><span class="math inline">\(L(w,a,\beta) =f(w)+a^Tg(w)+\beta^Th(w)\)</span></p><p>其中，</p><p><span class="math inline">\(a = [a_1,a_2,...,a_K]^T\)</span>,</p><p><span class="math inline">\(\beta =[\beta_1,\beta_2,...\beta_M]^T\)</span>,</p><p><span class="math inline">\(g(w) =[g_1(w),g_2(w),...,g_K(w)]^T\)</span></p><p><span class="math inline">\(h(w) =[h_1(w),h_2(w),...,h_M(w)]^T\)</span></p><p>然后，定义对偶问题如下：</p><p><span class="math inline">\(最大化：\theta(a,\beta) = inf L(w,a,\beta),所有定义域内的w\)</span>    </p><p>限制条件：<span class="math inline">\(a_i \geq0,i=1\simK\)</span></p><p>对偶问题是：最大化<spanclass="math inline">\(\theta(a,\beta)\)</span>, 它等于<spanclass="math inline">\(L(w,a,\beta)\)</span> 去遍历所有定义域上的<spanclass="math inline">\(w\)</span>找到使<spanclass="math inline">\(L(w,a,\beta)\)</span>最小的那个<spanclass="math inline">\(w\)</span>，同时将求得的<spanclass="math inline">\(L(w,a,\beta)\)</span> 赋值为<spanclass="math inline">\(\theta(a,\beta)\)</span>。注意限制条件。</p><p>联合原问题与对偶问题，有以下定理：</p><p>定理1：若<span class="math inline">\(w^*\)</span>是原问题的解，<spanclass="math inline">\((a^*,\beta^*)\)</span>是对偶问题的解，那么：<spanclass="math inline">\(f(w^*)\geq \theta(a^*,\beta^*)\)</span></p><p>证明略。</p><p>这个定理说明：原问题的解<spanclass="math inline">\(f(w^*)\)</span>总是大于等于对偶问题的解<spanclass="math inline">\(\theta(a^*,\beta^*)\)</span> 。</p><p>我们将<span class="math inline">\(f(w^*)-\theta(a^*,\beta^*)\)</span> 定义为对偶差距(DoalityGap)。根据定理1，对偶差距大于等于0。</p><h3 id="强对偶定理">强对偶定理</h3><p>如果<span class="math inline">\(g(w) =Aw+b,h(w)=Cw+d,f(w)\)</span>为凸函数，则有<spanclass="math inline">\(f(w^*) =\theta(a^*,\beta^*)\)</span>,对偶差距为0。</p><p>简单来说就是，<strong>如果原问题的目标函数是凸函数，而限制条件是线性函数</strong>那么<spanclass="math inline">\(f(w^*) = \theta(a^*,\beta^*)\)</span>。证明略。</p><h3 id="kkt条件">KKT条件</h3><p>如果强对偶定理成立，即<span class="math inline">\(f(w^*) =\theta(a^*,\beta^*)\)</span> ,则定理1中必然能推出：对于所有的<spanclass="math inline">\(i=1\sim K\)</span> ,要么<spanclass="math inline">\(a_i = 0\)</span>，要么<spanclass="math inline">\(g_i(w^*) = 0\)</span>。这个条件被称为KKT条件。</p><h2id="将支持向量机目标函数转化为对偶问题并求解">将支持向量机目标函数转化为对偶问题并求解</h2><h3 id="目标函数转化为原问题形式">目标函数转化为原问题形式</h3><p>回顾一下现在的支持向量机目标函数：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vecw||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>限制条件：</p><p><span class="math inline">\(\zeta_i\geq0,(i=1\sim n)\)</span></p><p><span class="math inline">\(y_i[\vec w\cdot\phi(\vec x_i)+b]\geq1-\zeta_i,(i=1 \sim n)\)</span></p><p>对比原问题(Prime problem)的形式：</p><p>最小化（Minimize）：<span class="math inline">\(f(w)\)</span></p><p>限制条件（Subject to）: <span class="math display">\[\begin{split}&amp;g_i(w) \leq0 ,i=1\sim K\\&amp;h_i(w) = 0,i=1\simm\end{split}\]</span> 注意到，原问题中不等式<span class="math inline">\(g_i(w)\leq0\)</span>,而支持向量机的限制条件中两个不等式都是大于等于0的。所以我们要先将支持向量机中的限制条件转为小于等于。</p><ul><li><p>将<span class="math inline">\(\zeta_i\)</span>转为相反数</p></li><li><p>展开并化简第二个不等式</p></li></ul><p>于是，目标函数将变为：</p><p><span class="math inline">\(minimize_{w,b}(\frac 1 2||\vecw||^2+C\sum_{i=1}^N\zeta_i)\)</span></p><p>限制条件：</p><p><span class="math inline">\(\zeta_i\leq0,(i=1\sim n)\)</span></p><p><spanclass="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib\leq0(i=1\simN)\)</span></p><p>因为目标函数是凸的，而其限制条件都是线性函数，所以满足强对偶定理。</p><h3 id="利用对偶定理求解">利用对偶定理求解</h3><p>现在，对偶问题中的<span class="math inline">\(w\)</span>就是这里的<spanclass="math inline">\((w,b,\zeta_i)\)</span>,而不等式<spanclass="math inline">\(g_i(w)\leq0\)</span>是这里限制条件（两部分）：</p><p><span class="math inline">\(\zeta_i\leq0,(i=1\sim n)\)</span></p><p><spanclass="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib\leq0(i=1\simN)\)</span></p><p>另外，因为限制条件不存在等式，所以不存在对偶问题中的<spanclass="math inline">\(h_i(w)\)</span>。</p><p>然后，对偶问题可以写成如下形式：</p><p><span class="math inline">\(最大化：\theta(a,\beta) =inf_{w,\zeta,b}\{\frac 12||w||^2-C\sum_{i=1}^N\beta_i\zeta_i+\sum_{i=1}^Na_i[1+\zeta_i-y_iw^T\phi(X_i)-y_ib]\}\)</span></p><p>限制条件：</p><ol type="1"><li><span class="math inline">\(a_i\geq0\)</span></li></ol><p>(2)<span class="math inline">\(\beta_i\geq0\)</span></p><h4id="如何将原目标函数转化为对偶问题">如何将原目标函数转化为对偶问题</h4><p>先对<spanclass="math inline">\((w,b,\zeta_i)\)</span>求导并令导数为0：</p><ol type="1"><li><p><span class="math inline">\(\frac {\partial \theta}{\partial w} =0\)</span>推出<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span></p></li><li><p><span class="math inline">\(\frac {\partial \theta}{\partial\zeta_i} = 0\)</span>推出<spanclass="math inline">\(a_i+\beta_i=C\)</span></p></li><li><p><span class="math inline">\(\frac {\partial \theta}{\partial b} =0\)</span>推出<spanclass="math inline">\(\sum_{i=1}^Na_iy_i=0\)</span></p></li></ol><p>(详细过程略)</p><p>于是，可以将支持向量机原目标函数化为以下对偶问题：</p><p><span class="math inline">\(最大化：\theta(a,\beta) =\sum_{i=1}^Na_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_ja_ia_j\phi(X_i)^T\)</span></p><p>限制条件：</p><p>(1)  <span class="math inline">\(0 \leq a_i\leq C,(i=1\simN)\)</span></p><p>(2)<span class="math inline">\(\sum_{i=1}^Na_iy_i=0,(i=1\simN)\)</span></p><p>可以看出，这个对偶问题也是一个凸优化的二次规划问题，可以通过最优化算法快速求解。（利用凸优化包）</p><h3 id="如何求解这个对偶问题">如何求解这个对偶问题</h3><p>由于<span class="math inline">\(K(X_i,X_j) =\phi(X_i)^T\phi(X_j)\)</span>,所以我们只需要知道核函数，就可以求解这个对偶问题了。当我们求解了这个对偶问题，解出了所有的<spanclass="math inline">\(a_i\)</span> 。我们可以继续观察<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span>,因为<spanclass="math inline">\(\phi(X_i)\)</span> 不具有显式表达，所以<spanclass="math inline">\(w\)</span>也不具有显式表达。但是我们可以推导：即使<spanclass="math inline">\(w\)</span>不具有显示表达，我们也可以通过核函数算出<spanclass="math inline">\(w^T\phi(X)+b\)</span>的值。</p><h4 id="首先如何求b">首先，如何求b</h4><p>由于<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span> ,则</p><p><span class="math inline">\(w^T\phi(X_i) =\sum_{j=1}^Na_jy_j\phi(X_j)^T\phi(X_i) =\sum_{j=1}^Na_jy_jK(X_j,X_i)\)</span></p><p>其次，根据KKT条件，我们可以推出：</p><ol type="1"><li><p><span class="math inline">\(a_i[1+\zeta_i-y_iw^T\phi(X_i)-y_ib] =0\)</span></p></li><li><p><span class="math inline">\(\beta_i\zeta_i=0\)</span>即<spanclass="math inline">\((c-a_i)\zeta_i = 0\)</span></p></li></ol><p>另外，如果对某个i，<span class="math inline">\(a_i \not= 0且a_i \not=c\)</span>，则根据上面KKT推出的两个公式必有<spanclass="math inline">\(\zeta_i = 0\)</span>,且<spanclass="math inline">\(1+\zeta_i-y_iw^T\phi(X_i)-y_ib = 0\)</span> 。</p><p>而这时<span class="math inline">\(y_iw^T\phi(X_i) =\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)\)</span></p><p>所以，只需要找一个<span class="math inline">\(0&lt;a_i&lt;c\)</span>，</p><p><span class="math inline">\(b =\frac{1-\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)}{y_i}\)</span></p><h4 id="如何求wtphixb-核函数戏法kernel-trick">如何求<spanclass="math inline">\(w^T\phi(X)+b\)</span>    —— 核函数戏法（KernelTrick）</h4><p>将<spanclass="math inline">\(w=\sum_{i=1}^Na_iy_i\phi(X_i)\)</span>代入得：</p><p><span class="math display">\[\begin{split}w^T\phi(X)+b &amp;=w\\&amp;=\sum_{i=1}^Na_iy_i\phi(X_i)^T\phi(X)+b\\&amp; =\sum_{i=1}^Na_iy_iK(X_i,X)+b\end{split}\]</span></p><p>我们发现，即使不知道<spanclass="math inline">\(\phi(X)和w\)</span>的显式形式，也可以通过核函数求得<spanclass="math inline">\(w^T\phi(X)+b\)</span>，这一结论被称为核函数戏法。</p><p>最后，我们可以用如下的判别标准来判定一个样本属于哪一类别：</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \geq0\)</span>，那么<span class="math inline">\(X \in C_1\)</span> ;</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \leq0\)</span>，那么<span class="math inline">\(X \in C_2\)</span> 。</p><h2id="总结支持向量机训练和测试流程">总结：支持向量机训练和测试流程</h2><p><strong>训练过程：</strong></p><p>输入训练集${(X_i,y_i)},i=1N <spanclass="math inline">\(,其中，\)</span>y_i = +1或-1$ 。</p><p>接下来，求解如下目标函数：</p><p><span class="math inline">\(最大化：\theta(a,\beta) =\sum_{i=1}^Na_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_ja_ia_j\phi(X_i)^T\)</span></p><p>限制条件：</p><p>(1)<span class="math inline">\(0 \leq a_i\leq C,(i=1\simN)\)</span></p><p>(2)<span class="math inline">\(\sum_{i=1}^Na_iy_i=0,(i=1\simN)\)</span></p><p>求出<span class="math inline">\(a\)</span> 。</p><p>然后，求出b：</p><p>    找一个<span class="math inline">\(a_i \not= 0且a_i\not=c\)</span>,</p><p>    <span class="math inline">\(b =\frac{1-\sum_{j=1}^Na_iy_iy_jK(X_j,X_i)}{y_i}\)</span></p><p>一旦求出了<spanclass="math inline">\(a，b\)</span>,就完成了支持向量机的训练过程。</p><p><strong>测试过程：</strong></p><p>给出一个测试数据X，预测它的类别y。</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b \geq0\)</span>，那么<span class="math inline">\(y = +1\)</span> ;</p><p>若<span class="math inline">\(\sum_{i=1}^Na_iy_iK(X_i,X)+b &lt;0\)</span>，那么<span class="math inline">\(y = -1\)</span> ;</p><p>关于支持向量机的具体应用以及更多细节比如核函数的选择、超参数的控制等等将会在下一章支持向量机中进行说明。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习笔记1——神经网络的搭建与简单应用</title>
      <link href="/article/f84b08dc3a2c/"/>
      <url>/article/f84b08dc3a2c/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ul><li><ahref="https://www.bilibili.com/video/BV1nt4y1h7jc?vd_source=747540861ba5c41c17852ccf069029f5">[双语人译|带测试]吴恩达2022机器学习专项课程(二）：高级学习算法Advanced Learning Algorithms</a></li></ul><h1 id="前言">前言</h1><p>在写这篇文章时，对于神经网络的理解还不够，并没有说明神经网络到底是什么，并且推导神经网络后向传播的过程。这篇文章个人觉得是失败的，但同时也有一定价值。</p><p>此文章说明了神经网络的环境搭建tensorflow以及配置其能够GPU加速的过程，同时利用tensorflow做了一个简单的案例，为后续神经网络的学习解决了杂碎的问题。</p><p>本来我不打算发出来，但是考虑到不管是pytorch还是tensorflow，搭建能够调用gpu环境的过程都是很容易出错的，于是还是决定发出来。因为当时我自己搭建这个环境因为各种版本问题还有其他奇怪的问题，加上现有的大部分教程由于版本迭代问题已经不适合，浪费了起码半天的时间。</p><h1 id="神经网络与深度学习">神经网络与深度学习</h1><p>    <ahref="https://baike.baidu.com/item/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/16600562?fr=aladdin">神经网络</a>诞生于一个尝试创建能够模拟大脑的软件的动机。对于神经网络的研究始于20世纪50年代。中途被冷落了两次。在2005年，神经网络东山再起，其中的一些算法被深度学习重新命名，实际上深度学习和神经网路阐释着非常相似的事情。网络上有不少区别深度学习和神经网络的文章，但就当下的环境来说，深度学习其实就是神经网络，深度学习不过是业界为神经网络取的一种营销名字。</p><h1 id="使用tensorflow搭建神经网络">使用Tensorflow搭建神经网络</h1><p>    常用的人工神经网络搭建框架有Tensorflow和Pytorch等，这里使用tensorflow进行搭建。</p><h2 id="环境搭建和导包遇到的问题">环境搭建和导包遇到的问题：</h2><h3 id="问题1">问题1：</h3><p>    最开始是用Acaconda把tensorflow和Keras都安装了，然后导入时报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AssertionError: Duplicate registrations <span class="keyword">for</span> <span class="built_in">type</span> <span class="string">&#x27;optimizer&#x27;</span></span><br></pre></td></tr></table></figure><p>    然后尝试卸载Keras，问题解决。而且卸载后condalist还找到了一个Keras，说明刚才可能多装了一个。</p><h3 id="问题2">问题2：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> xxx</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;tensorflow.keras&#x27;</span></span><br></pre></td></tr></table></figure><p>    最开始以为是Keras和tensorflow版本不搭配问题（当时还没分清Keras和tensorflow.keras是两个东西），检查了版本后无误，最终在这篇文章中找到了答案：<ahref="https://blog.csdn.net/Eric_Blog_CSDN/article/details/88420234">keras学习-No module named ' tensorflow.keras ' 报错，看清tf.keras与keras_Eric_Blog_csdn的博客-CSDN博客</a></p><p>    原因是安装路径中间多了个python，所以导入应该这样写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> xxx</span><br></pre></td></tr></table></figure><h3id="tensorflowgpu相关文件打开不了">tensorflow：GPU相关文件打开不了</h3><p>    各种报错：</p><p><img src="https://img.issey.top/img/202209211150591.png" /></p><p>    大概是说找不到调用GPU的这些文件，如果不解决它，就无法做到向量化，不能向量化那训练速度就会极慢，所以不能放着不管。    首先你得安装适合版本的Cudatoolkit，然后安装cuDNN，再安装tensorflow-gpu。参考：<ahref="https://blog.csdn.net/weixin_56197703/article/details/125192385">安装tensorflow的GPU版本（详细图文教程)</a>.</p><p>    难崩。搞了半天，终于把这个问题解决了。这个环境真的搞得我绝望，各个版本试了又试，强烈建议按照上面贴的这个教程来！！！</p><p>     最后选择的版本搭配：CUDA 11.6.0+cuDNN 8.4.0+tensorflow-gpu2.9.1。如果你用的pycharm，一定一定要把环境换成conda！！！这是我血与泪的教训。</p><p>    终于：</p><p><img src="https://img.issey.top/img/202209211150099.png" /></p><h2 id="搭建一个简单的神经网络">搭建一个简单的神经网络</h2><h3 id="问题引入">问题引入：</h3><p>    现在有一批经过烘烤的咖啡豆，以及它们的烘烤温度和烘烤时间，现在我们知道哪些咖啡豆是好的（GoodRoast）和坏的（BadRoast），请根据已有数据构建一个神经网络，推测在什么温度范围内和烘烤时间范围内咖啡豆是好的。</p><p>    数据绘图：</p><p><img src="https://img.issey.top/img/202209211150809.png" /></p><p>    注：中间的线不用管它。</p><h3 id="导入需要的包">导入需要的包：</h3><p>    这里只给出神经网络相关包，并不是接下来用到的所有包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure><p>Keras是一个高级的Python神经网络框架，已经被添加到TensorFlow中，成为其默认的框架，为TensorFlow 提供更高级的API。</p><p>这里除了这种导包方式还有另一种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>    但是注意你不能同时导入keras和tensorflow内置的keras，不然因为混用报错。我用的tensorflow2.9.0，用第二种方法还是会报错，所以改为了第一种，暂时不清除是不是版本问题。</p><h3 id="数据规范化">数据规范化</h3><p>    与之前学过的相同，对数据进行规范化可以加快反向传播的速度（暂时不用管反向传播是什么）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X,Y = load_coffee_data()</span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">norm_l = tf.keras.layers.Normalization(axis=-<span class="number">1</span>)</span><br><span class="line">norm_l.adapt(X)  <span class="comment"># learns mean, variance</span></span><br><span class="line">Xn = norm_l(X)</span><br></pre></td></tr></table></figure><p>X:（200,2）矩阵，200个样例，2个特征（温度和烘烤时间）；</p><p>Y：长200的列表，值为0或1，1代表Good Roast</p><p>    复制数据，增加训练集的数量并减少训练代数？（不太懂，可能就是要增加训练集个数吧）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Xt = np.tile(Xn,(<span class="number">1000</span>,<span class="number">1</span>))</span><br><span class="line">Yt= np.tile(Y,(<span class="number">1000</span>,<span class="number">1</span>))  </span><br></pre></td></tr></table></figure><h3 id="构建神经网络模型">构建神经网络模型</h3><p>    我们打算构建的神经网络模型长这样：</p><p><img src="https://img.issey.top/img/202209211151193.png" /></p><p>    这个模型的激活函数都是sigmoid函数。可以看出，它一共有两层，第一层（layer1）一共有三个神经元，第二层（layer2）有一个神经元。像这样的layer又叫做密集层（Dense）。</p><p>    现在考虑一个样例：输入值 <span class="math inline">\(\vec x =(x_1,x_2)\)</span>,因为layer1有三个神经元，每个神经元能算出一个值，所以<spanclass="math inline">\(\vec x\)</span>进入layer 1可以算出一个向量<spanclass="math inline">\(\vec a^{[1]} =(a_1^{[1]},a_2^{[1]},a_3^{[1]})\)</span> ,上[ ]表示所在层数。然后<spanclass="math inline">\(\vec a^{[1]}\)</span>进入layer 2,同理，可算出<spanclass="math inline">\(\vec a^{[2]} = (a_1^{[2]})\)</span>。当<spanclass="math inline">\(a_1^{[2]}&gt;=0.5\)</span>时，我们认为它是好的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">1234</span>)  <span class="comment"># applied to achieve consistent results</span></span><br><span class="line">layer_1 = Dense(<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer1&#x27;</span>)</span><br><span class="line">layer_2 = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer2&#x27;</span>)</span><br><span class="line">model = Sequential([layer_1,layer_2])  </span><br></pre></td></tr></table></figure><p>    设置随机数可以使每次训结果一样，便于对照。构建模型也可以写成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential(</span><br><span class="line">    [</span><br><span class="line">        tf.keras.Input(shape=(<span class="number">2</span>,)),</span><br><span class="line">        Dense(<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer1&#x27;</span>),</span><br><span class="line">        Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer2&#x27;</span>)</span><br><span class="line">     ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>    tf.keras.Input(shape=(2,）这一句可以去掉。加上这一句可以为让模型塑形。下面是查看模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209211151155.png" /></p><p>    检查模型各层的参数是否正确：</p><p>    权重W应为（输入的特征数，层中单元数），偏差b应为层数单元数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W1, b1 = model.get_layer(<span class="string">&quot;layer1&quot;</span>).get_weights()</span><br><span class="line">W2, b2 = model.get_layer(<span class="string">&quot;layer2&quot;</span>).get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;W1<span class="subst">&#123;W1.shape&#125;</span>:\n&quot;</span>, W1, <span class="string">f&quot;\nb1<span class="subst">&#123;b1.shape&#125;</span>:&quot;</span>, b1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;W2<span class="subst">&#123;W2.shape&#125;</span>:\n&quot;</span>, W2, <span class="string">f&quot;\nb2<span class="subst">&#123;b2.shape&#125;</span>:&quot;</span>, b2)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209211151293.png" /></p><p>    对应的值是模拟的初始值，随机数种子不同时会发现这些值会变化。我们这里注意它们的shape就行。</p><h3 id="训练模型">训练模型</h3><p>    下面的语句将在之后的文章详细介绍：</p><p>model.compile：定义损失函数和指定编译优化。</p><p>model.fit：运行梯度下降并训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss = tf.keras.losses.BinaryCrossentropy(),</span><br><span class="line">    optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(</span><br><span class="line">    Xt,Yt,            </span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>    在训练结束后，各权值就会更新成训练好的权值了：</p><p><img src="https://img.issey.top/img/202209211152191.png" /></p><p>    关于model.fit里的epochs参数：指训练集应该在训练期间被应用多少次，这里指定的10次。为了提高训练效率，训练集被分成了n个批次，一个批次大小为32，我们有200000个数据，所以被分成了6250批次。</p><p>    </p><p>    补充：训练一般比较慢，为了不每次改代码都重新训练一次，我们可以将以前的训练的结果手动赋给各层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.array([</span><br><span class="line">    [-<span class="number">0.13</span>,  <span class="number">14.3</span>, -<span class="number">11.1</span>],</span><br><span class="line">    [-<span class="number">8.92</span>, <span class="number">11.85</span>, -<span class="number">0.25</span>]] )</span><br><span class="line">b1 = np.array([-<span class="number">11.16</span>, -<span class="number">1.76</span>,  -<span class="number">12.1</span>])</span><br><span class="line">W2 = np.array([</span><br><span class="line">    [-<span class="number">45.71</span>],</span><br><span class="line">    [-<span class="number">42.95</span>],</span><br><span class="line">    [-<span class="number">50.19</span>]])</span><br><span class="line">b2 = np.array([<span class="number">26.14</span>])</span><br><span class="line">model.get_layer(<span class="string">&quot;layer1&quot;</span>).set_weights([W1,b1])</span><br><span class="line">model.get_layer(<span class="string">&quot;layer2&quot;</span>).set_weights([W2,b2])</span><br></pre></td></tr></table></figure><h3 id="模型预测">模型预测</h3><p>    预测，并将概率转化为决策：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">X_test = np.array([</span><br><span class="line">    [<span class="number">200</span>,<span class="number">13.9</span>],  <span class="comment"># postive example</span></span><br><span class="line">    [<span class="number">200</span>,<span class="number">17</span>]])   <span class="comment"># negative example</span></span><br><span class="line">X_testn = norm_l(X_test)</span><br><span class="line">predictions = model.predict(X_testn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predictions = \n&quot;</span>, predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yhat = np.zeros_like(predictions)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predictions)):</span><br><span class="line">    <span class="keyword">if</span> predictions[i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">        yhat[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        yhat[i] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;decisions = \n<span class="subst">&#123;yhat&#125;</span>&quot;</span>)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更简洁的代码</span></span><br><span class="line">yhat = (predictions &gt;= <span class="number">0.5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;decisions = \n<span class="subst">&#123;yhat&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209211152954.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记5——过拟合与正则化</title>
      <link href="/article/63ddafc1034a/"/>
      <url>/article/63ddafc1034a/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合与正则化">过拟合与正则化</h1><blockquote><p>前言：这篇文章皆以回归模型为例。</p></blockquote><p>在开始之前，你可能先要了解以下几个概念：</p><h2 id="前置知识">前置知识</h2><p><strong>偏差与方差</strong></p><blockquote><p>在机器学习中，偏差描述的是根据样本拟合出的模型输出结果与真实结果的差距，损失函数就是依据模型偏差的大小进行反向传播的。降低偏差，就需要复杂化模型，增加模型参数，但容易造成过拟合。方差描述的是样本上训练出的模型在测试集上的表现，降低方差，继续要简化模型，减少模型的参数，但容易造成欠拟合。根本原因是，我们总是希望用有限的训练样本去估计无限的真实数据。假定我们可以获得所有可能的数据集合，并在这个数据集上将损失函数最小化，则这样的模型称之为“真实模型”。但实际应用中，并不能获得且训练所有可能的数据，所以真实模型一定存在，但无法获得。<ahref="https://baike.baidu.com/item/%E6%AC%A0%E6%8B%9F%E5%90%88/22692155?fr=aladdin">欠拟合_百度百科(baidu.com)</a></p></blockquote><p><strong>泛化能力</strong></p><p>通俗来讲，就是训练出的模型对新鲜样本的适应能力。</p><h2 id="什么是欠拟合与过拟合">什么是欠拟合与过拟合</h2><p>示例：正常拟合情况</p><p><img src="https://img.issey.top/img/202209211121846.png" /></p><p>欠拟合：表现为高偏差。欠拟合模型在训练集、验证集和测试集表现均不佳。就像一个不好好学习的学生，在模拟考试和新的考试都考不好。</p><p><img src="https://img.issey.top/img/202209211121782.png" /></p><p>过拟合：表现为高方差。过拟合模型在训练集上表现很好，但遇到陌生数据时就表现得很差，即模型的泛化能力很差。就像一个学生在做模拟卷时太过于努力了，但是他学会的太贴合模拟卷的题型，但是遇到新的考试就做的很差。</p><p><img src="https://img.issey.top/img/202209211122845.png" /></p><p>下图是逻辑回归的过拟合示例：</p><p><img src="https://img.issey.top/img/202209211122089.png" /></p><p>注：红线为正常拟合情况</p><h2 id="导致欠拟合与过拟合的原因">导致欠拟合与过拟合的原因</h2><p><strong>欠拟合（underfitting</strong>）：</p><ul><li><p>特征量太少</p></li><li><p>模型复杂度过低</p></li></ul><p><strong>过拟合（overfitting）：</strong>这里只讨论回归模型的过拟合原因</p><ul><li><p>样本数量太少</p></li><li><p>模型复杂度过高</p></li><li><p>特征量太多</p></li><li><p>样本的噪音数据干扰过大</p></li></ul><h2 id="解决方法">解决方法</h2><p><strong>欠拟合</strong>：</p><p>相对较好解决。</p><ol type="1"><li><p>选择更复杂的模型</p></li><li><p>增加更多特征</p></li><li><p>调整学习率、训练次数等参数</p></li></ol><p>过拟合：</p><ol type="1"><li><p>增加训练数据量，数据量越多，过拟合可能越小</p></li><li><p>减小模型复杂度。</p></li><li><p>减少特征数目。去除某些特征，可以提高模型泛化能力。</p></li><li><p>正则化（推荐）</p></li></ol><h2 id="正则化">正则化</h2><p>这里只对线性回归和逻辑回归的正则化进行说明，其他模型和范数暂不拓展。</p><p>现在有下（右图）的过拟合模型</p><p><img src="https://img.issey.top/img/202209211122413.png" /></p><p>一种解决过拟合的方式是，我们可以手动筛选特征，直接将<spanclass="math inline">\(x^3,x^4\)</span>去除。于是我们可以得到左边的模型。但还有一种不那么暴力的方法，我们如果可以让<spanclass="math inline">\(w_3,w_4\)</span>尽量小，比如0.0000001，那么就相当于消除了<spanclass="math inline">\(x^3,x^4\)</span>。</p><p>这是原来的代价函数：</p><p><span class="math inline">\(J(\vec w,b) =\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2\)</span></p><p>现在，我们让<span class="math inline">\(J(\vecw,b)+1000w_3^2+1000w_4^2\)</span>，如果要让现在的代价函数尽量小，<spanclass="math inline">\(w_3,w_4\)</span>必须足够小。否则代价函数会非常非常大。于是我们就可以求得很小很小的<spanclass="math inline">\(w_3,w_4\)</span>，有效的消除了<spanclass="math inline">\(x^3,x^4\)</span>这两项特征的影响。</p><p>上面这个例子就是正则化的思想，我们正则化了<spanclass="math inline">\(x^3,x^4\)</span>两个特征。</p><p>然而，实际中我们可能有许多种特征，比如100个特征，我们可能分不清哪些特征是重要的，哪些应该被正则化。</p><p>所以正则化一般的实现方式是正则化所有特征。不过一般不会正则化常数b。</p><p>正则化后的线性回归代价函数：</p><p><span class="math inline">\(J(\vec w,b) =\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j =0}^{n-1}w_j^2\)</span></p><p>当<spanclass="math inline">\(\lambda\)</span>过小时，正则化效果较差。比如等于0时没有正则化。当<spanclass="math inline">\(\lambda\)</span>过大时，可能会发生欠拟合。比如<spanclass="math inline">\(\lambda =10^{10}\)</span>，拟合出来的可能是一条几乎平行于x轴的直线<spanclass="math inline">\(f(x) = b\)</span>.</p><h2 id="正则化后的线性回归和逻辑回归">正则化后的线性回归和逻辑回归</h2><h3 id="代价函数">代价函数</h3><p><strong>线性回归</strong></p><p>公式：<span class="math inline">\(J(\vec w,b) =\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j =0}^{n-1}w_j^2\)</span></p><p>其中，<span class="math inline">\(f_{\vec w,b}(x^{(i)}) = \vecw\cdot\vec x^{(i)}+b\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性回归正则化代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_linear_reg</span>(<span class="params">X, y, w, b, lambda_ = <span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        total_cost (scalar):  损失值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = np.dot(X[i],w) + b</span><br><span class="line">        cost = cost + (f_wb_i - y[i])**<span class="number">2</span></span><br><span class="line">    cost = cost / (<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">    reg_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        reg_cost += w[j]**<span class="number">2</span></span><br><span class="line">    reg_cost = (lambda_/(<span class="number">2</span>*m)) * reg_cost</span><br><span class="line"></span><br><span class="line">    total_cost = cost + reg_cost</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><p><strong>逻辑回归</strong></p><p>公式：</p><p><span class="math inline">\(J(\vec w,b) =\frac{1}{m}\sum_{i=0}^{m-1}[-y^{(i)}log(f_{\vec w,b}(\vecx^{(i)})-(1-y^{(i)})log(1-f_{\vec w,b}(\vecx^{(i)})]+\frac{\lambda}{2m}\sum_{j = 0}^{n-1}w_j^2\)</span></p><p>其中，</p><p><span class="math inline">\(f_{\vec w,b}(\vec x^{(i)}) = sigmoid(\vecw \cdot \vec x+b)\)</span></p><p><span class="math inline">\(sigmoid(z) =\frac{1}{1+e^{-z}}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param z: 标量</span></span><br><span class="line"><span class="string">    :return: 标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> g   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑函数正则化代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic_reg</span>(<span class="params">X, y, w, b, lambda_ = <span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        total_cost (scalar):  损失值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z_i = np.dot(X[i], w) + b</span><br><span class="line">        f_wb_i = sigmoid(z_i)</span><br><span class="line">        cost += -y[i] * np.log(f_wb_i) - (<span class="number">1</span> - y[i]) * np.log(<span class="number">1</span> - f_wb_i)</span><br><span class="line"></span><br><span class="line">    cost = cost / m</span><br><span class="line"></span><br><span class="line">    reg_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        reg_cost += (w[j] ** <span class="number">2</span>)</span><br><span class="line">    reg_cost = (lambda_ / (<span class="number">2</span> * m)) * reg_cost</span><br><span class="line"></span><br><span class="line">    total_cost = cost + reg_cost</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><h3 id="梯度计算求偏导">梯度计算（求偏导）</h3><p>线性回归和逻辑回归的梯度计算出来格式都一样，注意里面的<spanclass="math inline">\(f_{\vec w,b}(\vec x)\)</span>指代不同即可。</p><p>公式：<span class="math inline">\(\frac{\partial J(\vecw,b)}{\partial b} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}+\frac {\lambda}{m}w_j\)</span></p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>线性回归中,<span class="math inline">\(f_{\vec w,b}(\vec x^{(i)}) =\vec w \cdot \vec x+b\)</span></p><p>逻辑回归中,<span class="math inline">\(f_{\vec w,b}(x) = sigmoid(\vecw\cdot \vec x+b)\)</span></p><p>线性回归：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_linear_reg</span>(<span class="params">X, y, w, b, lambda_</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        dj_dw (ndarray (n,)): 代价函数对w的偏导</span></span><br><span class="line"><span class="string">        dj_db (scalar): 代价函数对b的偏导</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m, n = X.shape </span><br><span class="line">    dj_dw = np.zeros((n,))</span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        err = (np.dot(X[i], w) + b) - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err * X[i, j]</span><br><span class="line">        dj_db = dj_db + err</span><br><span class="line">    dj_dw = dj_dw / m</span><br><span class="line">    dj_db = dj_db / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dj_dw[j] = dj_dw[j] + (lambda_ / m) * w[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw</span><br></pre></td></tr></table></figure><p>逻辑回归：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逻辑回归梯度计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic_reg</span>(<span class="params">X, y, w, b, lambda_</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X(narray(m,n)): m个样例，n个特征</span></span><br><span class="line"><span class="string">    :param y(ndarray (m,)):目标值</span></span><br><span class="line"><span class="string">    :param w(ndarray (n,)):模型参数</span></span><br><span class="line"><span class="string">    :param b(scalar):模型参数</span></span><br><span class="line"><span class="string">    :param lambda_(scalar):正则化控制量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        dj_dw (ndarray (n,)): 代价函数对w的偏导</span></span><br><span class="line"><span class="string">        dj_db (scalar): 代价函数对b的偏导</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))</span><br><span class="line">    dj_db = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i], w) + b)</span><br><span class="line">        err_i = f_wb_i - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i, j]</span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw / m</span><br><span class="line">    dj_db = dj_db / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dj_dw[j] = dj_dw[j] + (lambda_ / m) * w[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw</span><br></pre></td></tr></table></figure><p>梯度迭代就不写了，之前的文章有详细写过，没什么变化。</p><h2 id="使用正则化后过拟合模型的变化">使用正则化后过拟合模型的变化</h2><p>这是正则化之后的结果：</p><p><img src="https://img.issey.top/img/202209211123110.png" /></p><p><img src="https://img.issey.top/img/202209211123447.png" /></p><p>嗯...对比最开始的两张过拟合，已经好得多了，虽然比起红线模拟的模型还是有一些差距。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型评估 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记4——逻辑回归</title>
      <link href="/article/bba895870268/"/>
      <url>/article/bba895870268/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：请确保你已学会了前几个线性回归的内容。之前涉及的相关概念在此文章不会再提及。</p></blockquote><h1 id="什么是逻辑回归">什么是逻辑回归？</h1><p>逻辑回归是一种广义的线性回归模型，主要用于解决二分类问题。所谓二分类问题，就是比如判断一个邮件是否是垃圾邮件、根据某些特征判断肿瘤是否为恶性肿瘤等问题，我们可以将是/否表示为1/0。简单的二分类数据图如下：</p><p><img src="https://img.issey.top/img/202209211106913.png" /></p><p>（注：左图为只有一个特征的数据模拟图，右图为有两个特征的数据模拟图。）</p><p>以仅有一个特征的二分类（左图）为例，如果我们模拟传统线性回归<spanclass="math inline">\(f(x) = \vec w\cdot \vecx+b\)</span>，并选择0.5作为阈值：当<span class="math inline">\(y \geq0.5\)</span>时我们认为它是种类1，当<spanclass="math inline">\(y&lt;0.5\)</span>时，我们认为它是种类0，那么可以得到以下图像：</p><p><img src="https://img.issey.top/img/202209211106198.png" /></p><p>但是，当我们再加一些数据，它就会模拟成这样：</p><p><img src="https://img.issey.top/img/202209211106951.png" /></p><p>很明显能看出，这样的预测并不准确。</p><p>所以当对二分类问题进行回归分析时，采用传统的线性回归函数进行拟合并不是一个好的方案。于是我们将使用另一种函数——<strong>Sigmoid函数</strong>。</p><h1 id="sigmoid函数">Sigmoid函数</h1><p>Sigmoid函数又称为Logistic函数（逻辑函数），Sigmoid函数输出值在<spanclass="math inline">\((0,1)\)</span>之间，而且可以解决离群点对拟合线性回归的影响，Sigmoid函数在诸多领域都有涉及，这里不再拓展。</p><p>Sigmoid函数表达式：<span class="math inline">\(g(z) =\frac{1}{1+e^{-z}}\)</span></p><p>函数图像：</p><p><img src="https://img.issey.top/img/202209211106626.png" /></p><p><strong>现在使用sigmoid函数拟合上面的例子，当</strong><spanclass="math inline">\(g(z)\geq0.5\)</span><strong>时，我们认为它属于种类1，当</strong><spanclass="math inline">\(g(z)&lt;0.5\)</span><strong>时，我们认为它属于种类0</strong>,于是可以得到下面的图像：</p><p><img src="https://img.issey.top/img/202209211106325.png" /></p><p>（注：橙色的线为决策边界，后面会详细解释。）</p><p>很明显看出该函数拟合的很好。</p><h1 id="决策边界">决策边界</h1><p>上面提到，我们使用sigmoid函数进行预测时，当<spanclass="math inline">\(g(z) \geq 0.5\)</span>，我们认为是种类1，当<spanclass="math inline">\(g(z)&lt;0.5\)</span>时，我们认为是种类0。那么，什么时候<spanclass="math inline">\(g(z)\)</span>等于0.5呢？观察sigmoid函数的图像，当<spanclass="math inline">\(z\)</span>等于0时，<spanclass="math inline">\(g(z) = 0.5\)</span>。所以我们令<spanclass="math inline">\(z = f_{\vec w,b}(\vec x) = \vec w\cdot\vecx+b\)</span>,决策边界即<span class="math inline">\(f_{\vec w,b}(\vec x)= 0\)</span>。</p><p>以上述右图为例（即以有两个特征的二分类为例），决策边界最终求出的函数为<spanclass="math inline">\(f(x) = x_0+x_1 -3\)</span>。当<spanclass="math inline">\(f(x) \geq 0\)</span>，属于种类1；当<spanclass="math inline">\(f(x)&lt;0\)</span>，属于种类0。图像为：</p><p><img src="https://img.issey.top/img/202209211107168.png" /></p><p>可以看到决策边界将两个不同的种类分开了。</p><p>又比如，当<span class="math inline">\(f_{\vec w,b}(\vecx)\)</span>为更复杂的多项式时，可以画出以下图像：</p><p><img src="https://img.issey.top/img/202209211107746.png" /></p><h1 id="逻辑回归的损失函数">逻辑回归的损失函数</h1><h2 id="为什么平方误差模型不可行">为什么平方误差模型不可行？</h2><p>在之前的线性回归中，我们使用平方误差作为我们的代价函数（损失函数）：、</p><p><span class="math display">\[\begin{split}J(\vec{w},b) = \frac {1} {2m}\sum_{i = 0}^{m-1}(\haty^{(i)}-y^{(i)})^2 \\=\frac {1} {2m}\sum_{i = 0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})^2\end{split}\]</span></p><p>其中，<span class="math inline">\((i)\)</span>为样例序号。</p><p>那时<span class="math inline">\(f_{\vec w,b}(\vec x) = \vecw\cdot\vec x+b\)</span>,平方误差代价函数可以平滑下降直到一个最低点。</p><p>下图是一元一次线性回归<span class="math inline">\(f(x) =wx+b\)</span>的代价函数图像：</p><p><img src="https://img.issey.top/img/202209211108683.png" /></p><p>但是逻辑回归的函数为<span class="math inline">\(sigmoid(\vecw\cdot\vec x+b) = \frac{1}{1+e^{-z}}\)</span>，其中，<spanclass="math inline">\(z = \vec w\cdot\vec x+b\)</span>。</p><p>如果再使用平方误差作为代价函数，它的图像将会是凹凸不平的：</p><p><img src="https://img.issey.top/img/202209211108105.png" /></p><p>这意味着梯度下降算法很可能无法找到最低点，会卡在某个极小值点。</p><p><img src="https://img.issey.top/img/202209211108417.png" /></p><p>为了解决这个问题，我们将使用另一种模型作为我们的代价函数：</p><h2 id="对数损失函数">对数损失函数</h2><blockquote><p>为什么使用对数损失函数作为逻辑回归的损失函数而不使用其他函数？</p><p>使用对数损失函数作为逻辑回归的损失函数是由极大似然估计推导所得，这里不进行拓展。</p></blockquote><h3 id="单个样例损失">单个样例损失：</h3><p><span class="math display">\[L(\hat y,y) = \left\{\begin{aligned}-log(\hat y)        if\quad y = 1\\-log(1-\hat y)        if\quad y = 0\\\end{aligned}\right.\]</span></p><p>其中，<span class="math inline">\(\hat y = g_{\vec w,b}(\vec x) =sigmoid(\vec w\cdot \vec x+b)，\hat y\in (0,1)\)</span></p><p>解释一下含义：</p><p>当真实值为1时，图像为：</p><p><img src="https://img.issey.top/img/202209211109293.png" /></p><p>可以看出，在真实值y为1的情况下：当预测值<spanclass="math inline">\(\haty\)</span>接近1时，计算出的损失很小；而当<spanclass="math inline">\(\haty\)</span>接近0时，计算出的损失很大很大。换成人话，就是比如说某人真实情况是“很胖”，但是预测出的却是“很廋”，这时预测值和真实值的差距就很大很大。</p><p>同理，当真实值为0时，图像为：</p><p><img src="https://img.issey.top/img/202209211109732.png" /></p><p>在真实值y为0的情况下：当预测值<span class="math inline">\(\haty\)</span>接近0时，计算出的损失很小；而当<spanclass="math inline">\(\hat y\)</span>接近0时，计算出的损失很大很大。</p><p>化简<span class="math inline">\(L(\hat y,y)\)</span>,可得<spanclass="math inline">\(L(\hat y,y) = -ylog(\hat y)-(1-y)log(1-\haty)\)</span></p><h3 id="逻辑回归损失函数">逻辑回归损失函数</h3><p>上式求和即可得：</p><p>公式：<span class="math inline">\(J(\vec w,b) = \frac 1 m\sum_{i=0}^{m-1}L(g_{\vec w,b}(\vec x),y)\)</span></p><p>让我们用这个新的代价函数模拟一下最开始左图的代价函数模型：</p><p><img src="https://img.issey.top/img/202209211110457.png" /></p><p>现在这个图像很适合使用梯度下降算法来找到它的最低点。</p><h1 id="梯度下降算法">梯度下降算法</h1><p>步骤与线性回归相同，同时进行以下直到收敛：</p><p><span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><p>其中，</p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial w_j}= \frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>这两个偏导求出来的公式和线性回归的几乎长得一样，但是并不代表他们一样。</p><p>这里<span class="math inline">\(g_{\vec w,b}(x) = sigmoid(\vec w\cdot\vec x+b)\)</span></p><blockquote><p>求导过程请参考文章：<ahref="https://blog.csdn.net/Piratesa/article/details/100586729">逻辑回归梯度下降法</a></p></blockquote><p>现在只需要利用该算法求得<span class="math inline">\(\vecw,b\)</span>即可。</p><blockquote><p>关于多分类问题：多分类问题也可以使用逻辑回归解决。之后会出专门的文章，这里暂时不写。</p></blockquote><h1 id="补充f1-score评价指标">补充：F1-score评价指标</h1><h2 id="f1-score简介">F1-Score简介</h2><p>F1分数（F1 Score），是<ahref="https://so.csdn.net/so/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6&amp;spm=1001.2101.3001.7020">统计学</a>中用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的精确率和召回率。F1分数可以看作是模型精确率和召回率的一种加权平均，它的最大值是1，最小值是0。</p><h3 id="相关概念">相关概念</h3><p>下面先介绍几个概念：</p><ul><li><p>TP（rue Positive）：正样本被判定为正样本</p></li><li><p>FP（False Positive）：负样本被判定为正样本</p></li><li><p>TN（True Negative）：负样本被判定为负样本</p></li><li><p>FN（False Negative）：正样本被判定为负样本</p></li></ul><p>精确度/查准率：指分类器预测为正例中正样本所占比重：</p><p><span class="math inline">\(Precision = \frac {TP}{TP+FP}\)</span></p><p>召回率/查全率：指预测为正例占总正例比重：</p><p><span class="math inline">\(Recall = \frac {TP}{TP+FN}\)</span></p><p>F-Score算法将同时使用以上两个公式，此外，介绍另一种常用的准确率概念：</p><p>准确率，指分类器判断正确的占总样本的比重：</p><p><span class="math inline">\(Accuracy = \frac{TP+TN}{TP+TN+FP+FN}\)</span></p><h3 id="f-score">F-Score</h3><p>具体来源等等就不拓展了，有兴趣可以自查。</p><p>F-Score是可以综合考虑精确度（Precision）和召回率（Recall）的调和值，公式如下：</p><p><span class="math inline">\(F Score =(1+\beta^2)*\frac{Precision*Recall}{\beta^2Precision+Recall}\)</span></p><p>当<spanclass="math inline">\(\beta=1\)</span>时，被称为F1-Score或F1-Measure。此时精确度和召回率权重相同。</p><p>当我们认为精确度更重要，调整<spanclass="math inline">\(\beta\)</span>&lt;1；</p><p>当我们认为召回率更重要，调整<spanclass="math inline">\(\beta\)</span>&gt;1。</p><h1 id="示例及代码">示例及代码：</h1><p>数据来源：</p><p>https://www.kaggle.com/datasets/muratkokludataset/pumpkin-seeds-dataset</p><h2 id="问题描述">问题描述：</h2><p>现在有两类南瓜种子（CERCEVELIK,URGUP_SIVRISI）以及它们的一些特征：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@ATTRIBUTE Area    INTEGER</span><br><span class="line">@ATTRIBUTE Perimeter    REAL</span><br><span class="line">@ATTRIBUTE Major_Axis_Length    REAL</span><br><span class="line">@ATTRIBUTE Minor_Axis_Length    REAL</span><br><span class="line">@ATTRIBUTE Convex_Area    INTEGER</span><br><span class="line">@ATTRIBUTE Equiv_Diameter    REAL</span><br><span class="line">@ATTRIBUTE Eccentricity    REAL</span><br><span class="line">@ATTRIBUTE Solidity    REAL</span><br><span class="line">@ATTRIBUTE Extent    REAL</span><br><span class="line">@ATTRIBUTE Roundness    REAL</span><br><span class="line">@ATTRIBUTE Aspect_Ration    REAL</span><br><span class="line">@ATTRIBUTE Compactness    REAL</span><br></pre></td></tr></table></figure><p>现在请根据已知数据集对这两类南瓜种子进行（逻辑回归）分类并判断准确率。</p><blockquote><p>代码说明：会使用sklearn进行数据分割以及模型评价（F1-score），逻辑回归部分全部自主实现。</p></blockquote><p>所需要的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math,copy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><h2 id="数据预处理">数据预处理</h2><h3 id="特征选择与数据集拆分">特征选择与数据集拆分</h3><p>为了可视化，这里只选择其中两个特征作为特征集（Major_Axis_Length、Minor_Axis_Length）。</p><p>导入和提取数据、拆分训练集和数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">data = pd.read_excel(<span class="string">r&#x27;.\Pumpkin_Seeds_Dataset\Pumpkin_Seeds_Dataset.xlsx&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换数据</span></span><br><span class="line">color = []</span><br><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">&#x27;Class&#x27;</span>]:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&#x27;Çerçevelik&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="string">&#x27;Ürgüp Sivrisi&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取所需数据</span></span><br><span class="line">x_1 = np.array(df[<span class="string">&#x27;Major_Axis_Length&#x27;</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">x_2 = np.array(df[<span class="string">&#x27;Minor_Axis_Length&#x27;</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X_features = np.c_[x_1,x_2]</span><br><span class="line">Y_target = np.array(label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X_features,Y_target,train_size=<span class="number">0.5</span>,random_state=<span class="number">45</span>)</span><br></pre></td></tr></table></figure><h3 id="特征缩放z-score标准化">特征缩放（Z-score标准化）</h3><p>注意：特征缩放一定要用对地方，应该在拆分完训练集和测试集后，<strong>仅对训练集使用，不应该把训练集和测试集放在一起标准化</strong>，对测试集的操作应该是在对训练集标准化后，使用通过训练集标准化时计算得到的平均值、方差等进行标准化。</p><p>这里使用Z-score标准化进行特征缩放。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Z-score标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Zscore</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;x是(m,n)的矩阵，m为样本个数，n为特征数目&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 找每列(特征)均值</span></span><br><span class="line">    mu = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 找每列(特征)标准差</span></span><br><span class="line">    sigma = np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = (X - mu) / sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm,mu,sigma</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准化特征集</span></span><br><span class="line">X_train,mu,sigma = Zscore(X_train)</span><br><span class="line"><span class="comment"># 绘制训练集</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y_train:</span><br><span class="line">     <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">         color.append(<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         color.append(<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">plt.scatter(X_train[:,<span class="number">0</span>],X_train[:,<span class="number">1</span>],color = color)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Major_Axis_Length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Minor_Axis_Length&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>标准化后的训练集：</p><p><img src="https://img.issey.top/img/202209211110611.png" /></p><p>注：橘色为Çerçevelik种类，蓝色为Ürgüp Sivrisi种类。</p><h2 id="实现逻辑回归">实现逻辑回归</h2><h3 id="sigmoid函数-1">sigmoid函数</h3><p>公式：<span class="math inline">\(g(z) =\frac{1}{1+e^{-z}}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param z: 标量</span></span><br><span class="line"><span class="string">    :return: 标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure><h3 id="损失函数">损失函数</h3><p>公式：<span class="math inline">\(J(\vec w,b) = \frac 1 m\sum_{i=0}^{m-1}L(g_{\vec w,b}(\vec x),y)\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逻辑回归损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 特征集，矩阵</span></span><br><span class="line"><span class="string">    :param y: 目标值，列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return: 对数损失值，标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z_i = np.dot(X[i],w)+b</span><br><span class="line">        g_wb_i = sigmoid(z_i)</span><br><span class="line">        cost += -y[i]*np.log(g_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-g_wb_i)</span><br><span class="line">    cost = cost/m</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="梯度计算函数求偏导">梯度计算函数（求偏导）</h3><p>公式：</p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p><span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b} =\frac{1}{m}\sum_{i=0}^{m-1}(g_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>其中，<span class="math inline">\(g_{\vec w,b}(x) = sigmoid(\vecw\cdot \vec x+b)\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 特征集，矩阵</span></span><br><span class="line"><span class="string">    :param y: 目标值，列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        dj_dw: 对数损失函数对向量w的偏导</span></span><br><span class="line"><span class="string">        dj_db: 对数损失函数对b的偏导</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))</span><br><span class="line">    dj_db = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        g_wb_x = sigmoid(np.dot(X[i],w)+b)</span><br><span class="line">        dj_db = dj_db + g_wb_x - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + (g_wb_x - y[i])*X[i,j]</span><br><span class="line">    dj_dw = dj_dw/m</span><br><span class="line">    dj_db = dj_db/m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dj_dw,dj_db</span><br></pre></td></tr></table></figure><h3 id="梯度迭代函数">梯度迭代函数</h3><p>公式：</p><p><span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_regression</span>(<span class="params">X_train,y_train,alpha,num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X_train: 特征集，矩阵</span></span><br><span class="line"><span class="string">    :param y_train: 目标值，列表</span></span><br><span class="line"><span class="string">    :param alpha: 学习率，标量</span></span><br><span class="line"><span class="string">    :param num_iters: 训练次数，标量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        w: 训练得出的w，向量</span></span><br><span class="line"><span class="string">        b: 训练得出的b，标量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m,n = X_train.shape</span><br><span class="line">    init_w = np.zeros((n,)) <span class="comment"># n个特征，所以w有n个</span></span><br><span class="line">    init_b = <span class="number">0</span></span><br><span class="line">    w = copy.deepcopy(init_w)</span><br><span class="line">    b = init_b</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(i)</span><br><span class="line">        dj_dw,dj_db = compute_gradient_logistic(X_train,y_train,w,b)</span><br><span class="line">        w = w - alpha*dj_dw</span><br><span class="line">        b = b - alpha*dj_db</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h3 id="训练数据集绘制拟合决策边界">训练数据集，绘制拟合决策边界</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_w,model_b = logistic_regression(X_train,y_train,alpha=<span class="number">0.4</span>,num_iters=<span class="number">10000</span>)</span><br><span class="line">x_line = np.c_[np.arange(-<span class="number">4</span>,<span class="number">4</span>,<span class="number">0.1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>),np.arange(-<span class="number">4</span>,<span class="number">4</span>,<span class="number">0.1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)]</span><br><span class="line"><span class="built_in">print</span>(model_w)</span><br><span class="line">y_line = np.dot(x_line,model_w)+model_b</span><br><span class="line"></span><br><span class="line">plt.plot(x_line,y_line,label = <span class="string">&#x27;Predicted Value&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这是当训练次数为10000时得出的决策边界：</p><p><img src="https://img.issey.top/img/202209211111755.png" /></p><h2 id="模型预测和评价">模型预测和评价</h2><p>算出<span class="math inline">\(\vec w,b\)</span>后，代入<spanclass="math inline">\(sigmoid(\vec w\cdot\vecx+b)\)</span>，阈值为0.5。大于等于0.5为1类，小于0.5为0类。</p><p>注意要先将特征测试集标准化。</p><p>这里使用F1-Score（F1分数）进行评价。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line"><span class="keyword">def</span>  <span class="title function_">model_predict</span>(<span class="params">X,w,b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 测试集</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return: 预测值（列表）</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">        <span class="keyword">if</span> sigmoid(np.dot(i,w)+b) &gt;= <span class="number">0.5</span>:</span><br><span class="line">            y.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">X_test = (X_test - mu) / sigma</span><br><span class="line">y_predict = model_predict(X_test,model_w,model_b)</span><br><span class="line">f1_score = f1_score(y_test,y_predict,average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;F1分数为:<span class="subst">&#123;<span class="built_in">round</span>(f1_score,<span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://img.issey.top/img/202209211111674.png" /></p><h3 id="补充现在将特征集扩大到所有">补充：现在将特征集扩大到所有</h3><p>直接上结果：</p><p><img src="https://img.issey.top/img/202209211111590.png" /></p><p>稍微比原来准确了一丢丢。</p><h1 id="补充使用sklearn完成逻辑回归">补充：使用sklearn完成逻辑回归</h1><p>前面讲了一大堆，实际上sklearn几行代码搞定~</p><p><del>泪目</del></p><p>还是刚才那个数据（扩大到所有特征后的），直接上代码!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math,copy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">data = pd.read_excel(<span class="string">r&#x27;.\Pumpkin_Seeds_Dataset\Pumpkin_Seeds_Dataset.xlsx&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">&#x27;Class&#x27;</span>]:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&#x27;Çerçevelik&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="string">&#x27;Ürgüp Sivrisi&#x27;</span>:</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_features_bk = np.array(df)</span><br><span class="line">X_features = X_features_bk[:,:-<span class="number">1</span>]</span><br><span class="line">X_features = np.float32(X_features)</span><br><span class="line">Y_target = np.array(label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分训练集和测试集</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X_features,Y_target,train_size=<span class="number">0.5</span>,random_state=<span class="number">45</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn进行Z-score标准化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)  <span class="comment"># 标准化训练集X</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制训练集</span></span><br><span class="line">color = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y_train:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">        color.append(<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        color.append(<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">plt.scatter(X_train[:, <span class="number">2</span>], X_train[:, <span class="number">3</span>], color=color)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Major_Axis_Length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Minor_Axis_Length&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练逻辑回归模型</span></span><br><span class="line">lr_model = LogisticRegression()</span><br><span class="line">lr_model.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化测试集x</span></span><br><span class="line"><span class="comment"># 只有训练集才fit_transform，测试集是transform，原因上面自己写代码的时候说过了</span></span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = lr_model.predict(X_test)</span><br><span class="line"><span class="comment"># F1-Score评估</span></span><br><span class="line">f1_score = f1_score(y_test,y_pred,average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;F1分数为:<span class="subst">&#123;<span class="built_in">round</span>(f1_score,<span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>中间我们拿出之前那两列来画了下图，这个图是同样的随机种子，采用sklearn的Z-score标准化后得出的图像：</p><p><img src="https://img.issey.top/img/202209211111790.png" /></p><p>对比我们之前自己处理的数据，只能说，完全一致好吧。</p><p>然后来看看结果：</p><p><img src="https://img.issey.top/img/202209211111682.png" /></p><p>甚至比我们自己写的差了这么一丢丢。导致这个的原因是它的学习率和训练次数和我们选的不一样，不过计算速度比我们快很多，我估计它训练次数选的比较少。如果我们调整自己的参数，也可以达到同样的效果！</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记3——多项式回归</title>
      <link href="/article/31f58288ca8b/"/>
      <url>/article/31f58288ca8b/</url>
      
        <content type="html"><![CDATA[<h1 id="多项式回归">多项式回归</h1><blockquote><p>多项式回归虽然不再用直线拟合，但也是线性回归的一种，可以转化为多元线性回归，利用多元线性回归的函数解决。所以请确保熟悉多元线性回归相关知识点：<ahref="https://www.issey.top/article/92ab78771d6d/">机器学习笔记2——多元线性回归| issey的博客</a></p></blockquote><p>在学习多项式回归之前，你可能需要先了解以下内容：</p><h2 id="前置知识">前置知识</h2><p><ahref="https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%BC%8F/10660961?fr=aladdin">多项式</a></p><p><ahref="https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0/10686272?fr=aladdin">多项式函数</a></p><h3 id="numpy-c_函数">Numpy c_函数</h3><p>该函数可以实现按列拼接矩阵，具体用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">X = np.c_[x,x**<span class="number">2</span>,x**<span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;np.c_(x,x**2,x**3):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209210054285.png" /></p><h2 id="问题引入">问题引入</h2><p>现实生活中呈（狭义）线性关系的事物联系很少，绝大部分都是（狭义）非线性的，即呈曲线形式的关系。所以我们需要引入多项式回归来更好的拟合数据。</p><h2 id="多项式回归函数">多项式回归函数</h2><p>多项式回归又分为多元多项式回归和一元多项式回归：</p><p>一元多项式：<span class="math inline">\(f(x) =w_nx^n+w_{n-1}x^{n-1}+...+w_2x^2+w_1x+b\)</span></p><p>多元多项式：这里只举二元二次多项式</p><p><span class="math inline">\(f(x_1,x_2) =w_1x_1^2+w_2x^2+w_3x_1x_2+w_4x_1+w_5x_5+b\)</span></p><p>不管它是什么多项式，在处理上不同的地方都只是在数据预处理上，核心方法不变。这里为了绘图，以一元多项式进行讲解。</p><h2 id="核心思路">核心思路</h2><p>令<span class="math inline">\(x_1 = x\)</span>,<spanclass="math inline">\(x_2 = x^2\)</span>,...,<spanclass="math inline">\(x_n =x^n\)</span>,于是多项式回归转化成了多元线性回归。然后套用多元线性回归的函数求解向量<spanclass="math inline">\(\vec w\)</span>和常量b即可。</p><h2 id="示例">示例</h2><blockquote><p>注：以下函数均来自上篇文章<ahref="https://www.issey.top/article/92ab78771d6d/">机器学习笔记2——多元线性回归| issey的博客</a></p><p>请配合使用！！！</p><p>Zscore()：Z-score标准化</p><p>gradient_descent()：拟合回归函数,为了更好用，把初始化w和b的函数重新写了一下。</p></blockquote><p>gradient_descent()初始化w，b修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m, n = X.shape</span><br><span class="line"><span class="comment"># initialize parameters</span></span><br><span class="line">init_w = np.zeros(n)</span><br><span class="line">init_b = <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="数据生成">数据生成</h3><p>以下代码可以生成该示例的训练集数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_generation</span>():</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        X: 自变量训练集（矩阵）</span></span><br><span class="line"><span class="string">        Y: 应变量训练集（一维）</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>,<span class="number">40</span>,<span class="number">1</span>)</span><br><span class="line">    y = <span class="number">0.25</span>*x**<span class="number">2</span>-<span class="number">0.20</span>*x**<span class="number">3</span>+<span class="number">0.0055</span>*x**<span class="number">4</span></span><br><span class="line">    <span class="comment"># 要将X训练集转化为(m,n)的矩阵，即m个例子，n个特征，现在是一元，所以只有一列</span></span><br><span class="line">    x = x.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 生成随机数，稍微打乱y</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    y = y + np.random.random(<span class="number">40</span>)*<span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure><h3 id="转化多项式回归训练集">转化多项式回归训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">x_transform</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param x:转化前的X训练集</span></span><br><span class="line"><span class="string">    :return: 转化后的X训练集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    X_train = np.c_[x,x**<span class="number">2</span>,x**<span class="number">3</span>,x**<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_train</span><br></pre></td></tr></table></figure><h3 id="预测和绘图">预测和绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成数据并绘制训练集图</span></span><br><span class="line">    x_train,y_train = data_generation()</span><br><span class="line">    plt.scatter(x_train,y_train,marker=<span class="string">&#x27;x&#x27;</span>,c=<span class="string">&#x27;r&#x27;</span>,label = <span class="string">&#x27;Actual Value&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;y&quot;</span>)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line">    <span class="comment"># 将x训练集转化为多元线性训练集</span></span><br><span class="line">    x_train_transform = x_transform(x_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测并绘图</span></span><br><span class="line">    model_w,model_b = gradient_descent(x_train_transform,y_train,alpha=<span class="number">0.000000000003</span>,num_iters=<span class="number">100000</span>)</span><br><span class="line">    plt.plot(x_train,np.dot(x_train_transform,model_w) + model_b,label=<span class="string">&quot;Predicted Value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://img.issey.top/img/202209210101983.png" /></p><h3 id="特征缩放">特征缩放</h3><p>因为转化为多元一次回归后，会发现每个特征的范围差的特别大（比如<spanclass="math inline">\(x^4\)</span>和<spanclass="math inline">\(x\)</span>），为了照顾取值大的特征，学习率必须设置的非常小，所以梯度下降特别慢。这时，就要用到之前说过的特征缩放了。</p><p>特征缩放后的预测和绘图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将x训练集转化为多元线性训练集</span></span><br><span class="line">x_train_transform = x_transform(x_train)</span><br><span class="line">x_train_transform, mu, sigma = Zscore(x_train_transform)</span><br><span class="line"><span class="comment"># 预测并绘图</span></span><br><span class="line">model_w,model_b = gradient_descent(x_train_transform,y_train,alpha=<span class="number">0.5</span>,num_iters=<span class="number">10000</span>)</span><br><span class="line">plt.plot(x_train,np.dot(x_train_transform,model_w) + model_b,label=<span class="string">&quot;Predicted Value&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>对比学习率和迭代次数，根本不是一个级别的。而且在更短时间内模拟的更好：</p><p><img src="https://img.issey.top/img/202209210055530.png" /></p><p>所以特征缩放很重要！！</p><h2 id="补充关于多项式的次数选择">补充：关于多项式的次数选择</h2><blockquote><p>次数的选择：多项式函数有多种，一般来说，需要先观察数据的形状，再去决定选用什么形式的多项式函数来处理问题。比如，从数据的散点图观察，如果有一个“弯”，就可以考虑用二次多项式；有两个“弯”，可以考虑用三次多项式；有三个“弯”，则考虑用四次多项式，以此类推。当然，如果预先知道数据的属性，则有多少个</p><p>虽然真实的回归函数不一定是某个次数的多项式，但只要拟合的好，用适当的多项式来近似模拟真实的回归函数是可行的。</p><p>原文链接：<ahref="https://blog.csdn.net/weixin_44225602/article/details/112752565">多项式回归详解从零开始 从理论到实践</a></p></blockquote><p>稍微尝试了一下这个规律，刚才我构造的函数虽然是四次方的，但是只用三次的多项式也可以模拟出来效果较好的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.c_[x,x**<span class="number">2</span>,x**<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p><img src="https://img.issey.top/img/202209210055887.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记2——多元线性回归</title>
      <link href="/article/92ab78771d6d/"/>
      <url>/article/92ab78771d6d/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐阅读">推荐阅读</h1><ul><li><ahref="https://www.issey.top/article/c4e174e36609/">机器学习笔记1——一元线性回归| issey的博客</a></li></ul><h1 id="前言">前言</h1><p>​多元线性回归总体思路和一元线性回归相同，都是代价函数+梯度下降，所以中心思想参考一元线性回归。</p><p>​在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只用一个自变量进行预测或估计更有效，更符合实际。</p><p>​ 在开始之前，你可能需要先了解以下知识：</p><h1 id="前置知识">前置知识</h1><h2 id="向量化">向量化</h2><p>​当我们想自己实现向量的点乘时，通常会想到利用for循环来完成，例如有<spanclass="math inline">\(\vec{a}\cdot\vec{b}\)</span>，可以写为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(a.shape[<span class="number">0</span>]):</span><br><span class="line">    x = x + a[i] * b[i]</span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>​然而，Python的Numpy库提供了一个dot()函数可以帮助我们进行向量化计算，作用是加快向量的运算速度，在数据量较大时效率会明显提高。其原理是Numpy利用了计算机底层硬件单指令多数据(SIMD)管道，这在数据集非常大的机器学习中至关重要。所以，向量化是机器学习中一个非常重要和实用的方法。</p><p>​下图是使用Numpy的dot函数与自己利用for循环实现的向量点乘分别对长度各为10000000的向量a、b点乘运行时间比较：</p><p><img src="https://img.issey.top/img/202209202204205.png" /></p><h2 id="特征缩放">特征缩放</h2><h3 id="为什么要特征缩放">为什么要特征缩放</h3><p>​当有多个特征时，在有的算法中，我们需要使这些特征具有相似的尺度（无量纲化）。</p><p>​ 这里介绍特征缩放在多元线性回归中的作用。</p><p>​拿下面”问题引入“里得数据来说，各个特征的范围差距太大，我们将每个特征对价格的影响可视化，可以看出哪些因素对价格影响更大。会得到以下图像：</p><p><img src="https://img.issey.top/img/202209202204109.png" /></p><p>​由于各个特征的数量差距过大，代价函数的等高线将会是扁长的，在梯度下降时也会是曲折的，而且计算时长相对会很长（<strong>因为学习率是通用的，为了照顾尺度大的特征，学习率必须设置的很小，学习率越小，下降速度就越慢</strong>）：</p><p><img src="https://img.issey.top/img/202209202205655.png" /></p><p>​特征缩放将每个特征的范围统一，可以使梯度下降变”平滑“，并且大大提高计算速度（<strong>因为可以调大学习率</strong>）。</p><p><img src="https://img.issey.top/img/202209202205070.png" /></p><h3 id="特征缩放的方法">特征缩放的方法</h3><p>​ 特征缩放的方法有许多种，这里介绍两种：</p><h4 id="均值归一化">均值归一化</h4><p>​ 公式：<span class="math inline">\(x_i = \frac{x_i -\mu}{max-min}\)</span></p><p>​ 其中，<spanclass="math inline">\(\mu\)</span>为样本中该特征的均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均值归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MeanNormalization</span>(<span class="params">x</span>):</span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;x为列表&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> [(<span class="built_in">float</span>(i)-np.mean(x))/<span class="built_in">float</span>(<span class="built_in">max</span>(x)-<span class="built_in">min</span>(x)) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br></pre></td></tr></table></figure><h4 id="z-score标准化推荐">Z-score标准化(推荐)</h4><p>​ 公式：<span class="math inline">\(x_j^{(i)} =\frac{x_j^{(i)}-\mu_j}{\delta_j}\)</span></p><p>​ 其中，<span class="math inline">\(j\)</span>为<spanclass="math inline">\(X\)</span>矩阵中的特征（或列),<spanclass="math inline">\((i)\)</span>为样本序号。<spanclass="math inline">\(u_j\)</span>为特征<spanclass="math inline">\(j\)</span>的均值，<spanclass="math inline">\(\delta_j\)</span>为特征<spanclass="math inline">\(j\)</span>的标准差。</p><p>​ <span class="math inline">\(\mu_j =\frac{1}{m}\sum_{i=0}^{m-1}x_j^{(i)}\)</span></p><p>​ <span class="math inline">\(\delta_j^2 =\frac{1}{m}\sum_{i=0}^{m-1}(x_j^{(i)}-\mu_j)^2\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Z-score标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Zscore</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;x是(m,n)的矩阵，m为样本个数，n为特征数目&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 找每列(特征)均值</span></span><br><span class="line">    mu = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 找每列(特征)标准差</span></span><br><span class="line">    sigma = np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = (X - mu) / sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm</span><br></pre></td></tr></table></figure><p>​ 或者使用sklearn:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># X为同上矩阵</span></span><br><span class="line">X_norm = preprocessing.scale(X)</span><br></pre></td></tr></table></figure><hr /><h1 id="问题引入">问题引入</h1><p>​示例：现在你有以下数据，请利用这些值构建一个线性回归模型，并预测一栋1200平米，3间卧室，1层，年龄为40年的房屋的价格。</p><table><thead><tr class="header"><th>面积(平方)</th><th>卧室数量</th><th>楼层数</th><th>房屋年龄</th><th>价格（1000s dollars）</th></tr></thead><tbody><tr class="odd"><td>952</td><td>2</td><td>1</td><td>65</td><td>271.5</td></tr><tr class="even"><td>1244</td><td>3</td><td>2</td><td>64</td><td>300</td></tr><tr class="odd"><td>1947</td><td>3</td><td>2</td><td>17</td><td>509.8</td></tr><tr class="even"><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td></tr></tbody></table><h1 id="多元线性回归模型">多元线性回归模型</h1><h2 id="多元线性回归函数">多元线性回归函数</h2><p>​对于上面提到的数据，一共有四种特征（面积，卧室数量，楼层，房屋面积），记为<spanclass="math inline">\(x_1,x_2,x_3,x_4\)</span>,每个特征分别需要一个<spanclass="math inline">\(w\)</span>,所以对应的线性回归函数为<spanclass="math inline">\(f(x) = w_1x_1+w_2x_2+w_3x_3+w_4x_4+b\)</span>.</p><p>​ 推广到一般多元线性回归函数，即：</p><p>​ <span class="math inline">\(f(x) = w_1x_1+...+w_nx_n+b\)</span>,    其中，n为特征数量。</p><p>​ 观察<span class="math inline">\(f(x)\)</span>,我们发现可以将<spanclass="math inline">\(w\)</span>看作一列，<spanclass="math inline">\(x\)</span>看作一列。于是<spanclass="math inline">\(f(x)\)</span>又可以写为：<spanclass="math inline">\(f_{\vec{w},b}(\vec{x}) =\vec{w}\cdot\vec{x}+b\)</span>,   （注意为点乘）</p><p>​ 这样我们的目标便是利用已知数据通过梯度下降算法找到最合适的<spanclass="math inline">\(\vec{w}\)</span>和b。</p><h2 id="转化为矩阵">转化为矩阵</h2><p>​ 我们可以将训练集转化为<spanclass="math inline">\((m,n)\)</span>的矩阵，m表示示例，n表示特征，于是训练集X可以写为：</p><p><span class="math display">\[\begin{pmatrix} x_{0}^{(0)}&amp;x_{1}^{(0)}&amp;...&amp; x_{n-1}^{(0)}\\ x_{0}^{(1)}&amp;x_{1}^{(1)}&amp;...&amp; x_{n-1}^{(1)}\\...\\x_{0}^{(m-1)}&amp;x_{1}^{(m-1)}&amp;...&amp;x_{n-1}^{(m-1)}\end{pmatrix} \quad\]</span> ​ 注：<spanclass="math inline">\(\vec{x}^{(i)}\)</span>表示含有第i个示例的<strong>向量</strong>，<spanclass="math inline">\(x_j^{(i)}\)</span>表示第i个示例的第j个特征。因为每种特征对应一个<spanclass="math inline">\(w_i\)</span>,所以有向量：</p><p><span class="math display">\[\vec{w} = \begin{pmatrix}w_0\\w_1\\...\\w_{n-1}\end{pmatrix} \quad\]</span></p><h2 id="多元线性回归模型的代价函数">多元线性回归模型的代价函数</h2><p>​ 因为<span class="math inline">\(\hat{y}^{(i)} =f_{\vec{w},b}(\vec{x}^{(i)})=\vec{w}\cdot\vec{x}^{(i)}+b\)</span>,    其中<spanclass="math inline">\(\hat{y}^{(i)}\)</span>为将第i个示例的向量带入回归函数得出的预测值。那么根据一元线性回归的代价函数的定义，可得多元线性回归模型的代价函数为：</p><p>​ <span class="math inline">\(J(\vec{w},b) =\frac{1}{2m}\sum_{i=0}^{m-1}(\hat{y}^{(i)}-y^{(i)})^2=\frac{1}{2m}\sum_{i=0}^{m-1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2\)</span></p><h2 id="多元线性回归模型梯度下降函数">多元线性回归模型梯度下降函数</h2><p>​ 重复同时进行下列行为直到收敛：</p><p>​ <span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p>​ <span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><p>​ 其中，a为学习率。化简偏导得：</p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partialw_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b}= \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><p>​ 其中，n是特征数量，m是训练集示例数量。</p><p>​ 收敛时的<span class="math inline">\(\vec w,b\)</span>即为所求。</p><h1 id="问题解析含代码">问题解析（含代码）</h1><h2 id="导入并标准化训练集">导入并标准化训练集</h2><p>​数据就不上传了，如果想动手测验，直接搞个矩阵把上面那三行当成训练集就行。</p><p><strong>注意标准化时需要把均值和标准差也返回过去，便于对测试数据进行缩放。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Z-score标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Zscore</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;X是(m,n)的矩阵，m为样本个数，n为特征数目&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找每列(特征)均值</span></span><br><span class="line">    mu = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 找每列(特征)标准差</span></span><br><span class="line">    sigma = np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = (X - mu) / sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm,mu,sigma</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;houses.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    X_train = data[:,:<span class="number">4</span>]</span><br><span class="line">    Y_train = np.ravel(data[:,<span class="number">4</span>:<span class="number">5</span>]) <span class="comment"># 把Y从列转为一维数组</span></span><br><span class="line">    <span class="comment"># 将X训练集拿去标准化</span></span><br><span class="line">    X_train = Zscore(X_train)</span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br></pre></td></tr></table></figure><h2 id="多元线性代价函数">多元线性代价函数</h2><p>​ 这里只是把代码写出来，实际上计算回归函数时用不上。</p><p>​ 对应公式：<span class="math inline">\(J(\vec{w},b) =\frac{1}{2m}\sum_{i=0}^{m-1}(\hat{y}^{(i)}-y^{(i)})^2=\frac{1}{2m}\sum_{i=0}^{m-1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param X: 矩阵，每一行代表一个示例向量</span></span><br><span class="line"><span class="string">    :param y: 列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return: cost(标量)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_x_i = np.dot(X[i],w)</span><br><span class="line">        cost = cost + (f_x_i - y[i])**<span class="number">2</span></span><br><span class="line">    cost = cost/(<span class="number">2</span>*m)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="梯度计算函数">梯度计算函数</h2><p>​ 对应公式：</p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partialw_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})x_j^{(i)}\)</span></p><p>​ <span class="math inline">\(\frac{\partial J(\vec w,b)}{\partial b}= \frac{1}{m}\sum_{i=0}^{m-1}(f_{\vec w,b}(\vecx^{(i)})-y^{(i)})\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X: 矩阵，每一行代表一个示例向量</span></span><br><span class="line"><span class="string">    :param y: 列表</span></span><br><span class="line"><span class="string">    :param w: 向量</span></span><br><span class="line"><span class="string">    :param b: 标量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        sum_dw(标量，wj对于代价函数的偏导)</span></span><br><span class="line"><span class="string">        sum_db(标量，b对于代价函数的偏导)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    sum_dw = np.zeros((n,))</span><br><span class="line">    sum_db = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        err = (np.dot(X[i],w) + b) - y[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sum_dw[j] = sum_dw[j] + err*X[i,j]</span><br><span class="line">        sum_db = sum_db + err</span><br><span class="line">    sum_dw = sum_dw/m</span><br><span class="line">    sum_db = sum_db/m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sum_dw,sum_db</span><br></pre></td></tr></table></figure><h2 id="梯度迭代函数">梯度迭代函数</h2><p>​ 对应公式：</p><p>     重复同时进行下列行为直到收敛。</p><p>     <span class="math inline">\(w_j = w_j - a\frac{\partial J(\vecw,b)}{\partial w_j},\quad j = 0,1,...,n-1\)</span></p><p>     <span class="math inline">\(b = b - a\frac{\partial J(\vecw,b)}{\partial b}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w_init, b_init,alpha, num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param X: 矩阵，每一行代表一个示例向量</span></span><br><span class="line"><span class="string">    :param y: 列表</span></span><br><span class="line"><span class="string">    :param w_init: w初始值，向量</span></span><br><span class="line"><span class="string">    :param b_init: b初始值，标量</span></span><br><span class="line"><span class="string">    :param alpha: 学习率</span></span><br><span class="line"><span class="string">    :param num_iters: 迭代次数</span></span><br><span class="line"><span class="string">    :return: 训练出的w和b</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    w = copy.deepcopy(w_init)</span><br><span class="line">    b = b_init</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        dw_j, db = compute_gradient(X, y, w, b)</span><br><span class="line">        w = w - alpha * dw_j</span><br><span class="line">        b = b - alpha * db</span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h2id="设置学习率计算回归函数和预测">设置学习率，计算回归函数和预测</h2><p>​预测时必须先使用之前计算出的均值和标准差把测试数据缩放再进行预测！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;houses.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    X_train = data[:,:<span class="number">4</span>]</span><br><span class="line">    Y_train = np.ravel(data[:,<span class="number">4</span>:<span class="number">5</span>]) <span class="comment"># 把Y转为一维数组</span></span><br><span class="line">    <span class="comment"># 将X训练集拿去标准化</span></span><br><span class="line">    X_train,mu,sigma = Zscore(X_train)</span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br><span class="line">    <span class="comment"># 设置学习率等</span></span><br><span class="line">    w_init = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    b_init = <span class="number">0</span></span><br><span class="line">    alpha = <span class="number">0.5</span></span><br><span class="line">    num_iters = <span class="number">10000</span></span><br><span class="line">    w,b = gradient_descent(X_train,Y_train,w_init,b_init,alpha,num_iters)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测出的w:<span class="subst">&#123;w&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测出的b:<span class="subst">&#123;b&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    X_forecast = [<span class="number">1200</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">40</span>]</span><br><span class="line">    X_forecast = (X_forecast-mu)/sigma</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;压缩后的测试数据<span class="subst">&#123;X_forecast&#125;</span>&quot;</span>)</span><br><span class="line">    X_predice_price = np.dot(X_forecast,w)+b</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;一个1200平米，3个卧室，1层楼，存在40年的房屋 = $<span class="subst">&#123;X_predice_price*<span class="number">1000</span>:<span class="number">0.0</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>​ 结果：</p><p><img src="https://img.issey.top/img/202209202210479.png" /></p><p>​ <del>就当那个世界的房子都这么便宜吧。</del></p><hr /><p>​ 关于线性回归模型的评估：</p><p>​常用的有均方误差和均方根误差等方法，这里留个空以后出篇文章，暂时放一放。</p><hr /><h1 id="补充学习率a的评估">补充：学习率a的评估</h1><p>​ 一般可以通过两个方面来评估：</p><p>​ 1.每次计算出的梯度，如果学习率合适，那么这个值应该在不断下降。</p><p>​2.梯度下降在整个代价函数上的”跳跃“变化，如果学习率合适，它应该是不断往下跳的。</p><p>​学习率过大：如果学习率过大，每次计算出的梯度会发生类似下面左图的情况。右图是学习率过大的情况下每次梯度下降在总成本函数上的变化，注意它是从下往上跳的，最后会导致结果发散。</p><p><img src="https://img.issey.top/img/202209202211389.png" /></p><p>​ 学习率过小：它虽然可以收敛，但是过程比较慢。</p><p><img src="https://img.issey.top/img/202209202211411.png" /></p><p>​ 学习率合适：</p><p><img src="https://img.issey.top/img/202209202211432.png" /></p><h1 id="补充浅谈正规方程">补充：浅谈正规方程</h1><p>​求解线性回归不止有梯度下降一种方法，其实还有另一种方法求解，即正规方程。不过在这里不讨论它的详细公式，只拿它与梯度下降做一个对比。</p><table><colgroup><col style="width: 35%" /><col style="width: 64%" /></colgroup><thead><tr class="header"><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr class="odd"><td>需要选择学习率</td><td>不需要选择学习率</td></tr><tr class="even"><td>需要多次迭代</td><td>一次计算得出</td></tr><tr class="odd"><td>当特征数量较多时也能较好使用</td><td>时间复杂度为<spanclass="math inline">\(O(n^3)\)</span>,当特征数量较多时太慢</td></tr><tr class="even"><td>也适用于除线性回归的其他模型</td><td>只适用于线性回归模型</td></tr></tbody></table><h1id="补充利用sklearn完成线性回归预测">补充：利用sklearn完成线性回归预测</h1><blockquote><p>关于StandardScaler()函数：<ahref="https://blog.csdn.net/qq_47175528/article/details/110480918">sklearn中StandardScaler()</a></p><p>关于SGDRegressor：随机梯度线性回归，随机梯度下降是不将所有样本都进行计算后再更新参数，而是选取一个样本，计算后就更新参数。</p><p>关于LinearRegression：也是线性回归模型，这里没用，可以自己查。</p></blockquote><p>​ 绘图模块的代码可能要自己改改。其余详细说明见代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(precision=<span class="number">2</span>) <span class="comment"># 设置numpy可见小数点</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">dlblue = <span class="string">&#x27;#0096ff&#x27;</span>; dlorange = <span class="string">&#x27;#FF9300&#x27;</span>; dldarkred=<span class="string">&#x27;#C00000&#x27;</span>; dlmagenta=<span class="string">&#x27;#FF40FF&#x27;</span>; dlpurple=<span class="string">&#x27;#7030A0&#x27;</span>;</span><br><span class="line">plt.style.use(<span class="string">&#x27;../deeplearning.mplstyle&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;houses.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    X_train = data[:, :<span class="number">4</span>]</span><br><span class="line">    X_features = [<span class="string">&#x27;size(sqft)&#x27;</span>,<span class="string">&#x27;bedrooms&#x27;</span>,<span class="string">&#x27;floors&#x27;</span>,<span class="string">&#x27;age&#x27;</span>]</span><br><span class="line">    y_train = np.ravel(data[:, <span class="number">4</span>:<span class="number">5</span>])  <span class="comment"># 把Y转为一维数组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用sklearn进行Z-score标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    X_norm = scaler.fit_transform(X_train) <span class="comment"># 标准化训练集X</span></span><br><span class="line">    <span class="comment"># print(X_norm)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建回归拟合模型</span></span><br><span class="line">    sgdr = SGDRegressor(max_iter=<span class="number">1000</span>) <span class="comment"># 设置最大迭代次数</span></span><br><span class="line">    sgdr.fit(X_norm, y_train)</span><br><span class="line">    <span class="built_in">print</span>(sgdr)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;完成的迭代次数: <span class="subst">&#123;sgdr.n_iter_&#125;</span>, 权重更新数: <span class="subst">&#123;sgdr.t_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看参数</span></span><br><span class="line">    b_norm = sgdr.intercept_</span><br><span class="line">    w_norm = sgdr.coef_</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;SGDRegressor拟合参数:       w: <span class="subst">&#123;w_norm&#125;</span>, b:<span class="subst">&#123;b_norm&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;之前自己编写的线性回归拟合参数: w: [110.61 -21.47 -32.66 -37.78], b: 362.23&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用sgdr.predict()进行预测</span></span><br><span class="line">    y_pred_sgd = sgdr.predict(X_norm)</span><br><span class="line">    <span class="comment"># 使用w和b进行预测</span></span><br><span class="line">    y_pred = np.dot(X_norm, w_norm) + b_norm</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;使用 np.dot() 预测值是否与sgdr.predict预测值相同: <span class="subst">&#123;(y_pred == y_pred_sgd).<span class="built_in">all</span>()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;训练集预测:\n<span class="subst">&#123;y_pred[:<span class="number">4</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;训练集真实值\n<span class="subst">&#123;y_train[:<span class="number">4</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制训练集与预测值匹配情况</span></span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">12</span>, <span class="number">3</span>), sharey=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ax)):</span><br><span class="line">        ax[i].scatter(X_train[:, i], y_train, label=<span class="string">&#x27;target&#x27;</span>)</span><br><span class="line">        ax[i].set_xlabel(X_features[i])</span><br><span class="line">        ax[i].scatter(X_train[:, i], y_pred, color=<span class="string">&#x27;orange&#x27;</span>, label=<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">    ax[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Price&quot;</span>)</span><br><span class="line">    ax[<span class="number">0</span>].legend()</span><br><span class="line">    fig.suptitle(<span class="string">&quot;target versus prediction using z-score normalized model&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>​ 结果：</p><p><img src="机器学习笔记2%20—%20多元线性回归\sklearn.png" /></p><p>​ 训练集预测值与真实值结果对比</p><p><img src="https://img.issey.top/img/202209202211375.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记1——一元线性回归</title>
      <link href="/article/c4e174e36609/"/>
      <url>/article/c4e174e36609/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：本系列为机器学习的学习笔记，参考教程链接：</p><p><ahref="https://www.bilibili.com/video/BV1Pa411X76s?p=9&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=747540861ba5c41c17852ccf069029f5">(强推|双字)2022吴恩达机器学习Deeplearning.ai课程</a></p><p>观点不一定完全正确，欢迎指出错误的地方。</p></blockquote><h2 id="什么是线性回归模型">什么是线性回归模型？</h2><p>回归分析是研究自变量与因变量之间数量变化关系的一种分析方法，它主要是通过因变量Y与影响它的自变量<spanclass="math inline">\(X_{i}\)</span>（i=1,2,3…）之间的<ahref="https://so.csdn.net/so/search?q=%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020">回归模型</a>，衡量自变量<spanclass="math inline">\(X_{i}\)</span>对因变量Y的影响能力的，进而可以用来预测因变量Y的发展趋势。线性回归模型指因变量和自变量呈直线型关系的模型，是回归分析中最常用且最简单的方法，线性归回模型又分为一元线性回归模型和多元回归模型。</p><hr /><h2 id="一元线性回归模型">一元线性回归模型</h2><p>一元线性回归模型即自变量只有一个的线</p><h3 id="问题引入">问题引入：</h3><p><img src="https://img.issey.top/img/202209191544874.png" /></p><p>已知上图数据集，其中，X为自变量，Y为因变量，请预测当X为5000时Y的取值。</p><h3 id="问题解析">问题解析：</h3><p>因为自变量只有一个，即让你模拟一个<spanclass="math inline">\(f_{w,b}(x)=wx+b\)</span>,使该函数与上图自变量与应变量的变化趋势尽量满足，<spanclass="math inline">\(f_{w,b}(x)\)</span>即一元线性回归函数，再用计算出的回归函数去预测值即可。难点在于，这里的w和b都是未知数，我们要做的就是推断出最合适的w和b。</p><h3 id="代价函数损失函数">代价函数（损失函数）：</h3><p>如何判断w和b是否合适，我们引入了代价函数。代价函数用于判断<strong>整体来看</strong>，每个点（Y）实际值与估计值的差距大小。</p><p>这里先随便画一条线。</p><p><img src="https://img.issey.top/img/202209191550757.png" /></p><p>令模拟出来的自变量对应应变量的值为<spanclass="math inline">\(\hat{y}\)</span>,即<spanclass="math inline">\(\hat{y} = f_{w,b}(x)\)</span>,则代价函数为：</p><p><span class="math inline">\(J(w,b)=\frac{1}{2m}\sum_{i=0}^{m-1}{(\hat{y}_{i}-y_{i})^2} =\frac{1}{2m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})^2}\)</span></p><p>其中，m为训练集样例数，第一个点下标为0。这里除以2是方便后续计算。</p><h4 id="代价函数的图像">代价函数的图像</h4><p>我们先将<span class="math inline">\(f_{w,b}(x)\)</span>简化为<spanclass="math inline">\(f_{w}(x) = wx\)</span>,那么<spanclass="math inline">\(J(w) =\frac{1}{2m}\sum_{i=0}^{m-1}{(wx_{i}-y_{i})^2}\)</span></p><p>此时<span class="math inline">\(J(w)\)</span>的图像为一个凸函数</p><p><img src="https://img.issey.top/img/202209191551748.png" /></p><p>对应的<span class="math inline">\(f_w(x)\)</span>模拟情况：</p><p><img src="https://img.issey.top/img/202209191553611.png" /></p><p>当我们将<span class="math inline">\(f_{w,b}(x)\)</span>简化为<spanclass="math inline">\(f_{b}(x) = x+b\)</span>,此时<spanclass="math inline">\(J(b)\)</span>的图像也是一个凸函数，我们姑且借用<spanclass="math inline">\(J(w)\)</span>的图像，不过变量变为了b:</p><p><img src="https://img.issey.top/img/202209191552365.png" /></p><p>对应的<span class="math inline">\(f_{b}(x)\)</span>模拟情况：</p><p><img src="https://img.issey.top/img/202209191554982.png" /></p><p>现在将<span class="math inline">\(J(w)\)</span>和<spanclass="math inline">\(J(b)\)</span>合在一起，<spanclass="math inline">\(J(w,b)\)</span>便是一个三维碗装图像：</p><p>注：图中的w和b并不对应上面的例子，只是大致图像！</p><p><img src="https://img.issey.top/img/202209191555588.png" /></p><p><u>代价函数计算出的值越小，说明模拟值与实际值差距越小，则w，b越合适，回归函数模拟的越好。所以，当代价函数值最小时，w和b最合适。</u></p><p>于是问题转化为了：求w和b使得<spanclass="math inline">\(J(w,b)\)</span>能取到极小值。</p><h4 id="为什么不是最小而是极小值">为什么不是最小而是极小值？</h4><p>这与之后要用到的算法（梯度下降法）有关，梯度下降法只能求到极小值。不过梯度下降法常用于求凸函数的极小值，而凸函数只有一个极小值，所以通常求得的是最小值。这里举个非凸函数的例子，此时用梯度下降法不一定能求得最优解。</p><p><img src="https://img.issey.top/img/202209191556964.png" /></p><h3 id="梯度下降算法">梯度下降算法</h3><blockquote><p>梯度下降算法并不只用于求解线性回归问题。</p></blockquote><p>梯度算法在讲座中被描述为：假设你站在一个山坡上，你想最快下降到你四周最低的山谷。</p><p>即选择一个基点，以四周斜率绝对值最大的方向下降，直到下降到极小值点（此时斜率为0）停止。我们认为这个极小值点对应的w和b即为所求，一般我们选择<spanclass="math inline">\((0,0)\)</span>作为基点，即w和b开始为<spanclass="math inline">\((0,0)\)</span>，不过实际上基点怎么选都可以。</p><h4id="梯度下降算法公式对于一元线性回归模型">梯度下降算法公式（对于一元线性回归模型）：</h4><p>重复以下行为直到收敛：</p><p><span class="math inline">\(w = w - a\frac{\partial J(w,b)}{\partialw}\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(w,b)}{\partialb}\)</span></p><p>其中，<spanclass="math inline">\(a\)</span>被称为学习率。之后会讨论学习率<spanclass="math inline">\(a\)</span>的选择。</p><p>注意：w和b应该同时更新！（会在代码块详细说明）</p><p><span class="math inline">\(\frac{\partial J(w,b)}{\partial w} =\frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}x_{i}\)</span></p><p><span class="math inline">\(\frac{\partial J(w,b)}{\partial w} =\frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}\)</span></p><p>(之前代价函数除个2就是为了这里化简)</p><h4 id="学习率a的选择">学习率a的选择</h4><p>如果<spanclass="math inline">\(a\)</span>很小，那么每一步都走的很小，收敛过程就会很慢。</p><p><img src="https://img.issey.top/img/202209191556779.png" /></p><p>如果<span class="math inline">\(a\)</span>很大，<spanclass="math inline">\(J(w,b)\)</span>可能不会每次迭代都下降，可能错过最佳点，甚至导致发散。</p><p><img src="https://img.issey.top/img/202209191557675.png" /></p><p>关于学习率的设置有许多种方法，这里不做专门讨论<del>（其实是还没学到）</del>，姑且采用网上查到的一种简单的方法：在运行梯度下降法的时候会尝试一系列学习率的取值：...0.001,0.003，0.01, 0.03，0.1,0.3，1....尽量以三倍增长，直到找到一个合适的学习率。</p><h4 id="关于梯度下降每一步的变化">关于梯度下降每一步的变化</h4><p>梯度下降每一步并不是相等的，因为每一次迭代时，偏导数都会不断变化。在学习率选择合适的情况下，大概可以得到以下的每一步梯度变化图像。x轴为迭代次数，y轴为梯度。</p><p><img src="https://img.issey.top/img/202209191557819.png" /></p><p><img src="https://img.issey.top/img/202209191557852.png" /></p><p>可以看到最开始梯度很大，到后来慢慢接近于0。</p><h4 id="补充">补充：</h4><p>这里解释下为什么非凸函数中找到的不一定是最优解：</p><p><img src="https://img.issey.top/img/202209191558036.png" /></p><p>我们选择1和2分别作为起点，可能到达两个极小值点，我们无法判断找到的极小值点是否是全局最小值。当然凸函数只有一个极值点，所以对于凸函数，不存在这个问题。</p><h3 id="代码部分---案例实现">代码部分 - 案例实现</h3><h4 id="数据">数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2104.000000</span>,<span class="number">1600.000000</span>,<span class="number">2400.000000</span>,<span class="number">1416.000000</span>,<span class="number">3000.000000</span>,<span class="number">1985.000000</span>,<span class="number">1534.000000</span>,<span class="number">1427.000000</span>,<span class="number">1380.000000</span>,<span class="number">1494.000000</span>,<span class="number">1940.000000</span>,<span class="number">2000.000000</span>,<span class="number">1890.000000</span>,<span class="number">4478.000000</span>,<span class="number">1268.000000</span>,<span class="number">2300.000000</span>,<span class="number">1320.000000</span>,<span class="number">1236.000000</span>,<span class="number">2609.000000</span>,<span class="number">3031.000000</span>,<span class="number">1767.000000</span>,<span class="number">1888.000000</span>,<span class="number">1604.000000</span>,<span class="number">1962.000000</span>,<span class="number">3890.000000</span>,<span class="number">1100.000000</span>,<span class="number">1458.000000</span>,<span class="number">2526.000000</span>,<span class="number">2200.000000</span>,<span class="number">2637.000000</span>,<span class="number">1839.000000</span>,<span class="number">1000.000000</span>,<span class="number">2040.000000</span>,<span class="number">3137.000000</span>,<span class="number">1811.000000</span>,<span class="number">1437.000000</span>,<span class="number">1239.000000</span>,<span class="number">2132.000000</span>,<span class="number">4215.000000</span>,<span class="number">2162.000000</span>,<span class="number">1664.000000</span>,<span class="number">2238.000000</span>,<span class="number">2567.000000</span>,<span class="number">1200.000000</span>,<span class="number">852.000000</span>,<span class="number">1852.000000</span>,<span class="number">1203.000000</span></span><br><span class="line"><span class="number">399.899994</span>,<span class="number">329.899994</span>,<span class="number">369.000000</span>,<span class="number">232.000000</span>,<span class="number">539.900024</span>,<span class="number">299.899994</span>,<span class="number">314.899994</span>,<span class="number">198.998993</span>,<span class="number">212.000000</span>,<span class="number">242.500000</span>,<span class="number">239.998993</span>,<span class="number">347.000000</span>,<span class="number">329.998993</span>,<span class="number">699.900024</span>,<span class="number">259.899994</span>,<span class="number">449.899994</span>,<span class="number">299.899994</span>,<span class="number">199.899994</span>,<span class="number">499.997986</span>,<span class="number">599.000000</span>,<span class="number">252.899994</span>,<span class="number">255.000000</span>,<span class="number">242.899994</span>,<span class="number">259.899994</span>,<span class="number">573.900024</span>,<span class="number">249.899994</span>,<span class="number">464.500000</span>,<span class="number">469.000000</span>,<span class="number">475.000000</span>,<span class="number">299.899994</span>,<span class="number">349.899994</span>,<span class="number">169.899994</span>,<span class="number">314.899994</span>,<span class="number">579.900024</span>,<span class="number">285.899994</span>,<span class="number">249.899994</span>,<span class="number">229.899994</span>,<span class="number">345.000000</span>,<span class="number">549.000000</span>,<span class="number">287.000000</span>,<span class="number">368.500000</span>,<span class="number">329.899994</span>,<span class="number">314.000000</span>,<span class="number">299.000000</span>,<span class="number">179.899994</span>,<span class="number">299.899994</span>,<span class="number">239.500000</span></span><br></pre></td></tr></table></figure><h4 id="导入数据并绘制初始图">导入数据并绘制初始图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(<span class="string">&#x27;test.txt&#x27;</span>,dtype=np.float32,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">x_train = data[<span class="number">0</span>]</span><br><span class="line">y_train = data[<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_train,y_train,marker=<span class="string">&#x27;o&#x27;</span>,c=<span class="string">&#x27;r&#x27;</span>) <span class="comment"># marker 将样式设置为叉，c将颜色设置为红色</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="梯度产生函数">梯度产生函数</h4><p>对应公式：</p><p>sum_dw = <span class="math inline">\(\frac{\partial J(w,b)}{\partialw} = \frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}x_i\)</span></p><p>sum_db = <span class="math inline">\(\frac{\partial J(w,b)}{\partialw} = \frac{1}{m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 产生梯度函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient</span>(<span class="params">x,y,w,b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        x: x训练集</span></span><br><span class="line"><span class="string">        y: y训练集</span></span><br><span class="line"><span class="string">        w,b: 模型参数</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">        sum_dw: 代价函数对w的偏导数</span></span><br><span class="line"><span class="string">        sum_db: 代价函数对d的偏导数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 训练样例个数</span></span><br><span class="line">    sum_dw = <span class="number">0</span></span><br><span class="line">    sum_db = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb = w*x[i]+b</span><br><span class="line">        dw_i = (f_wb - y[i])*x[i]</span><br><span class="line">        db_i = f_wb - y[i]</span><br><span class="line">        sum_dw += dw_i</span><br><span class="line">        sum_db += db_i</span><br><span class="line"></span><br><span class="line">    sum_dw = sum_dw / m</span><br><span class="line">    sum_db = sum_db / m</span><br><span class="line">    <span class="keyword">return</span> sum_dw,sum_db</span><br></pre></td></tr></table></figure><h4 id="梯度迭代函数">梯度迭代函数</h4><p>对应公式：</p><p>重复以下行为直到收敛：</p><p><span class="math inline">\(w = w - a\frac{\partial J(w,b)}{\partialw}\)</span></p><p><span class="math inline">\(b = b - a\frac{\partial J(w,b)}{\partialb}\)</span></p><p>注：代码中是让他迭代一定次数而并非以收敛为结束判断条件。这是因为当迭代次数足够大，也无限接近收敛了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度迭代函数(计算w和b)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x,y,init_w,init_b,alpha,num_iters</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数说明:</span></span><br><span class="line"><span class="string">        x: x训练集</span></span><br><span class="line"><span class="string">        y: y训练集</span></span><br><span class="line"><span class="string">        init_w: w初始值</span></span><br><span class="line"><span class="string">        init_b: b初始值</span></span><br><span class="line"><span class="string">        alpha: 学习率</span></span><br><span class="line"><span class="string">        num_iters: 迭代次数</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w,b:最终找到的w和b</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    w = init_w</span><br><span class="line">    b = init_b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># 产生梯度</span></span><br><span class="line">        sum_dw,sum_db = compute_gradient(x, y, w, b)</span><br><span class="line">        <span class="comment"># 同时更新w和b</span></span><br><span class="line">        w = w - alpha*sum_dw</span><br><span class="line">        b = b - alpha*sum_db</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h4 id="代价函数">代价函数</h4><p>对应公式：</p><p><span class="math inline">\(J(w,b)=\frac{1}{2m}\sum_{i=0}^{m-1}{(\hat{y}*{i}-y*{i})^2} =\frac{1}{2m}\sum_{i=0}^{m-1}{(f_{w,b}(x_{i})-y_{i})^2}\)</span></p><p>这里只用于检验结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 代价函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">x, y, w, b</span>):</span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb = w * x[i] + b</span><br><span class="line">        cost = cost + (f_wb - y[i]) ** <span class="number">2</span></span><br><span class="line">    total_cost = <span class="number">1</span> / (<span class="number">2</span> * m) * cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><h4 id="绘图和预测">绘图和预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;test.txt&#x27;</span>, dtype=np.float32, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    x_train = data[<span class="number">0</span>]</span><br><span class="line">    y_train = data[<span class="number">1</span>]</span><br><span class="line">    plt.scatter(x_train, y_train, marker=<span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;r&#x27;</span>)  <span class="comment"># marker 将样式设置为叉，c将颜色设置为红色</span></span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    init_m = <span class="number">0</span></span><br><span class="line">    init_b = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 一些梯度下降的设置</span></span><br><span class="line">    iterations = <span class="number">100000</span></span><br><span class="line">    tmp_alpha = <span class="number">0.000000095</span></span><br><span class="line">    w,b = gradient_descent(x_train,y_train,init_m,init_b,tmp_alpha,iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;线性回归函数为:f(x) = <span class="subst">&#123;w&#125;</span>x + <span class="subst">&#123;b&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;此时代价函数为:<span class="subst">&#123;compute_cost(x_train,y_train,w,b)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测当x = 5000是，y的值为:<span class="subst">&#123;w*<span class="number">5000</span>+b&#125;</span>&quot;</span>)</span><br><span class="line">    x = np.linspace(<span class="number">0</span>,<span class="number">5000</span>,<span class="number">100</span>)</span><br><span class="line">    y = w*x+b</span><br><span class="line">    plt.plot(x,y)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>在设置学习率alpha时，如果大了会报错，过小模拟出来的图像差距过大，这里尝试了许多次选了一个自认为比较合适的值。</p><h4 id="结果">结果</h4><p><img src="https://img.issey.top/img/202209191559138.png" /></p><p><img src="https://img.issey.top/img/202209191600322.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归模型 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PageHelper返回给前端的数据示例以及参数说明</title>
      <link href="/article/da43cda93935/"/>
      <url>/article/da43cda93935/</url>
      
        <content type="html"><![CDATA[<h1id="pagehelper返回给前端的数据示例以及参数说明">PageHelper返回给前端的数据示例以及参数说明</h1><h2id="请求成功后前端拿到的response-body">请求成功后，前端拿到的Responsebody：</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;total&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span>    <span class="comment">// 所有数据条数</span></span><br><span class="line">  <span class="attr">&quot;list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>        <span class="comment">// 数据列表</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span> </span><br><span class="line">      <span class="attr">&quot;pwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span>    </span><br><span class="line">      <span class="attr">&quot;dongjie&quot;</span><span class="punctuation">:</span> *<span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*********&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;qq&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;tishi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;huida&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dizhi&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;youbian&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*****&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;regtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;truename&quot;</span><span class="punctuation">:</span> <span class="string">&quot;**********&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;pageNum&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>        <span class="comment">// 当前页码</span></span><br><span class="line">  <span class="attr">&quot;pageSize&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span>    <span class="comment">// 每页多少条数据</span></span><br><span class="line">  <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span>        <span class="comment">// 当前页有多少条数据，因为总数据只有7条，这是最后一页，少了一条</span></span><br><span class="line">  <span class="attr">&quot;startRow&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span>    <span class="comment">// 数据起始行，指本页面第一条数据在数据库中的行数</span></span><br><span class="line">  <span class="attr">&quot;endRow&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span>        <span class="comment">// 数据末行，指本页面最后一条数据在数据库中的行数</span></span><br><span class="line">  <span class="attr">&quot;pages&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>        <span class="comment">// 总页数</span></span><br><span class="line">  <span class="attr">&quot;prePage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span>        <span class="comment">// 前一页，第一页的prePage = 0</span></span><br><span class="line">  <span class="attr">&quot;nextPage&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span>    <span class="comment">// 后一页，最后一页的nextPage = 0</span></span><br><span class="line">  <span class="attr">&quot;isFirstPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>        <span class="comment">// 是否是第一页</span></span><br><span class="line">  <span class="attr">&quot;isLastPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span>    <span class="comment">// 是否是最后一页</span></span><br><span class="line">  <span class="attr">&quot;hasPreviousPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span>    <span class="comment">// 有前一页吗</span></span><br><span class="line">  <span class="attr">&quot;hasNextPage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>        <span class="comment">// 有后一页吗</span></span><br><span class="line">  <span class="attr">&quot;navigatePages&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span>    <span class="comment">// 每页显示的页码个数</span></span><br><span class="line">  <span class="attr">&quot;navigatepageNums&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>        <span class="comment">// 显示的页码数（大概是这意思，没试过，应该是配合上一个属性用的）</span></span><br><span class="line">    <span class="number">1</span><span class="punctuation">,</span>    </span><br><span class="line">    <span class="number">2</span>    </span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;navigateFirstPage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span>    <span class="comment">// 起始页码</span></span><br><span class="line">  <span class="attr">&quot;navigateLastPage&quot;</span><span class="punctuation">:</span> <span class="number">2</span>        <span class="comment">// 尾页码</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
